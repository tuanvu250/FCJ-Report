[
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/4-eventparticipated/4.1-event1/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": "Summary Report: “GenAI \u0026amp; Data” Event Objectives Update on GenAI trends and strategies on AWS Learn how to build a unified data foundation for AI/Analytics Introduction to the AI-Driven Development Lifecycle (AI-DLC) in software development Gain insights into security for GenAI applications and the role of AI Agents in enterprises Speakers Jun Kai Loke – AI/ML Specialist SA, AWS Kien Nguyen – Solutions Architect, AWS Tamelly Lim – Storage Specialist SA, AWS Binh Tran – Senior Solutions Architect, AWS Taiki Dang – Solutions Architect, AWS Michael Armentano – Principal WW GTM Specialist, AWS Key Highlights 1. Building a Unified Data Foundation on AWS for AI \u0026amp; Analytics Strategies for constructing a unified, scalable data foundation for AI/Analytics. End-to-end data pipeline: ingestion → storage → processing → access → governance. Overcoming 3 silos (Data/People/Business); enable self-service with standardized governance. Core services: S3, Glue, Redshift, Lake Formation, OpenSearch, Kinesis/MSK. 2. Building the Future: GenAI Adoption Strategy on AWS Vision and trends of GenAI; roadmap for enterprise adoption. Amazon Bedrock: model choice, customization/RAG, guardrails, cost/latency optimization. AgentCore: framework-independent runtime, tool integration gateway, identity \u0026amp; observability. Amazon Nova and ecosystem frameworks (CrewAI, LangGraph, LlamaIndex, Strands). 3. Securing Generative AI Applications with AWS Risks per OWASP LLM (LLM01/02); ensuring safe output handling. Security at multiple layers: infrastructure, models, applications; IAM, encryption, zero-trust, continuous monitoring. 5 Security Pillars: Compliance \u0026amp; Governance, Legal \u0026amp; Privacy, Controls, Risk Management, Resilience. Generative AI Security Scoping Matrix (Scope 1 → 5): from consumer apps to self-trained models. Bedrock Guardrails: filter sensitive content with configurable thresholds. Human-in-the-loop: human approval/intervention when needed. Observability (OpenTelemetry): transparent monitoring, logging, and tracing of AI behaviors. 4. Beyond Automation: AI Agents as Productivity Multipliers Agentic AI: from assistants → multi-agent systems; less human oversight, more autonomy. Applications: customer support, BI with Amazon Q (QuickSight), workflow automation. Amazon Q in QuickSight: Build Dashboards/Reports, Data Q\u0026amp;A, Executive Summaries. Expected value: exponential productivity gains; requires strong data foundation \u0026amp; governance. 5. Reliability and Veracity of GenAI Challenge of hallucination → mitigated via Prompt Engineering, RAG, Fine-tuning, Parameter Tuning. RAG in action: user input → embeddings → contextual retrieval → LLM → grounded response. 6. AI-Driven Development Lifecycle (AI-DLC) An AI-centric lifecycle: Inception → Construction → Operation. Evolution: AI-Assisted → AI-Driven → AI-Managed; AI orchestrates, humans approve. Deployment infrastructure: IaC, automated testing, observability, risk management. 7. Amazon SageMaker (Unified Studio – Next Gen) One unified environment for data, analytics, and AI: SQL analytics, data processing, model development/training, GenAI app development, BI, streaming, search analytics.\nLakehouse + Governance: catalog/lineage, policy-based access, auditing; unified Data \u0026amp; AI governance.\nZero-ETL integration: core S3 ↔ Redshift, connections to Aurora, DynamoDB, RDS, OpenSearch, Kinesis/MSK, Salesforce, SAP, ServiceNow.\nFull MLOps: pipelines/experiments, model registry, deployment endpoints, Feature Store, monitoring.\nIntegrated with Bedrock \u0026amp; JumpStart: access to foundation models (via Bedrock), reference solutions, and accelerated deployment on SageMaker.\nSDLC automation: From planning to maintenance\nCode transformation: Java upgrade, .NET modernization\nAWS Transform agents: VMware, Mainframe, .NET migration\nKey Takeaways Design Mindset Business-first approach: Always start from business needs, not technology. Ubiquitous language: The importance of shared vocabulary between business and tech, especially in teamwork and communication with mentors. Bounded contexts: Understanding how to partition domains to avoid complexity when scaling. Architecture \u0026amp; Technology Unified Data Foundation: ingestion → storage → processing → access → governance. GenAI on AWS: Bedrock (model choice, guardrails, RAG), AgentCore (runtime, gateway, identity, observability), Nova LLMs. AI Agents: from assistants → multi-agent systems; real-world use cases like customer support and BI with Amazon Q. AI-DLC: AI as the core collaborator in SDLC (Inception → Construction → Operation). Security-first mindset: Guardrails, human-in-the-loop, governance \u0026amp; monitoring (OpenTelemetry). Strategy \u0026amp; Application Phased approach: Avoid rushing; need a clear roadmap for modernization \u0026amp; AI adoption. Zero-ETL \u0026amp; Unified Studio (SageMaker): Simplify data integration, centralize AI lifecycle management. ROI measurement: Not just cost savings, but agility and productivity. Applying to Work In my project:\nExperiment with AI Agents for workflows like registration/login or customer support. Apply validation/guardrails to ensure safe integration of GenAI features. In team projects (Sprint 0, serverless vs containerization):\nApply AI-DLC principles to split tasks logically: AI supports research/code generation, team reviews \u0026amp; approves. Understand when to use Lambda (serverless) vs ECS/Fargate (containers). In my learning path:\nRecognize the need for a business-first approach when writing documents and gathering requirements. Acknowledge that a strong data foundation is critical for any successful GenAI application. Event Experience Learned directly from AWS experts on Data, GenAI adoption, Security, AI Agents, and AI-DLC. Slides and case studies gave me a clear picture of how AgentCore works in real-world scenarios. Understood how AWS envisions the future of software development: AI not just as an assistant but as a core lifecycle component. Realized that successful GenAI adoption requires solid data foundation + strong security + structured strategy. Lesson learned AI Agents and AgentCore will soon become critical in enterprise applications → I should learn early to stay ahead. Data platform \u0026amp; governance are essential → not just coding, but also managing data properly. AI-DLC highlights AI’s role in future SDLC → I can experiment with small projects now. Security is not an afterthought; it must be built into GenAI systems from the start. Some event photos "
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/4-eventparticipated/4.2-event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": "Summary Report: \u0026ldquo;AI-Driven Development Life Cycle: Reimagining Software Engineering\u0026rdquo; Event Objectives Explore how AI is transforming the entire software development lifecycle, from planning to deployment. Learn how to integrate AI to increase productivity and focus on high-value creative tasks.\nEvent Information Time: 2:00 PM - 4:30 PM, Friday, October 3rd, 2025 Location: AWS Event Hall, L26 Bitexco Tower, HCMC Speakers: Toan Huynh, My Nguyen Key Highlights 1. AI in Development - Outcomes AI adoption brings:\nVelocity - Reduce time-to-market Quality - Meet usability \u0026amp; reliability expectations Market Responsiveness - React quickly to market changes Innovation - Drive innovation Developer Engagement - Increase developer satisfaction Productivity - Increase value, reduce costs 2. Challenges with AI Development AI-Managed: Unreliable, hard to explain, lacks flexibility\nAI-Assisted: Not truly effective, manual inefficiencies, technical debt accumulation\n3. AI-Driven Development Lifecycle (AI-DLC) Core Concept:\nAI as Collaborator: AI assists developers, humans control critical decisions Human-Centric: Developers remain central, AI enhances not replaces Accelerated Delivery: Development cycles reduced from weeks/months to hours/days Two phases:\nInception: Build Context → User Stories → Plan Construction: Domain Model → Generate code → Deploy with IaaC 4. 5-Stage Sequential Process Product Owner → 2. Architect (Design) → 3. Architect (Construction) → 4. Engineer (POC) → 5. Engineer (MVP) 5. Anti-Patterns - 7 Things to Avoid Don\u0026rsquo;t single-shot multi-step problems Maximize semantics-to-token ratio Refresh context strategically Control AI over-reach Model knows old better than new Brownfield needs special context building Think surgical precision 6. Amazon Q Developer Prompt Structure: Role → Plan (markdown with checkboxes) → Task\nExample: Build travel booking app with AI integration\nWorkflow: Create folder → User stories → Clarification → Checkbox → Review → Execute\n7. Kiro - AI-Powered Coding Assistant Kiro is AWS\u0026rsquo;s AI coding assistant with 4 key features:\nAgent Hooks: Auto-trigger tasks on events (file save), generate docs/tests/optimize VS Code Compatible: Support plugins, themes, settings Claude Models: Sonnet 3.7/4 with powerful reasoning Enterprise Security: Built \u0026amp; operated by AWS Advantages: High automation, context-aware, documentation-driven, enterprise-ready\nKey Takeaways Participating in the \u0026ldquo;GenAI-powered App-DB Modernization\u0026rdquo; workshop was an incredibly enriching experience, providing me with a comprehensive perspective on modernizing applications and databases using contemporary methods and tools. Some notable highlights include:\nLearning from highly skilled speakers Speakers from AWS and major technology organizations shared best practices in modern application design. Through real-world case studies, I gained a deeper understanding of how to apply Domain-Driven Design (DDD) and Event-Driven Architecture to large-scale projects. Hands-on technical exposure Participating in event storming sessions helped me visualize how to model business processes into domain events. Learned how to split microservices and define bounded contexts to manage complexity in large systems. Understood the trade-offs between synchronous and asynchronous communication, as well as integration patterns like pub/sub, point-to-point, and streaming. Leveraging modern tools Directly explored Amazon Q Developer, an AI tool supporting the SDLC from planning to maintenance. Learned how to automate code transformation and pilot serverless with AWS Lambda, thereby enhancing development productivity. Lessons learned Applying DDD and event-driven patterns helps reduce coupling while increasing scalability and resilience for systems. Modernization strategies require a phased approach and ROI measurement; rushing to transform the entire system should be avoided. AI tools like Amazon Q Developer can boost productivity when integrated into the current development workflow. Some event photos "
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/4-eventparticipated/4.3-event3/",
	"title": "Event 3",
	"tags": [],
	"description": "",
	"content": "Report: “WORKSHOP: DATA SCIENCE ON AWS” Event Objectives Share AWS AI services Demonstrate deploying AI models using Amazon SageMaker Show how to deploy models and access them via APIs Speakers Van Hoang Kha - Cloud Solutions Architect, AWS User Group Leader Bach Doan Vuong - Cloud Developer Engineer, AWS Community Builder Highlights 1. The Importance of Cloud Computing in Data Science Opened with an affirmation of cloud computing\u0026rsquo;s pivotal role in modern data science, especially in processing and storing large datasets. Cloud vs. On-premise: Cloud: Superior scalability, deployment agility, and flexible cost model (OPEX). On-premise: High upfront infrastructure cost (CAPEX), difficult to scale resources instantly, and maintenance burden. AWS provides an \u0026ldquo;End-to-End\u0026rdquo; platform for the Data Science pipeline: from collection, storage, and processing to training and model operation. 2. The 3-Layer Stack of AWS AI AWS designs its AI ecosystem in 3 distinct layers to serve diverse needs:\nLayer 1: AI Services (Pre-managed AI Services)\nSuitable for: App Developers not specialized in ML.\nIntelligent APIs pre-trained by AWS. Quickly integrate into applications to add AI features immediately. Key Services: Amazon Comprehend: Text and sentiment analysis. Amazon Translate: Automated translation. Amazon Textract: Identify and extract data from text/images. Amazon Rekognition: Image and video analysis. Amazon Bedrock: Gateway to powerful Foundation Models. Layer 2: ML Services (Amazon SageMaker)\nSuitable for: Data Scientists \u0026amp; ML Engineers.\nComprehensive Integrated Development Environment (IDE) for Machine Learning. Provides tools for every step: Data Wrangler: Data preparation. Feature Store: Feature repository. SageMaker Autopilot: Automated training (AutoML). Model Registry: Model lifecycle management. Layer 3: AI Infrastructure (ML Infrastructure)\nSuitable for: Expert Practitioners needing deep optimization.\nProvides the most powerful computing resources: EC2 Instances (P5, G6, Trn1\u0026hellip;): Specialized chipsets for training/inference. EKS/ECS: Run AI workloads on container platforms. 3. Powerful Support Tools for Students Amazon SageMaker: The best place to start learning and practicing industry-standard ML workflows. Amazon Comprehend: Powerful tool for NLP tasks like review analysis, text classification. Amazon Translate: Supports building multilingual applications at low cost. Amazon Textract: Effective solution for digitizing data from paper documents. 4. Demo \u0026amp; Practice Demo 1: No-code ML with SageMaker Canvas\nThe speaker demonstrated how to create a prediction model without writing any code using a visual drag-and-drop interface. Lesson: Enables non-experts (Business Analysts) to apply AI as well. Demo 2: Deploy Model as an API Service\nThe process of deploying a trained model to a SageMaker Endpoint and exposing it via API Gateway. Lesson: Understand the process of \u0026ldquo;Productionizing\u0026rdquo; an AI model for end-users. Event Experience The workshop was a truly valuable experience, helping me systematize my knowledge of AI/ML on the Cloud platform.\nSystems Thinking: Understanding the 3 distinct layers helps me choose the right tool for the right problem, avoiding \u0026ldquo;using a sledgehammer to crack a nut\u0026rdquo;. Real-world Perspective: Visual demos clearly illustrated the path from a notebook file to a functioning real-world API service. Motivation: Seeing the strong support AWS offers to the community and students through Free Tier programs and learning materials. Some photos from the event "
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/4-eventparticipated/4.4-event4/",
	"title": "Event 4",
	"tags": [],
	"description": "",
	"content": "Workshop Harvest Report: “AWS Cloud Mastery Series #1: GENERATIVE AI, RAG \u0026amp; AWS AGENTIC AI” Event Objectives Master the art of Prompt Engineering to effectively control AI models. Explore the ecosystem of Pretrained AI Services available on AWS. Gain a deep understanding of building AI applications using RAG (Retrieval-Augmented Generation). Update on the latest trends in Agentic AI and how to move AI Agents from prototype (POC) to production using Amazon Bedrock AgentCore. Explore the Pipecat Framework for building real-time voice-based virtual assistants. Speakers Lam Tuan Kiet - Sr. DevOps Engineer (FPT Software) Danh Hoang Hieu Nghi - AI Engineer (Renova Cloud) Dinh Le Hoang Anh - Cloud Engineer Trainee (First Cloud AI Journey) Key Highlights 1. Prompt Engineering \u0026amp; Foundation Models (The Core Foundation) Before diving into complex services, the event emphasized the importance of understanding and communicating with Foundation Models via Amazon Bedrock.\nZero-shot / Few-shot Prompting: Techniques involving direct instructions or providing examples to guide the model\u0026rsquo;s output format. Chain of Thought (CoT): A crucial technique requiring the model to \u0026ldquo;think step-by-step,\u0026rdquo; significantly improving accuracy for complex logical problems. 2. Pretrained AWS AI Services (Ready-to-Use APIs) Introduction to \u0026ldquo;ready-to-use\u0026rdquo; APIs that integrate intelligent features without model training:\nImage/Video: Amazon Rekognition. Language: Amazon Translate, Comprehend, Textract (OCR). Audio: Amazon Polly (Text-to-Speech), Transcribe (Speech-to-Text). 3. RAG - Retrieval Augmented Generation A process helping AI answer based on enterprise data, reducing hallucinations:\nEmbeddings: Using Amazon Titan Text Embeddings V2 to vectorise text for semantic search. Knowledge Bases for Amazon Bedrock: Fully managed process handling Chunking -\u0026gt; Vector Store -\u0026gt; Retrieval -\u0026gt; Generation. 4. The Evolution to Agentic AI The event introduced the next evolution of GenAI:\nGenAI Assistants: Follow rules, automate repetitive tasks. GenAI Agents: Goal-oriented, handling a broader range of tasks. Agentic AI Systems: Multi-agent systems acting fully autonomously with minimal human oversight. The \u0026ldquo;Prototype to Production Chasm\u0026rdquo;: Moving Agents from POC to Production faces major hurdles regarding:\nPerformance \u0026amp; Scalability. Security \u0026amp; Governance. Complexity: Difficulties in managing Memory, access controls, and auditing Agent interactions. 5. Amazon Bedrock AgentCore: Bridging the Gap To solve these challenges, AWS introduced AgentCore - a comprehensive platform for building and operating Agents:\nKey Components: Runtime \u0026amp; Memory: Execution environment and the ability to \u0026ldquo;remember\u0026rdquo; interaction history/learning. Identity \u0026amp; Gateway: Identity management and secure connection gateways. Code Interpreter: Allows Agents to write and execute code to process complex data. Observability: Tools to monitor and audit agent activities. Benefit: Allows developers to focus on business logic rather than infrastructure security or context management. 6. Pipecat: Framework for Real-time Voice AI An interesting Open Source framework introduced for building Multimodal Virtual Assistants:\nFocus: Optimized for Real-time interactions and conversational streaming. Pipeline Mechanism: WebRTC Input: Receives audio signals from the user. STT (Speech-to-Text): Converts voice to text. LLM Processing: Processes natural language to generate a response. TTS (Text-to-Speech): Converts text back to voice. Output: Streams audio back to the user with ultra-low latency. Event Experience \u0026amp; Reflection Participating in this workshop expanded my perspective from basic concepts to the cutting-edge technologies shaping the future of AI.\n1. The Shift from \u0026ldquo;Q\u0026amp;A\u0026rdquo; to \u0026ldquo;Action\u0026rdquo; (Agentic AI) The most impressive concept for me was Agentic AI. Previously, I viewed AI primarily for chatting or summarization. However, through the AgentCore presentation, I see a future of \u0026ldquo;virtual employees\u0026rdquo; capable of planning, using tools (like web browsers or code interpreters), and solving complex workflows without constant human hand-holding.\n2. Solving the \u0026ldquo;Production\u0026rdquo; Puzzle I resonated deeply with the discussion on the \u0026ldquo;Chasm\u0026rdquo; between POC and Production. Tools like Amazon Bedrock AgentCore are essentially the key to building enterprise trust. They provide the necessary security layers (Identity) and control mechanisms (Observability) that allow businesses to confidently delegate tasks to AI.\n3. The Potential of Voice AI with Pipecat The Pipecat demo was fascinating. Combining WebRTC with AI models to create fluid, low-latency conversations opens up endless practical applications, such as intelligent virtual call centers, AI interview assistants, or real-time language tutors.\nConclusion The “Generative AI \u0026amp; Agentic AI on AWS” workshop provided a valuable panoramic view:\nPresent: We rely on RAG and Prompt Engineering to work effectively with data. Future: We are entering the era of Agentic AI, where Autonomous Agents will transform business operations. Tools: With the AWS ecosystem (Bedrock, AgentCore) and Frameworks (Pipecat, LangChain), technical barriers are being removed, empowering engineers to turn breakthrough ideas into reality. Some photos from the event "
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/4-eventparticipated/4.5-event5/",
	"title": "Event 5",
	"tags": [],
	"description": "",
	"content": "Report “AWS Cloud Mastery Series #2: From DevOps, IaC to Containers \u0026amp; Observability” Event Objectives Standardize Mindset: Deeply understand the Value Cycle and the core role of DevOps in continuous, reliable software delivery. Modernize Infrastructure (IaC): Shift from manual operations (ClickOps) to managing infrastructure as code using CloudFormation, Terraform, and CDK. Optimize Applications (Containerization): Master the architecture and strategy for selecting the appropriate container platform: App Runner, ECS, or EKS. Comprehensive Monitoring (Observability): Build proactive monitoring systems to detect errors and optimize performance using CloudWatch and X-Ray. Speakers AWS Experts \u0026amp; Cloud Engineers: Shared insights on system architecture, Platform Engineering strategies, and deep-dive technical demos. Key Content Details 1. DevOps Mindset \u0026amp; CI/CD Pipeline (The Foundation) The event began by redefining DevOps not just as a set of tools, but as a culture of optimizing the value stream.\nThe Value Cycle:\nA closed-loop 5-step process: Insights \u0026amp; Analysis -\u0026gt; Portfolio \u0026amp; Backlog -\u0026gt; Continuous Integration -\u0026gt; Continuous Testing -\u0026gt; Continuous Delivery. Core Goal: Increase delivery speed (Speed) to meet market demands faster, while ensuring system stability (Stability) and quality. Redefining CI/CD Concepts:\nContinuous Integration (CI): Developers merge code frequently (daily). The system automatically builds and runs Unit Tests. The goal is to detect errors early (Fail fast). Continuous Delivery: Automates the deployment process to Staging/Pre-prod environments. Deployment to Production requires human approval (Manual Trigger). Continuous Deployment: Fully automated 100% from Code Commit to running in Production (no manual intervention). Effective Pipeline Strategy:\nCentralized CI: Build a centralized CI system for security and resource management, but ensure Self-service capabilities for Developers to avoid bottlenecks. Artifact Management: Apply the \u0026ldquo;Build Once, Deploy Anywhere\u0026rdquo; principle. Source code is built only once into a Binary package (Artifact). Subsequent environments (Staging, Prod) use this exact Artifact for deployment, ensuring absolute consistency. Fail Fast Conditions: The pipeline must be configured to fail immediately if violations occur: Compilation errors, Code Style violations, Security scans finding vulnerabilities, or Tests running too slow. Measuring Efficiency (Metrics):\nUse Heatmaps to monitor the Pipeline health of the entire organization. Golden Metrics: Deployment Frequency, Change Failure Rate, and MTTR (Mean Time To Recovery). 2. Infrastructure as Code (IaC) - From ClickOps to Code This section delved into eliminating manual habits (ClickOps) and moving towards full infrastructure automation.\nThe Problem with \u0026ldquo;ClickOps\u0026rdquo;: Manual operations on the AWS Console are prone to Human Error, slow, hard to scale, and cause inconsistency between Dev/Prod environments. IaC Solutions: Provide Automation, Scalability, Reproducibility, and Collaboration. Deep Dive into Top 3 IaC Tools:\n1. AWS CloudFormation (Native Tool):\nUses text files (YAML or JSON) to describe the desired state. Template Anatomy: Structure includes Parameters (Dynamic inputs), Mappings (Handling regional differences - e.g., different AMI IDs per Region), and Resources (The actual assets to create). Stack: The unit for managing resource lifecycles. Deleting a Stack deletes all associated resources. 2. Terraform (Multi-Cloud Powerhouse):\nOpen-source tool, uses HCL (HashiCorp Configuration Language). Strength: Multi-platform support (Multi-cloud: AWS, Azure, GCP\u0026hellip;). Workflow: Write (Code) -\u0026gt; Plan (Preview changes) -\u0026gt; Apply (Execute). The Plan step is critical for safety checks. State File: Stores the actual state of the infrastructure for synchronization. 3. AWS CDK (Cloud Development Kit):\nAllows defining infrastructure using programming languages (Python, TypeScript, Java\u0026hellip;). Constructs: L1 (Cfn Resources): Detailed configuration for every line (like CloudFormation). L2 (Curated): Automatically applies Best Practices and secure default configurations. L3 (Patterns): Builds complex architectures (e.g., VPC + ALB + ECS) in just a few lines of code. Drift Detection: A crucial feature to detect discrepancies between Code and Reality (caused by manual \u0026ldquo;ClickOps\u0026rdquo; changes), helping maintain operational discipline.\n3. Containerization - Application Strategy Deep analysis of container orchestration platforms:\nKubernetes (K8s):\nArchitecture includes Control Plane (API Server, etcd, Scheduler) and Worker Nodes (Kubelet, Pods). Powerful and flexible but complex to operate. Comparison: Amazon ECS vs. Amazon EKS:\nAmazon ECS: Simple, deeply integrated with AWS (ALB, IAM). Suitable for teams wanting to reduce operational overhead and deploy fast. Amazon EKS: Based on standard Kubernetes. Powerful, massive ecosystem. Suitable for Enterprises, complex systems, or Hybrid-cloud. Compute Options:\nEC2 Launch Type: You manage the servers (Patching, Scaling). Highest control but high operational effort. AWS Fargate (Serverless): No server management required. AWS handles the infrastructure; users only define CPU/RAM for Tasks. Secure and convenient. AWS App Runner:\n\u0026ldquo;Zero-ops\u0026rdquo; solution for Web Apps/APIs. Fully automated from Source Code/Image -\u0026gt; Public URL (HTTPS) without configuring networking or servers. 4. Observability - Monitoring \u0026amp; Optimization Closing the development lifecycle loop with deep observability to ensure stable system operation.\nAmazon CloudWatch (System Eyes \u0026amp; Ears):\nMetrics: Collect performance data (CPU, Memory, Disk). Logs: Centralized application log collection. Use Logs Insights to query errors. Alarms: Automatically trigger actions (Auto Scaling, Restart Server, Send Notifications) when thresholds are breached. AWS X-Ray (Distributed Tracing):\nSolves the \u0026ldquo;needle in a haystack\u0026rdquo; problem in Microservices. Distributed Tracing: Tracks the journey of a request across multiple services to identify bottlenecks and root causes. AWS Observability Best Practices:\nUtilize AWS resources to reference standard Patterns and Recipes. Clear distinction: Logs (Discrete events) vs. Traces (Connected journeys). Event Experience \u0026amp; Reflection Participating in this series brought significant changes to my perception and technical skills:\n1. The Shift from \u0026ldquo;Ops\u0026rdquo; to \u0026ldquo;Platform Engineering\u0026rdquo; I realized the role of modern DevOps is not running after Developers to manually deploy code. DevOps is about architecting a \u0026ldquo;Highway\u0026rdquo; (Pipeline \u0026amp; Platform). A good platform allows Developers to Self-service environment creation and code deployment quickly, while staying within the safety boundaries (Governance) established by the DevOps team.\n2. Operational Discipline Lessons on Artifact Management and Drift Detection are golden rules. In an Enterprise environment, consistency is vital. Differences in build processes across environments (Dev/Test/Prod) are strictly prohibited, and manual changes to code-managed systems must be forbidden.\n3. Smart Tool Selection Strategy There is no \u0026ldquo;best\u0026rdquo; tool, only the \u0026ldquo;most suitable\u0026rdquo; one:\nNeed absolute stability and deepest support for new AWS services: Choose CloudFormation. Enterprise using Multi-cloud or Hybrid-cloud: Terraform is the optimal choice. Strong Programming Development Team needing to build complex architectures fast with high code reuse: AWS CDK is the strongest weapon. Simple Web Applications: Use App Runner instead of wasting resources operating a Kubernetes cluster. Conclusion The \u0026ldquo;DevOps \u0026amp; IaC Mastery\u0026rdquo; series provided a complete roadmap for the Cloud journey:\nMindset: Transitioning from manual work to automation and data-driven measurement. Infrastructure: Mastering IaC for scalable, reproducible systems with drift control. Operations: Combining flexible Containerization and deep Observability to ensure system stability, high performance, and self-healing capabilities. This is a solid knowledge foundation for building large-scale, modern software systems on AWS.\nSome photos from the event "
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/4-eventparticipated/4.6-event6/",
	"title": "Event 6",
	"tags": [],
	"description": "",
	"content": "Report: “AWS Cloud Mastery Series #3” Event Purpose This series was not just about individual services but a journey in System Thinking, helping to transition from traditional infrastructure management to a Cloud-Native Security model. The core objectives included:\nCommunity Connection: Spreading the spirit of learning and skill development through AWS Cloud Clubs. Governance Foundation: Managing scale with hundreds of AWS accounts while ensuring compliance. Defense in Depth: Combining Identity, Network, and Data protection to eliminate Single Points of Failure. Automated Response: Removing human latency from the incident response process. Speakers List Huynh Hoang Long, Dinh Le Hoang Anh - AWS Builders Tran Duc Anh, Nguyen Tuan Thinh, Nguyen Do Thanh Dat - Cloud Engineer Trainee FCJ Kha Van - Cloud Security Engineer, AWS Community Builders Thinh Lam, Viet Nguyen - FCJer Mendel Grabski (Long) - ex Head of Security \u0026amp; DevOps Cloud Security Solution Architect Tinh Truong - AWS Builders, Platform Engineer at TymeX Detailed Content PART 1: KICK-OFF - AWS CLOUD CLUBS \u0026amp; OPPORTUNITIES The journey began with the introduction of AWS Cloud Clubs, a place to nurture future Cloud talents.\n1. Vision:\nEmpower students to explore and grow cloud computing skills. Develop technical leadership and build global connections. 2. Core Benefits:\nBuild Skills: Learn through hands-on projects, access AWS exam vouchers and Udemy accounts. Build Community: Connect with AWS experts and industry speakers. Build Opportunities: Enhance personal portfolios, receive AWS credits, and get career support. 3. The Badging Journey:\nGamified development roadmap for Core Team members and Captains. Levels ranging from Bronze, Silver, Gold, Platinum to Diamond. Rewards: AWS Credits ($200+), Certification Vouchers, Exclusive Swag kits, and pre-approval for Student Community Day. PART 2: IDENTITY \u0026amp; GOVERNANCE FOUNDATION Security in the Cloud starts with controlling \u0026ldquo;Who can do what\u0026rdquo;.\n1. Modern IAM Mindset:\nIdentity First: In the Cloud environment, Identity is the new firewall. Credential Spectrum: Absolute shift from Long-term Credentials (Permanent Access Keys - high risk) to Short-term Credentials (STS tokens - secure, auto-expire). Least Privilege: Apply minimum necessary permissions. Avoid using * in Policies unless absolutely necessary. 2. Governance at Scale with AWS Organizations:\nHierarchical Structure: Divide the organization into Organizational Units (OUs) like Security, Shared Services, Workloads (Prod/Dev) to isolate risks. Service Control Policies (SCPs): This is the \u0026ldquo;Constitution\u0026rdquo; of the organization. SCPs establish Guardrails that block dangerous actions (e.g., prohibiting CloudTrail disablement, restricting Regions) that even Admin accounts cannot bypass. PART 3: VISIBILITY \u0026amp; DETECTION You cannot protect what you cannot see.\n1. Amazon GuardDuty - Intelligent Scout:\nUses Machine Learning to detect anomalies from 3 foundational data sources: CloudTrail (management events), VPC Flow Logs (network traffic), and DNS Logs (domain queries). Runtime Monitoring: Advanced feature that looks \u0026ldquo;deep\u0026rdquo; inside the operating system (via a lightweight Agent) to detect strange processes, file modifications, or privilege escalation behaviors. 2. AWS Security Hub - Command Center:\nSolves the \u0026ldquo;alert fatigue\u0026rdquo; problem using ASFF (AWS Security Finding Format). It normalizes alerts from GuardDuty, Inspector, and Macie into a single JSON language. Acts as a Cloud Security Posture Management (CSPM) tool, automatically checking if the system complies with CIS, PCI-DSS standards. PART 4: NETWORK SECURITY Building a \u0026ldquo;Digital Fortress\u0026rdquo; with a defense-in-depth strategy from the edge to the core.\n1. Fundamental Controls (VPC Fundamentals):\nSecurity Groups (Stateful): Apply Micro-segmentation. Instead of whitelisting IP addresses (which change easily), use Security Group Referencing (e.g., SG-DB only allows traffic from SG-App). NACLs (Stateless): Act as a coarse filtering layer at the Subnet boundary, used to block blacklisted IPs or untrusted subnets. 2. Advanced Defense (Advanced Filtering):\nDNS Firewall (Route 53 Resolver): Blocks connections to Command \u0026amp; Control (C2) servers right at the domain resolution step. This is a crucial choke point against malware (like the Mélofée case study). AWS Network Firewall: Next-gen firewall with Deep Packet Inspection (DPI) capabilities. Stateless Engine: Fast filtering based on 5-tuple (IP/Port). Stateful Engine: Uses Suricata-compatible rules for Intrusion Prevention (IPS) and Domain filtering (FQDN) for Egress traffic. 3. Modern Network Architecture:\nUses AWS Transit Gateway with Native Network Firewall integration to simplify the network model, removing the complexity of routing through an \u0026ldquo;Inspection VPC\u0026rdquo;. Applies Active Threat Defense: Automatically syncs malicious IP lists from GuardDuty to Network Firewall for immediate blocking without manual intervention. PART 5: DATA PROTECTION Data is the ultimate asset that must be protected by encryption.\n1. Envelope Encryption:\nUnderstanding the AWS KMS mechanism: Master Key (resides in HSM) encrypts the Data Key, and the Data Key is what encrypts the actual data. This mechanism ensures high performance and absolute security. 2. Secrets Management:\nProblem: Hardcoding passwords in source code is a basic but common error. Solution: Use AWS Secrets Manager for storage and, more importantly, Automatic Rotation of Database passwords using Lambda. Applications always retrieve the latest password via API. 3. Infrastructure Encryption:\nUses AWS Nitro System: Encryption tasks are offloaded to specialized hardware (Nitro Cards), enabling data encryption without compromising the host server\u0026rsquo;s CPU performance (Zero Performance Impact). PART 6: INCIDENT RESPONSE When defense layers are breached, the response process determines the extent of the damage.\n1. Prevention Strategy (Sleep Better):\nGolden Rules: Eliminate long-lived SSH/Keys, Block Public S3 access, Default to Private Subnets. Infrastructure as Code (IaC): Mandate all infrastructure changes via Code (Terraform/CDK) and approval processes (PR Review), completely eliminating manual changes (ClickOps) that cause configuration drift. 2. Standard 5-Step Process:\nPreparation: Have tools and Playbooks ready. Detection: Rely on CloudTrail and GuardDuty. Containment: \u0026ldquo;Jail\u0026rdquo; infected resources by changing Security Groups or revoking IAM permissions. Eradication \u0026amp; Recovery: Remove malware, restore from clean backups. Post-Incident: Learn lessons. 3. Automation is King:\nHumans cannot race against machine speed. Hands-on labs demonstrated the necessity of using EventBridge + Lambda to automatically isolate compromised EC2 instances or auto-remediate public S3 buckets in seconds. Conclusion The \u0026ldquo;Cloud Security \u0026amp; Operations Mastery\u0026rdquo; series has provided a comprehensive overview of building secure systems on AWS through key pillars:\nGovernance \u0026amp; Identity: The foundation of every security system starts with strict user management and organizational policies. Network \u0026amp; Monitoring: Establishing defense-in-depth layers and comprehensive visibility to detect potential threats. Data \u0026amp; Response: Protecting digital assets with encryption and readying automated incident response processes to ensure service continuity. "
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/1-worklog/1.1-week1/",
	"title": "Week 1 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1 Objectives: Onboarding \u0026amp; Setup: Get familiar with FCJ culture, prepare the environment (VS Code, Git, CLI), and set up secure AWS accounts (IAM, MFA). Networking Foundations: Master VPC, Subnet, Route Table, NAT Gateway, and core networking components. Compute Services: Deploy EC2, establish SSH connections, and understand Global Infrastructure and VM types. Hybrid DNS \u0026amp; Connectivity: Set up Route 53 Resolver for Hybrid environments and configure Site-to-Site VPN labs. Compute Deep Dive: Dive deep into Auto Scaling, EFS, FSx, and other compute services. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - FCJ Member Onboarding - Read rules/regulations - Prepare workshop tools (VS Code, Hugo, \u0026hellip;) - Introduction to AWS \u0026amp; basic concepts 08/09/2025 08/09/2025 https://van-hoang-kha.github.io/vi/ 2 - Create new AWS Account - Activate MFA for AWS Account - Create Admin Group and Admin User - Support Account Verification - Explore and configure AWS Management Console - Create and Manage AWS Support Cases 09/09/2025 09/09/2025 https://000001.awsstudygroup.com/vi/ 3 - AWS Networking Services: + VPC, Subnet, Route Table + Internet Gateway, NAT Gateway + Security Group \u0026amp; NACL + VPC Peering, Transit Gateway (concept) + Elastic Load Balancing, Route 53 (DNS) 10/09/2025 10/09/2025 https://github.com/tuanvu250/AWS-FCJ/blob/main/module/module-02/note.md 4 - EC2 Practice: + Create EC2 instance + SSH Connection + Configure Site to Site VPN 11/09/2025 11/09/2025 https://000003.awsstudygroup.com/ 5 - Setup Hybrid DNS with Route 53 Resolver: + Connect to RDGW + Deploy Microsoft AD + Setup DNS 12/09/2025 12/09/2025 https://000010.awsstudygroup.com/ 6 - Setup VPC Peering + Create Peering connection between VPCs + Update Route tables and test connection - AWS Compute VM Services - Amazon Elastic Compute Cloud (EC2) - EC2 Auto Scaling - EFS/FSx - Lightsail - MGN - Understand AWS Global Infrastructure 13/09/2025 13/09/2025 https://000019.awsstudygroup.com/ https://github.com/tuanvu250/AWS-FCJ/blob/main/module/module-03/note.md Week 1 Achievements: Environment Ready: Completed tool setup, secured AWS account with Admin Group and MFA. Network Deployed: Successfully deployed VPC, Subnets, Security Groups, and understood network flows. Compute Launched: Successfully created and connected to EC2 instances, mastered purchasing options. Hybrid Connectivity: Successfully configured DNS Resolver and VPN for hybrid models. Knowledge Base: Full notes on Global Infrastructure, Regions, AZs, and extended Compute services. "
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/1-worklog/1.2-week2/",
	"title": "Week 2 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 2 Objectives: Advanced Networking: Deploy AWS Transit Gateway to manage routing between multiple VPCs. Storage Solutions: Hands-on with FSx for Windows and Storage Gateway for Hybrid environments. Data Protection: Establish automated AWS Backup strategies for resources. S3 Mastery: Master Amazon S3 (Static Web, CloudFront, Versioning, Replication). Compute Optimization: Optimize costs and performance with EC2 Instance Types and Purchasing Options. Tasks to be carried out this week: Day Task Start Date Completion Date Reference 1 - AWS Transit Gateway Practice: + Create Transit Gateway + Create Transit Gateway Attachments + Create Transit Gateway Route Tables + Add Transit Gateway Routes to VPC Route Tables 15/09/2025 15/09/2025 https://000020.awsstudygroup.com/ 2 - Deployment Practice: FSx for Windows: + Create SSD and HDD Multi-AZ file systems + Create File Shares, test and monitor performance + Enable deduplication and shadow copies + Enable storage quotas and continuous access sharing with throughput and storage scaling capacity; 16/09/2025 16/09/2025 https://000025.awsstudygroup.com/vi/ 3 - Deployment Practice: AWS Backup for systems: + Create S3 Bucket \u0026amp; deploy infrastructure + Create Backup plan, setup notifications + Verify operation 17/09/2025 17/09/2025 https://000013.awsstudygroup.com/vi/ 4 - Attend Vietnam Cloud Day 2025: Ho Chi Minh City Connect Edition for Builders (Track 1: GenAI \u0026amp; Data) - Explore EC2 Instance Types \u0026amp; Purchasing Options 18/09/2025 18/09/2025 https://pages.awscloud.com/vietnam-cloud-day-hcmc-connect-edition https://000051.awsstudygroup.com/vi/ 5 - Deployment Practice: AWS Storage Gateway: + Create S3 Bucket \u0026amp; EC2 for Storage Gateway + Create Storage Gateway and File Shares + Connect File shares on On-premise machine 18/09/2025 18/09/2025 https://000024.awsstudygroup.com/vi/ 6 - Practice Getting Started with Amazon S3: + Create S3 bucket and upload sample web data + Enable static website hosting and verify + Accelerate Static Website with Cloudfront + Bucket Versioning and Cross-Region Replication 20/09/2025 20/09/2025 https://000057.awsstudygroup.com/vi/ Week 2 Achievements: Network Hub: Successfully configured Transit Gateway for inter-VPC traffic routing. Hybrid Storage: Deployed FSx and Storage Gateway, successfully connected from On-premise. Robust Backup: Automated backup system using AWS Backup is stable and operational. S3 \u0026amp; CDN: Static website on S3 integrated with CloudFront and Replication is ready. Cost Efficiency: Mastered EC2 instance selection strategies and purchasing options for cost optimization. "
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/1-worklog/1.3-week3/",
	"title": "Week 3 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 3 Objectives: Advanced Storage: Master Amazon S3 (Classes, Lifecycle, Access Points, Glacier) and security features. Data Migration: Understand data migration solutions with Snow Family and Storage Gateway. VM Import/Export: Practice migrating On-premise VMs to AWS and vice versa with VM Import/Export. Resource Optimization: Optimize EC2 costs using Lambda and manage resources efficiently with Tags/Resource Groups. Backup Strategy: Deploy a comprehensive backup strategy for the system with AWS Backup. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - AWS Storage Services: + Amazon Simple Storage Service ( S3 ) + Access Point + Storage Class + S3 Static Website \u0026amp; CORS 22/09/2025 22/09/2025 https://github.com/tuanvu250/AWS-FCJ/blob/main/module/module-04/note.md 2 - AWS Storage Services: + Control Access + Object Key \u0026amp; Performance + Glacier + Snow Family - Storage Gateway - Backup 23/09/2025 23/09/2025 https://github.com/tuanvu250/AWS-FCJ/blob/main/module/module-04/note.md 3 - Deployment Practice: AWS Backup for systems: + Create S3 Bucket \u0026amp; deploy infrastructure + Create Backup plan, setup notifications + Verify operation 24/09/2025 24/09/2025 https://000013.awsstudygroup.com/vi/ 4 - VM Import/Export Practice: + Import VM to AWS + Deploy EC2 Instance from AMI + Export EC2 Instance from AWS 24/09/2025 24/09/2025 https://000014.awsstudygroup.com/vi/ 5 - Practice Optimizing EC2 Costs with Lambda: + Create Tag for Instance \u0026amp; Create Role for Lambda + Create Lambda Function \u0026amp; test result - Resource Management with Tags and Resource Groups - Manage access to EC2 Resource Tags with AWS IAM - Limit user permissions with IAM Permission Boundary - Explore S3 Lifecycle Policies \u0026amp; Intelligent-Tiering 26/09/2025 26/09/2025 https://000022.awsstudygroup.com/vi/ https://000027.awsstudygroup.com/vi/ https://000028.awsstudygroup.com/vi/ https://000030.awsstudygroup.com/vi/ 6 - Practice Getting Started with Amazon S3: + Create S3 bucket and upload sample web data + Enable static website hosting and verify + Accelerate Static Website with Cloudfront + Bucket Versioning and Cross-Region Replication 27/09/2025 27/09/2025 https://000057.awsstudygroup.com/vi/ Week 3 Achievements: Storage Expert: Deep understanding of S3 storage classes, lifecycle policies, and Intelligent-Tiering for cost optimization. Migration Ready: Clear grasp of using Snow Family and Storage Gateway for large data migration scenarios. VM Mobility: Successfully imported/exported VMs and deployed EC2s from custom AMIs. Cost Savings: Automated instance tagging and start/stop scheduling using Lambda to save costs. Governance: Strict access and resource management through Tags, Resource Groups, and IAM Permission Boundaries. "
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/1-worklog/1.4-week4/",
	"title": "Week 4 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 4 Objectives: Security Core: Master Shared Responsibility Model and identity management with IAM, Cognito, Organizations. Database Services: Master database services like RDS, Aurora, Redshift, and ElastiCache. Data Encryption: Practice comprehensive data encryption with AWS KMS integrated with S3 and CloudTrail. App Protection: Explore web application protection layers with AWS WAF, Shield, and Security Hub. Access Control: Deploy granular access control with IAM Role and Conditions. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - AWS Security Services: + Shared Responsibility Model + AWS Identity and Access Management + Amazon Cognito + AWS Organizations + Amazon Key Management Service 29/09/2025 29/09/2025 https://github.com/tuanvu250/AWS-FCJ/blob/main/module/module-05/note.md 2 -Getting Started with AWS Security Hub: + Enable Security Hub \u0026amp; review security standards - Explore AWS Shield \u0026amp; AWS WAF 30/09/2025 30/09/2025 https://000018.awsstudygroup.com/vi/ https://000053.awsstudygroup.com/vi/ 3 - AWS Database Services: + Amazon RDS \u0026amp; Amazon Aurora + Redshift - Elasticache 01/10/2025 01/10/2025 https://github.com/tuanvu250/AWS-FCJ/blob/main/module/module-05/note.md 4 - Encryption at Rest with AWS KMS: + Create Key Management Service \u0026amp; Amazon S3 + Create AWS CloudTrail \u0026amp; Amazon Athena + Test and share encrypted data on S3 - Practice IAM Role \u0026amp; Condition 02/10/2025 02/10/2025 https://000033.awsstudygroup.com/vi/ https://000044.awsstudygroup.com/vi/ 5 - Event [AWS GenAI Builder Club] AI-Driven Development Life Cycle: Reimagining Software Engineering (2pm Friday 3/10/2025) 03/10/2025 03/10/2025 6 - Practice Granting application access to AWS services with IAM Role - AWS Database Services: Database Concepts review 04/10/2025 04/10/2025 https://000048.awsstudygroup.com/ https://github.com/tuanvu250/AWS-FCJ/blob/main/module/module-06/note.md Week 4 Achievements: Security Proficiency: Clearly understood the shared responsibility model and secure IAM/Cognito configuration. Database Mastery: Differentiated and selected the right DB service (SQL/NoSQL/In-memory) for specific use cases. Encryption Lab: Successfully encrypted data with KMS and audited access via CloudTrail/Athena. Threat Protection: Enabled and evaluated security scores with Security Hub, understood WAF/Shield mechanisms. Granular Access: Granted precise access rights to applications and users via complex IAM conditions. "
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/1-worklog/1.5-week5/",
	"title": "Week 5 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 5 Objectives: Containerization: Master Docker, Docker Compose, and deploy containerized applications on AWS ECS. Data Lake Architecture: Build a complete Data Lake with S3, AWS Glue (Catalog), and Athena (Analytics). NoSQL Deep Dive: Hands-on deep dive with Amazon DynamoDB (Partition/Sort Key, GSI/LSI). DB Comparison: Detailed comparison of architecture and performance between Amazon RDS and Amazon Aurora. Analytics Optimization: Optimize costs and performance for big data queries. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Explore docker, docker-compose - Compare Amazon RDS vs Amazon Aurora 06/10/2025 06/10/2025 https://github.com/tuanvu250/AWS-FCJ/tree/main/bonus/docker https://docs.docker.com/get-started/ https://000054.awsstudygroup.com/vi/ 2 - Practice Deploying Applications on Docker with AWS - Practice Deploying Applications on Amazon Elastic Container Service - Practice DataLake on AWS + Collect and store data + Create Data Catalog (Amazon Glue)\n+ Analyze and visualize (Amazon Athena) 07/10/2025 07/10/2025 https://000015.awsstudygroup.com/vi/ https://000016.awsstudygroup.com/vi/ https://000005.awsstudygroup.com/vi/ https://000035.awsstudygroup.com/vi/ 3 - Practice Amazon DynamoDB Immersion Day 08/10/2025 08/10/2025 https://000039.awsstudygroup.com/vi/ 4 - Practice Amazon DynamoDB Immersion Day 09/10/2025 09/10/2025 https://000039.awsstudygroup.com/vi/ 5 - Practice Analyzing Cost and Performance with AWS Glue and Amazon Athena - Practice Working with Amazon DynamoDB 10/10/2025 10/10/2025 https://000040.awsstudygroup.com/vi/ https://000060.awsstudygroup.com/vi/ 6 - Practice Building Datalake with your own data 11/10/2025 11/10/2025 https://000070.awsstudygroup.com/vi/ Week 5 Achievements: Container Ops: Successfully packaged and managed Docker images on ECR and operated ECS clusters. Data Lake Live: Serverless data collection, storage, and analysis system is operational. DynamoDB Pro: Desinged efficient NoSQL tables, understood data models and performance metrics. DB Architecture: Clear analysis of pros/cons of RDS vs Aurora for decision making. Cost Effective: Applied partition and query optimization techniques to reduce data analysis costs. "
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/1-worklog/1.6-week6/",
	"title": "Week 6 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 6 Objectives: Learn about Nginx (reverse proxy, load balancing) and participate in \u0026ldquo;DATA SCIENCE ON AWS\u0026rdquo; workshop. Practice Serverless Architecture (API Gateway, Lambda, SAM) and Amazon Cognito (authentication, authorization). Deploy S3 Static Website with SSL, Microservices, and Single Page Application (SPA). Build CI/CD Pipeline with AWS Developer Tools, configure Github Actions and learn Container Security. Practice Amazon Kinesis (streaming data) and optimize data structure. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Practice deploying static website to AWS and configuring Github Actions with AWS IAM User - Practice configuring automated application release (AWS CodeStar, AWS CodePipeline, AWS CodeBuild, AWS CodeDeploy) 13/10/2025 13/10/2025 https://www.youtube.com/watch?v=oSZ0tzlkuWo https://000051.awsstudygroup.com/vi/ 2 - Practice CI/CD Pipeline: automate build, test, and deploy processes - Practice restructuring data and workflows 14/10/2025 14/10/2025 https://000051.awsstudygroup.com/vi/ https://000053.awsstudygroup.com/vi/ 3 - Learn about Nginx - Practice data streaming with Kinesis 15/10/2025 15/10/2025 https://github.com/tuanvu250/AWS-FCJ/tree/main/bonus/nginx https://000054.awsstudygroup.com/vi/ 4 - \u0026ldquo;DATA SCIENCE ON AWS\u0026rdquo; WORKSHOP – UNLOCKING DATA POWER WITH CLOUD COMPUTING - Convert Monolith to Microservices \u0026amp; Create Microservice - Build SPA with authentication - Container Security Best Practices 16/10/2025 16/10/2025 https://qhdn-hcmuni.fpt.edu.vn/2025/10/13/workshop-data-science-on-aws-mo-khoa-suc-manh-du-lieu-cung-dien-toan-dam-may/ https://000050.awsstudygroup.com/vi/ https://000055.awsstudygroup.com/vi/ 5 - Serverless - Guide to writing Front-end calling API Gateway - Serverless - Deploy application on SAM 17/10/2025 17/10/2025 https://000079.awsstudygroup.com/vi/ https://000080.awsstudygroup.com/vi/ 6 - Serverless - Authentication with Amazon Cognito - Serverless - Setup SSL S3 Static website 18/10/2025 18/10/2025 https://000081.awsstudygroup.com/vi/ https://000082.awsstudygroup.com/vi/ Week 6 Achievements: Successfully learned Nginx (reverse proxy, load balancing) and participated in \u0026ldquo;DATA SCIENCE ON AWS\u0026rdquo; workshop to expand data knowledge. Successfully practiced CI/CD Pipeline with AWS Developer Tools, automating build/test/deploy processes and configuring Github Actions. Completed labs on Serverless Architecture (API Gateway, Lambda, SAM), Amazon Cognito (Authentication), and S3 Static Website (SSL). Successfully converted Monolith to Microservices, built Single Page Application (SPA), and learned Container Security. Configured Amazon Kinesis for real-time data processing and completed data restructuring for performance optimization. "
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/1-worklog/1.7-week7/",
	"title": "Week 7 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 7 Objectives: AWS Cloud Practitioner Study: Review foundational knowledge following the kananinirav.com roadmap. Cloud Computing \u0026amp; IAM: Master cloud computing concepts and access management (Users, Groups, Roles). Compute \u0026amp; Storage Services: Deep dive into EC2, EBS, S3, EFS, and storage models. Networking \u0026amp; Global Infrastructure: Understand VPC architecture, Regions, AZs, and Edge Locations. Databases \u0026amp; Security: Explore Database services (RDS, DynamoDB) and the Shared Responsibility Model. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Cloud Computing: Deep research into cloud computing models (IaaS, PaaS, SaaS) and core AWS benefits. - IAM: Practice creating and managing Users, Groups, Roles; applying security policies (IAM Policies) and MFA. - EC2: Detailed analysis of Instance types (General Purpose, Compute Optimized,\u0026hellip;) and Pricing models (On-Demand, Spot, Reserved). 20/10/2025 20/10/2025 Cloud Computing IAM EC2 2 - EC2 Storage: Compare differences between EBS, Instance Store, and EFS; practice mounting EFS to multiple instances. - ELB \u0026amp; ASG: Configure Elastic Load Balancer (ELB) for load distribution and Auto Scaling Group (ASG) for automatic resource scaling. 21/10/2025 21/10/2025 EC2 Storage ELB \u0026amp; ASG 3 - S3: Explore Storage Classes (Standard, IA, Glacier), configure Versioning, and setup Lifecycle Policies for cost optimization. - Databases: Overview of managed database services: RDS (SQL), DynamoDB (NoSQL), ElastiCache, and Redshift. 22/10/2025 22/10/2025 S3 Databases 4 - Deploying: Understand Infrastructure as Code process with CloudFormation and application management with Elastic Beanstalk. - Global Infrastructure: Master concepts of Regions, Availability Zones (AZs), and Edge Locations to design high-availability systems. 23/10/2025 23/10/2025 Deploying Global Infrastructure 5 - Cloud Integration: Loosely coupled application integration using SQS (Message Queue) and SNS (Notification). - Cloud Monitoring: Monitor system metrics with CloudWatch, track user activity logs with CloudTrail, and manage configuration with AWS Config. 24/10/2025 24/10/2025 Cloud Integration Cloud Monitoring 6 - VPC: Design Private Cloud virtual network including Subnets, Route Tables, Security Groups, and NACLs to control network traffic. - Security \u0026amp; Compliance: Understand Shared Responsibility Model, use Inspector for security assessment, and Shield for DDoS protection. 25/10/2025 25/10/2025 VPC Security Compliance Week 7 Achievements: Solid Foundation: Completed the entire AWS Cloud Practitioner roadmap (Cloud Computing, IAM, Billing). Core Services Mastery: Deep understanding and effective use of Compute (EC2) and Storage (S3, EBS) services. Networking Competence: Mastered network architecture in AWS (VPC, Subnets, Security Groups). Security \u0026amp; Compliance: Clear understanding of the shared responsibility model and cloud security standards. Deployment \u0026amp; Monitoring: Familiarized with deployment tools (CloudFormation) and monitoring (CloudWatch). "
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/1-worklog/1.8-week8/",
	"title": "Week 8 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 8 Objectives: AWS Cloud Practitioner Study: Complete final modules and comprehensive review. Account Management \u0026amp; Security: Explore Billing, Cost Explorer, Organizations, and advanced identity services. Architecture \u0026amp; Ecosystem: Master Well-Architected Framework and Cloud Adoption Framework. Exam Preparation: Practice tests, mock exams, and complete the mid-term exam. Project Kickoff: Finalize requirements, technology stack, and officially kick off the project. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Account Management: Configure AWS Organizations for multi-account management, setup Billing dashboard and Cost Explorer for cost control. - Advanced Identity: Learn about Amazon Cognito for app authentication and AWS SSO for single sign-on. 27/10/2025 27/10/2025 Account Management Advanced Identity 2 - Other AWS Services: Explore other services like Amazon WorkSpaces (VDI), Amazon Connect (Contact Center), and Amazon Chime. 28/10/2025 28/10/2025 Other AWS Services 3 - Architecting \u0026amp; Ecosystem: Deep dive into Well-Architected Framework (6 pillars) and Cloud Adoption Framework (CAF) to standardize design processes. 29/10/2025 29/10/2025 Architecting 4 - Review \u0026amp; Self-study: Systematize all learned knowledge. Perform practice exams on AWS Skill Builder. 30/10/2025 30/10/2025 Practice Exam 5 - Practice Exam \u0026amp; Mid-term Exam: Take full-test practice exams (65 questions) for time management and complete the mandatory mid-term exam. 31/10/2025 31/10/2025 Practice Exam 6 - Project Kickoff: Organize official project kickoff meeting. Finalize business requirements (SRS), select Technology Stack, and create detailed plan (Gantt chart). - Calculate system costs 01/11/2025 01/11/2025 Week 8 Achievements: AWS Certified Cloud Practitioner: Completed 100% of course content and fully ready for the exam. Mid-term Exam: Passed mid-term exam with good results, solidifying foundational knowledge. Architecture Proficiency: Clear understanding of Well-Architected Framework for optimal system design. Advanced Tools Mastery: Proficient use of account management and advanced security tools. Project Readiness: Official project kicked off with high consensus on technology and processes. "
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/1-worklog/1.9-week9/",
	"title": "Week 9 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 9 Objectives: Project Initialization: Initialize project with Next.js 16, React 19, TypeScript, and configure ESLint, Husky. Design System Setup: Set up Neobrutalism design system, install Tailwind CSS and Shadcn UI. Authentication Flow: Integrate AWS Cognito, build Login/Register pages with Zod validation. Dashboard Development: Build responsive layout for User/Admin and core interface components. State Management \u0026amp; API: Configure Axios client, Redux/Zustand store, and integrate React Query. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Project Setup: Initialize project with Next.js 16.0.1 (App Router), React 19, and TypeScript. - Dev Tools: Configure Git hook process with Husky, control commit messages with Commitlint, and setup standard ESLint/Prettier. 03/11/2025 03/11/2025 Next.js 16, Husky 2 - Design System Setup: Install Tailwind CSS and configure Design Tokens for Neobrutalism style (bold, hard shadows). Customize Shadcn UI components and add Framer Motion for animation. 04/11/2025 04/11/2025 Framer Motion 3 - Authentication Flow: Integrate AWS Cognito Identity Provider. Build Login/Register/Forgot Password forms using React Hook Form combined with Zod validation. Connect authService. 05/11/2025 05/11/2025 AWS Cognito 4 - Dashboard Layout: Build separate Layouts for Group Route (member) and (admin). Implement complete responsive Sidebar navigation and Neobrutalist style system Widgets. 06/11/2025 06/11/2025 5 - API Client \u0026amp; Store: Setup Axios Interceptor (lib/api/core.ts) to attach Token and refresh token automatically. Configure Global State with Zustand (authStore) and integrate React Query (apiStore). 07/11/2025 07/11/2025 Zustand 6 - Wallet Management Start: Analyze Swagger API, implement fetchWallet service and useWallet custom hook. Build wallet list UI and create new wallet modal. - Weekly Review: Review directory structure and code conventions. 08/11/2025 08/11/2025 Week 9 Achievements: Development Environment: Successfully initialized Next.js 16/React 19 project, standard ESLint/Prettier configuration. Neobrutalism Design System: Synchronized setup of UI components, typography, and color palette. Authentication System: Completed registration/login flow with AWS Cognito and strict form validation. Dashboard UI: Operational responsive Dashboard interface for Admin and User. Foundation Layer: Completed configuration for API Client (Axios) and State Management (Zustand/TanStack Query). "
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/1-worklog/1.10-week10/",
	"title": "Week 10 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 10 Objectives: Wallet Management: Complete multi-wallet feature, display asset statistics chart. Jar System Implementation: Build backend/frontend for 6 Jars System. Budget Allocation Logic: Develop budget allocation logic and input data validation. Transaction Features: Create transaction form, history list, and search filters. AI Service Integration: Connect AI API and implement Voice-to-Transaction feature. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Wallet Management (Finish): Finalize total asset calculation logic, handle loading state/skeleton, and display asset allocation Pie Chart. - Jar System: Create components/jars, build fetchJars service. Implement full CRUD for the 6 financial jars system. 10/11/2025 10/11/2025 2 - Budget Allocation: Build Drag \u0026amp; Drop or Slider UI for allocating budget to jars. Implement total 100% validation logic with Zod schema. 11/11/2025 11/11/2025 3 - Transaction Creation: Build complex add transaction form (components/transactions): select wallet, jar, category, date, and note. Custom Select component for optimized UX. 12/11/2025 12/11/2025 date-fns 4 - Transaction History \u0026amp; Filters: Display transaction history list with Infinite Scroll or Pagination (React Query). Build advanced filters (by date, wallet, type). 13/11/2025 13/11/2025 TanStack Query 5 - Unit Testing: Write high-coverage unit tests for critical business logic in useWallet and useJars hooks using Jest/Vitest. 14/11/2025 14/11/2025 6 - AI Service \u0026amp; Voice Input: Implement useAIService connecting OpenAI/Gemini API. Build recording UI, handle Speech-to-Text conversion, and map data to transaction form. - Participate in AWS Cloud Mastery Series #1: GENERATIVE AI, RAG \u0026amp; AWS AGENTIC AI 15/11/2025 15/11/2025 Week 10 Achievements: Wallet Management Complete: Completed CRUD for wallets, displayed balance info and visual charts. Jar System Operational: 6 Jars system is operational with full management functionality. Budget Logic Implemented: Automated budget allocation algorithm works correctly. Transaction Flow: Create new, view list, and filter transactions features are complete. AI Integration Live: Voice input and transaction categorization suggestions are integrated. "
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/1-worklog/1.11-week11/",
	"title": "Week 11 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 11 Objectives: Complete AI features: Bill Scanning, Categorization, Analytics. Build Chatbot and Chatbot Management page. Optimize Mobile-First. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Bill Scanning (OCR): Use AI API to upload and analyze bill images. Extract information: Date, Total Amount, Product Names. - Participate in AWS Cloud Mastery Series #2: DevOps, IaC, Container \u0026amp; Observability 17/11/2025 17/11/2025 2 - AI Categorization \u0026amp; Suggestions: Build AI algorithm to automatically map transaction names to corresponding categories. Display suggestion UI for user confirmation or modification. 18/11/2025 18/11/2025 3 - Statistics \u0026amp; History: Build detailed statistics display (daily, weekly, monthly, yearly) and consolidated transaction history for each Wallet and Jar. 19/11/2025 19/11/2025 4 - Chatbot Page: Build chat interface (/services/chatbot) with ChatGPT-like layout. Integrate response streaming from askChatbot API and display markdown messages. 20/11/2025 20/11/2025 5 - Chatbot Management: Build Knowledge Base management module (/services/chatbot-management). Allow Admin to upload PDF/Docx to train chatbot and delete old context files. 21/11/2025 21/11/2025 6 - Mobile Optimization: Refine all CSS for mobile devices (Mobile-first). Increase touch target sizes, optimize drawer/modal layouts, and verify display on real devices. 22/11/2025 22/11/2025 Week 11 Achievements: AI \u0026amp; Chatbot: Completed full suite of smart features and virtual assistant. Mobile Ready: Application runs smoothly on mobile devices. Analytics: Visual reports help users grasp their financial situation. "
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/1-worklog/1.12-week12/",
	"title": "Week 12 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 12 Objectives: Production Deployment: Deploy to AWS S3 + CloudFront. Docker Deployment: Test deployment with Docker. Quality Assurance: UAT, Security Audit, Documentation, and Final Polish. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Static Export \u0026amp; AWS Deployment: Configure Next.js output: export. Upload static build to S3 Bucket. Configure CloudFront distribution pointing to S3, set up Custom Domain with Route 53 and ACM SSL certificate. 24/11/2025 24/11/2025 AWS S3 Hosting 2 - Docker Setup: Write multi-stage Dockerfile optimizing size and docker-compose.yml for production. Build image and test deployment on Staging EC2 environment. 25/11/2025 25/11/2025 Docker Docs 3 - UAT \u0026amp; Hotfixes: Open access for testers/users to run User Acceptance Testing (UAT). Monitor error logs, fix arising bugs, and refine caching policy. 26/11/2025 26/11/2025 4 - Security \u0026amp; Performance: Audit API keys security, configure Content Security Policy (CSP). Analyze bundle size, implement code splitting and lazy loading to improve Lighthouse scores. 27/11/2025 27/11/2025 5 - Documentation \u0026amp; Code Quality: Finalize API documentation (Swagger/Postman), update README.md with installation instructions. Run full linting and type-checking one last time to clean code. 28/11/2025 28/11/2025 6 - Final Polish \u0026amp; Preparation: Review overall UI/UX, ensuring no minor bugs remain. Package source code and prepare technical documentation for handover. - Participate in AWS Cloud Mastery Series #3: Cloud Security \u0026amp; Operations Mastery 29/11/2025 29/11/2025 Week 12 Achievements: Successful Deployment: Website runs stably on Production (AWS) and Docker. Finished Product: Meets all requirements, good security, high performance. Ready for Handover: Fully prepared documentation and quality source code. "
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/3-blogstranslated/3.1-blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": "Running and Optimizing Small Language Models On-Premises and at the Edge By Chris McEvilly, Fernando Galves Guy Ben Baruchn | Date: 23/06/2025\n| In Advanced (300), AWS Outposts, Technical How-to\nAs you move your generative AI deployments from the prototype stage to production, you may find the need to run foundation models (FMs) on-premises or at the edge to meet data residency requirements, information security (InfoSec) policies, or low latency needs. For example, customers in highly regulated industries such as financial services, healthcare, and telecom may want to leverage chatbots to support customer queries, optimize internal workflows for complex reporting, and automate request approvals - while keeping data within national borders. Similarly, some organizations choose to deploy their own small language models (SLMs) to align with strict internal InfoSec requirements. In another example, manufacturers might want to deploy SLMs right on their factory floors to analyze production data and provide real-time equipment diagnostics. To meet user data residency, latency, and InfoSec needs, this article provides guidance on deploying generative AI FMs to AWS Local Zones and AWS Outposts. The goal is to present a framework for running various types of SLMs to satisfy data processing requirements based on customer engagements.\nGenerative AI Deployment Options The growth of generative AI in deployment and testing has accelerated with two main enterprise deployment options. The first option is using a large language model (LLM) to meet business needs. LLMs have incredible versatility: a single model can perform completely different tasks, such as answering questions, coding, summarizing documents, translating languages, and content generation. LLMs have the potential to change how humans create content as well as how search engines and virtual assistants are used. The second deployment option is using small language models (SLMs), focused on a specific use case. SLMs are compact transformer models primarily using decoder-only or encoder-decoder architectures, generally having fewer than 20 billion parameters, although this definition is evolving as larger models emerge. SLMs can achieve comparable or even superior performance when fine-tuned for specific domains or tasks, making them an excellent alternative for specialized applications.\nAdditionally, SLMs offer faster inference times, lower resource requirements, and are suitable for deployment on a wider range of devices, which is particularly useful for specialized applications and edge computing where space and power are limited. Although SLMs have a more limited scope and accuracy compared to LLMs, you can enhance their performance for a specific task through Retrieval Augmented Generation (RAG) and fine-tuning. This combination creates an SLM capable of answering queries related to a specific domain with an accuracy level comparable to an LLM, while minimizing hallucinations. Overall, SLMs provide effective solutions that balance user needs and cost efficiency.\nArchitecture Overview The solution presented in this article uses Llama.cpp, an optimized framework written in C/C++ to efficiently run various types of SLMs. Llama.cpp can operate effectively in diverse computing environments, allowing generative AI models to function in Local Zones or Outposts without requiring massive GPU clusters often seen when running LLMs in their native frameworks. This framework expands model selection and increases operational performance when deploying SLMs to Local Zones and Outposts.\nThis architecture provides a template for deploying various types of SLMs to support use cases such as chatbots or content generation. The solution consists of a front-end application that receives user queries, formats prompts to present to the model, and returns responses from the model to the user. To support a scalable solution, application servers and Amazon EC2 G4dn GPU-enabled instances are placed behind an Application Load Balancer (ALB).\nIn cases where the volume of incoming prompts exceeds the processing capability of the SLMs, a message queue can be deployed in front of the SLMs. For example, you can deploy a RabbitMQ cluster to act as a queue manager for the system.\nFigure 1: Architecture overview\nSolution Deployment The following instructions describe how to launch an SLM using Llama.cpp in Local Zones or on Outposts. Although the previous architecture overview presented a complete solution with multiple components, this article focuses specifically on the steps necessary to deploy an SLM in an EC2 instance using Llama.cpp.\nPrerequisites To deploy this solution, you need to prepare the following:\nAn AWS account that has been allowlisted for Local Zones, or has a logical Outpost installed, configured, and operational. Access to G4dn instances in your account at the selected location (check in AWS Service Quotas). A VPC created to host the deployment environment. Public and private subnets to support the environment in the VPC. A security group associated with your EC2 instance. An AWS Identity and Access Management (IAM) role with AWS Systems Manager Session Manager permissions. 1. Launch GPU instance for SLM Log in to the AWS Management Console, open the Amazon EC2 console, and launch a g4dn.12xlarge EC2 instance in your Local Zone or Outposts environment.\nConfiguration includes:\nRed Hat Enterprise Linux 9 (HVM), SSD Volume Type Private subnet associated with the Local Zone or Outposts rack 30 GiB gp2 root volume and an additional 300 GiB gp2 EBS volume IAM role configured with necessary permissions for Systems Manager SSM Agent installed to connect to the instance (refer to instructions in Install SSM Agent on RHEL 8.x and 9.x in the Systems Manager User Guide) For detailed instructions on launching an EC2 instance, refer to: Launch an EC2 instance using the launch instance wizard in the console or Launch an instance on your Outposts rack.\nFigure 2: SLM instance launched\n2. Install NVIDIA drivers Connect to the SLM instance using Systems Manager. You can follow the instructions at Connect to your Amazon EC2 instance using Session Manager in the Amazon EC2 User Guide.\nInstall kernel packages and necessary tools:\nsudo su - dnf update -y subscription-manager repos --enable codeready-builder-for-rhel-9-x86_64-rpms dnf install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-9.noarch.rpm dnf install -y ccache cmake gcc-c++ git git-lfs htop python3-pip unzip wget dnf install -y dkms elfutils-libelf-devel kernel-devel kernel-modules-extra \\ libglvnd-devel vulkan-devel xorg-x11-server-Xorg systemctl enable --now dkms reboot Install Miniconda3 in the /opt/miniconda3 directory or another compatible package manager to manage Python dependencies.\nInstall NVIDIA drivers:\ndnf config-manager --add-repo \\ http://developer.download.nvidia.com/compute/cuda/repos/rhel9/x86_64/cuda-rhel9.repo dnf module install -y nvidia-driver:latest-dkms dnf install -y cuda-toolkit echo \u0026#39;export PATH=/usr/local/cuda/bin:$PATH\u0026#39; \u0026gt;\u0026gt; ~/.bashrc echo \u0026#39;export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH\u0026#39; \u0026gt;\u0026gt; ~/.bashrc source ~/.bashrc 3. Download and Install Llama.cpp Create and mount the filesystem of the Amazon EBS volume you created earlier to the /opt/slm directory. See instructions at Make an Amazon EBS volume available for use in the Amazon EBS User Guide.\nRun the following commands to download and install Llama.cpp:\ncd /opt/slm git clone -b b4942 https://github.com/ggerganov/llama.cpp.git cd llama.cpp cmake -B build -DGGML_CUDA=ON cmake --build build --config Release -j$(nproc) conda install python=3.12 pip install -r requirements.txt pip install nvitop 4. Download and Convert SLM model To run SLMs efficiently with Llama.cpp, you need to convert the model to the GGUF (GPT-Generated Unified Format) format. This conversion optimizes performance and memory usage for resource-constrained edge deployments. GGUF is designed specifically to work with the Llama.cpp inference engine. The following steps illustrate how to download SmolLM2 1.7B and convert it to GGUF format:\nmkdir /opt/slm/models cd /opt/slm/models git lfs install git clone https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct cd /opt/slm/llama.cpp python3 convert_hf_to_gguf.py --outtype f16 \\ --outfile /opt/slm/llama.cpp/models/SmolLM2-1.7B-Instruct-f16.gguf \\ /opt/slm/models/SmolLM2-1.7B-Instruct echo \u0026#39;export PATH=/opt/slm/llama.cpp/build/bin:$PATH\u0026#39; \u0026gt;\u0026gt; ~/.bashrc echo \u0026#39;export LD_LIBRARY_PATH=/opt/slm/llama.cpp/build/bin:$LD_LIBRARY_PATH\u0026#39; \u0026gt;\u0026gt; ~/.bashrc source ~/.bashrc You can also download other publicly available models from Hugging Face if needed, and perform a similar conversion process.\nSLM Operation and Optimization Deploying SLMs via Llama.cpp offers high operational flexibility, allowing environment customization and optimization for specific use cases. With Llama.cpp, you can choose various parameters to optimize system resource usage and model operation, effectively utilizing resources without unnecessary waste or impacting performance. Common parameters when running Llama.cpp to control model behavior include:\n-ngl N, --n-gpu-layers N: When compiling with GPU support, this option allows offloading some layers to the GPU for computation, increasing processing performance. -t N, --threads N: Determines the number of threads used during content generation. For optimal performance, set this value equal to the number of physical CPU cores in the system. -n N, --n-predict N: Determines the number of tokens to generate when creating text. Adjusting this value affects the output length of the text. -sm, --split-mode: Determines how to split the model across multiple GPUs when running in a multi-GPU environment. Try the \u0026ldquo;row\u0026rdquo; splitting mode, as in some cases it offers better performance than the default layer-based splitting. --temp N: Temperature controls the randomness in the SLM\u0026rsquo;s output. Lower values (e.g., 0.2-0.5) produce more consistent and deterministic responses, while higher values (e.g., 0.9-1.2) allow the model to be more creative and diverse (default: 0.88). -s SEED, --seed SEED: Provides a method to control model randomness. Setting a fixed seed helps reproduce consistent results across multiple runs (default: -1, -1 = random seed). -c, --ctx-size N: Determines the context size, the number of tokens the FM can process in a prompt. This value affects the required RAM and model accuracy. For example: with Phi-3, it is recommended to reduce context size to 8k or 16k to optimize performance. Sample command: --ctx-size XXXX where XXXX is the context size. This section illustrates how to optimize SLM performance for specific use cases using Llama.cpp, covering two common scenarios: Chatbot interactions and Text summarization.\nChatbot Use Case Example Token Size Requirements For chatbot applications, typical token sizes: Input: approximately 50-150 tokens, supporting 1-2 user questions, and Output: approximately 100-300 tokens, helping the model respond concisely but with detail.\nSample Command ./build/bin/llama-cli -m ./models/SmolLM2-1.7B-Instruct-f16.gguf \\ -ngl 99 -n 512 --ctx-size 8192 -sm row --temp 0 --single-turn \\ -sys \u0026#34;You are a helpful assistant\u0026#34; -p \u0026#34;Hello\u0026#34; Figure 3: Chatbot example\nCommand Explanation -m ./models/SmolLM2-1.7B-Instruct-f16.gguf : Specifies the model file to use -ngl 99 : Assigns 99 GPU layers for optimal performance -n 512 : Maximum 512 output tokens (sufficient for the needed 100-300 tokens) --ctx-size 8192 : Sets context window size to handle complex conversations -sm row : Splits across GPUs by row --temp 0 : Sets temperature to 0 to reduce creativity --single-turn : Optimized for single-turn responses -sys \u0026quot;You are a helpful assistant\u0026quot; : Sets system prompt defining the assistant role -p \u0026quot;Hello\u0026quot; : User prompt input Text Summarization Example The command line below shows SmolLM2-1.7B running a text summarization task:\nPROMPT_TEXT=\u0026#34;Summarize the following text: Amazon DynamoDB is a serverless, NoSQL database service that allows you to develop modern applications at any scale. As a serverless database, you only pay for what you use and DynamoDB scales to zero, has no cold starts, no version upgrades, no maintenance windows, no patching, and no downtime maintenance. DynamoDB offers a broad set of security controls and compliance standards. For globally distributed applications, DynamoDB global tables is a multi-Region, multi-active database with a 99.999% availability SLA and increased resilience. DynamoDB reliability is supported with managed backups, point-in-time recovery, and more. With DynamoDB streams, you can build serverless event-driven applications.\u0026#34; ./build/bin/llama-cli -m ./models/SmolLM2-1.7B-Instruct-f16.gguf \\ -ngl 99 -n 512 --ctx-size 8192 -sm row --single-turn \\ -sys \u0026#34;You are a technical writer\u0026#34; \\ --prompt \u0026#34;$PROMPT_TEXT\u0026#34; Figure 4: Summarization example\nCleaning Up To avoid unnecessary costs, perform the following steps to delete resources after completion:\nTerminate EC2 instance to stop charges. Verify that the 300 GiB EBS volume was deleted correctly by checking the Volumes section under Elastic Block Store. If the volume remains, select it and perform: Actions \u0026gt; Delete volume. Conclusion This article has guided you step-by-step through deploying SLMs to an AWS on-premises or edge environment to meet local data processing needs. The beginning of the article discussed the business benefits of SLMs, including: Faster inference time, reduced operational costs, and improved model outputs. SLMs deployed using Llama.cpp and optimized for specific use cases can deliver efficient user services from the edge in a scalable manner. The optimization parameters described in this article provide various configuration methods to tune the model for diverse deployment scenarios. You can follow the steps and techniques presented to deploy generative AI tailored to data residency, latency, or InfoSec compliance requirements, while operating efficiently within the resource constraints of edge computing environments. To learn more, visit AWS Local Zones and AWS Outposts.\n"
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": "AWS EUC New York Summit: EUC201 - The AI Advantage: Unlocking the full potential of your EUC Services By Dave Jaskie and Matt Aylward | Date: 27/06/2025 | Amazon AppStream 2.0, Amazon Bedrock, Amazon Bedrock Agents, Amazon CloudWatch, Amazon WorkSpaces, Desktop \u0026amp; Application Streaming, End User Computing.\nAre you looking to apply AI to optimize administrative tasks and enhance user productivity?\nIn the constantly evolving digital landscape, the success of an enterprise\u0026rsquo;s End-User Computing (EUC) strategy depends on the ability of end users to access and use services effectively. This interactive session will demonstrate how you can leverage Amazon Bedrock\u0026rsquo;s agentic AI combined with Amazon WorkSpaces and Amazon CloudWatch. These tools help automate administrative tasks and provide actionable insights from key metrics and logs.\nDuring the session, attendees will be introduced to critical EUC strategies and learn how AI can transform their workflows. You will discover how Amazon Bedrock helps simplify complex processes, giving administrators the tools they need to increase performance. Additionally, hands-on labs with Amazon CloudWatch will teach you how to collect crucial data - including user connectivity, platforms, and IP addresses. Through Amazon Bedrock, you will analyze data to derive insights that help optimize end-user operations.\nThis session not only provides in-depth knowledge but is also a practical learning experience. Attendees will gain a clear understanding of how to integrate AI and CloudWatch into their existing frameworks. Whether you are an IT professional, system administrator, or decision-maker, this is an opportunity to elevate your EUC strategy and ensure both admins and users benefit from optimized tools and processes.\nThis builders session takes place on July 16th at 9:15 AM EDT at the Javits Convention Center. Please add this session to your schedule via the following link after registering.\nDon\u0026rsquo;t miss the chance to change how you approach end-user computing. Join us to unlock the potential of AI, automate with confidence, and capture insights that help your organization move forward. Register today!\nDave Jaskie has 15 years of experience in the End User Computing field. Outside of work, Dave enjoys traveling and hiking with his wife and 4 children. Matt Aylward is a Solutions Architect at Amazon Web Services (AWS), specializing in creating simple solutions for complex business challenges. Before joining AWS, Matt worked in infrastructure deployment, disaster recovery testing, and virtual application delivery management. Outside of work hours, he enjoys spending time with his family, watching movies, and going on outings with his energetic dog. "
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/3-blogstranslated/3.3-blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": "How to Setup Automated Alerts for Newly Purchased AWS Savings Plans By Syed Muhammad Tawha and Dan Johns | Date: 26/06/2025 | Amazon Simple Notification Service (SNS), AWS Cloud Financial Management, AWS CloudFormation, Cloud Cost Optimization\nAs organizations grow, FinOps teams need a holistic view of AWS Savings Plans to optimize usage. This solution enables automated monitoring and setting up alerts to detect underutilized Savings Plans within their eligible return window.\nWhen purchasing a Savings Plan, you commit to usage for 1 or 3 years. Savings Plans with an hourly commitment of ≤ $100 can be returned if purchased within the last 7 days and in the same calendar month, provided you have not exceeded the return limit. After the month ends (UTC time), that Savings Plan cannot be returned.\nIn this article, we provide AWS CloudFormation templates that create an AWS Step Functions state machine, Amazon Simple Notification Service (SNS) topic, Amazon EventBridge scheduler, and necessary AWS Identity and Access Management (IAM) roles to automatically monitor newly purchased Savings Plans and detect underutilized ones.\nSolution Overview This solution follows AWS security best practices by separating deployment across two accounts. One CloudFormation stack will be created in the Management account to set up the necessary IAM roles to retrieve Savings Plans usage data. Another CloudFormation stack will be deployed in a chosen Member Account within your AWS Organization.\nThe CloudFormation stack in the Member Account creates a state machine that assumes a role in your Management Account and parses all Savings Plans in the Management Account, including those purchased across your entire organization. The workflow filters active Savings Plans by purchase date, focusing on plans purchased within the last 7 days and the current month. The system then evaluates their utilization rate and identifies plans below a predefined threshold.\nThe state machine will execute at the frequency you specify and use Amazon SNS to send email alerts to the addresses you provided when creating the CloudFormation stack. These alerts will contain details about underutilized Savings Plans and instructions on the return process.\nFigure 1: AWS Architecture - Member Account assumes read permissions from Management Account and triggers Step Function to send alerts via SNS.\nSolution Deployment Prerequisites An AWS Account IAM permissions to create CloudFormation Stack and IAM Role in the Management Account IAM permissions to create Step Functions, SNS, IAM Roles, and EventBridge in the Member Account Deploying the Solution In this section, we will deploy the resources for this solution in your account:\nPart 1 - Deploy in Member Account In this section, we will deploy resources for this solution in the chosen Member Account.\nLog in to the AWS Management Console of the Member Account where you want to deploy the solution.\nDeploy this CloudFormation Stack Launch Stack\nProvide the Stack Name as new-sp-utilization-alert-member.\nIn the AlertEmails parameter, enter a comma-separated list of emails that will receive notifications about underutilized Savings Plans.\nIn the ManagementAccountId parameter, enter the 12-digit AWS Account ID of the Management Account.\nIn the ScheduleExpression parameter, specify the execution frequency for the Step Functions state machine in cron format (default is daily at 9 AM UTC).\nIn the UtilizationThreshold parameter, specify the minimum utilization rate for your Savings Plans. You will receive a notification when utilization drops below this threshold.\nClick Next, check the acknowledgment box, and create the stack.\nWait until the stack completes and shows the status CREATE-COMPLETE.\nYou will receive an email to confirm subscription to notifications from the SNS topic created by this stack. Please confirm the subscription to start receiving notifications.\nAccess the Outputs tab of the created stack and record the values of ExecutionRoleArn and StateMachineArn, you will need them in the next section.\nPart 2 - Deploy in Management Account Log in to the AWS Management Console. Note: This must be the same account as entered in the ManagementAccountId parameter in the previous section.\nDeploy this CloudFormation Stack Launch Stack\nProvide the Stack Name as new-sp-utilization-alert-management.\nIn the ExecutionRoleArn parameter, provide the value copied from the stack outputs of the stack deployed in the Member Account.\nIn the StateMachineArn parameter, provide the value copied from the stack outputs of the stack deployed in the Member Account.\nClick Next, check the acknowledgment box, and create the stack.\nWait until the stack completes and shows the status CREATE-COMPLETE.\nTesting the Solution Now that the Step Functions state machine and related resources have been deployed in the Member Account, we will test the implementation:\nLog back in to the AWS Management Console of the Member Account where you deployed Part 1 of this solution.\nAccess the Resources tab in the CloudFormation stack and find the SavingsPlansAlerts Step Functions state machine. Click on the blue hyperlink.\nYou will be redirected to the Step Functions console. Click Start execution on the right.\nView execution details in the Events section to monitor the state machine\u0026rsquo;s progress. If there are Savings Plans purchased within the last 7 days and current month, you will receive an email notification.\nA successful execution is shown by a green box in the Graph view. If any Savings Plans fall below the specified utilization threshold, you will receive an email at the provided address.\nResource Cleanup All resources deployed for this solution can be deleted by deleting the CloudFormation stacks. You can delete the stack via the AWS Management Console or AWS CLI.\nTo delete the stack in the Management Account (CLI):\naws cloudformation delete-stack --stack-name new-sp-utilization-alert_management To delete the stack in the Member Account (CLI):\naws cloudformation delete-stack --stack-name new-sp-utilization-alert_member Understanding and Handling Alerts When you receive an alert about underutilized Savings Plans, you should review the usage details provided in the email notification. Analyze your utilization metrics against the original commitment when purchasing the Savings Plan, and investigate whether the low utilization was expected or due to other factors such as workload migration, architectural changes, or capacity demand miscalculations. Consider returning the Savings Plan if utilization remains below your threshold, the plan was purchased within the last 7 days, purchased in the current month, and has an hourly commitment ≤ $100. Record the reason for the return for future reference and planning.\nConclusion This article guided you on how to use Savings Plan and Cost Explorer APIs to identify underutilized Savings Plans in your organization. Then, we illustrated how to use a Step Functions State Machine to filter Savings Plans purchased within the last 7 days and current month, which is critical because you can return Savings Plans within the eligible return period if they are not used effectively. For more details on returning a Savings Plan, refer to the document Returning a Purchased Savings Plan\nSyed Muhammad Tawha Syed Muhammad Tawha is a Principal Technical Account Manager at AWS, based in Dublin, Ireland. Tawha specializes in Storage, Resilience, and Cloud Cost Optimization. He is passionate about helping AWS customers optimize costs and improve performance. Outside of work, Tawha loves spending time with friends and family. Dan Johns Dan Johns is a Senior Solutions Architect Engineer, supporting customers building on AWS and meeting business requirements. Outside of work, he enjoys reading, spending time with family, and automating household tasks. "
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/5-workshop/5.3-knowledge-base/5.3.1-create-kb/",
	"title": "Initialize Knowledge Base",
	"tags": [],
	"description": "",
	"content": "Target We will use the Amazon Bedrock Wizard to set up the entire RAG architecture. This process will connect the S3 data source, the Embedding model, and automatically initialize the Vector storage (OpenSearch Serverless).\nImplementation Steps Log in to the AWS Management Console and access the Amazon Bedrock service. In the left-hand menu, select Knowledge bases. Click the Create knowledge base button in the top right corner of the screen. Step 1: Configure Knowledge Base\nOn the first configuration screen:\nKnowledge base name: Enter knowledge-base-demo Knowledge Base description - optional: Enter Knowledge Base from AWS Overview (This section requires you to describe the data you have previously uploaded to S3). IAM permissions: Select the option Create and use a new service role. Service role name: Keep the default value suggested by AWS (starting with AmazonBedrockExecutionRoleForKnowledgeBase_...). Click Next. Step 2: Configure Data Source\nConnect to the S3 Bucket containing the documents:\nData source name: Enterknowledge-base-demo S3 URI:\nClick the Browse S3 button. In the pop-up window, select the bucket rag-workshop-demo you created in the previous section. Click Choose. Keep Default configurations. Click Next.\nStep 3: Storage \u0026amp; Processing\nThis is the most critical step to define the AI model and vector storage location:\nEmbeddings model:\nClick Select model. Select model: Titan Embeddings G1 - Text v2. Vector Store:\nVector store creation method: Choose Quick create a new vector store - Recommended Vector store type - new: Choose Amazon OpenSearch Serverless Note: This option allows AWS to automatically create an Amazon OpenSearch Serverless cluster to store data, saving you from manual infrastructure management. Click Next. Step 4: Review and Create Knowledge Base\nReview all configuration information on the Review page. Ensure the S3 URI and Model items are correct. Scroll to the bottom of the page and click the Create knowledge base button. Step 5: Wait for Initialization\nAfter clicking Create, the system will begin the background infrastructure initialization process for the Vector Store.\nWait time: Approximately 2 - 5 minutes. Note: Please do not close the browser during this time. Success: When the screen displays a green notification \u0026ldquo;Knowledge base created successfully\u0026rdquo;, you have completed this step and are ready for the next section. "
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report Student Information: Full Name: Uong Tuan Vu\nPhone Number: 0329 06 024\nEmail: vuutse180241@fpt.edu.vn\nUniversity: FPT University Campus Ho Chi Minh\nMajor: Software Engineering\nClass: SE180241\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 12/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/5-workshop/5.1-workshop-overview/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Overview In this workshop, we will focus on building an intelligent AI assistant capable of \u0026ldquo;reading and understanding\u0026rdquo; and answering questions based on proprietary enterprise data (RAG technique).\nThe main objective is to establish a fully automated and serverless data processing workflow, consisting of the following steps:\nIngestion: Loading source documents into the system. Indexing: Converting text into vectors and storing them for retrieval. Retrieval \u0026amp; Generation: Configuring the AI model to search for relevant information and generate answers to user questions. 💡 Highlight: This solution allows you to eliminate the need to manage any server infrastructure, optimizing costs and operational time.\nImplementation Steps RAG Explanation Service Introduction "
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/5-workshop/5.2-prerequiste/5.2.1-model-access/",
	"title": "Verify Model Access",
	"tags": [],
	"description": "",
	"content": "Overview According to AWS\u0026rsquo;s new policy, Foundation Models are often automatically enabled. However, for third-party partner models like Anthropic (Claude), first-time users in a new Region must declare usage information (Use Case) to be able to invoke the model.\nEnsure your AWS account has permission to access and use the Anthropic Claude 3 Sonnet model. This is a mandatory step to avoid AccessDenied errors when the Chatbot operates later. If this is the first time you are using this model in a new Region, you need to declare the intended use (Use Case).\nAccess Check We will perform a quick test (Test Run) to ensure your account is ready.\nIn the search bar, access the Amazon Bedrock.\nStep 1. Access Chat Playground\nIn the left menu of the Bedrock Console, find Playgrounds. Click Chat. Step 2. Select Test Model\nClick Select model (above the chat box). Category: Select Anthropic. Model: Select Claude 3 Sonnet (or Claude 3.5 Sonnet). Throughput: Select On-demand. Click Apply. Step 3. Send Activation Message\nIn the chat box: Enter Hello. Click Run. Observe result: If AI answers: Success (Proceed immediately to section 5.2.2). If a red error or \u0026ldquo;Submit use case details\u0026rdquo; popup appears: Information declaration required (Continue to step 4 below). Step 4. Submit Use Case (Only perform if error occurred in step 3)\nClick Submit use case details (in the error message). Fill in the form: Company Name: Enter Personal Learning. Industry: Select Technology. Intended Use: Select Research \u0026amp; Development. Click Submit. Wait 1 minute, return to the chat box, Click Run on the Hello message again to confirm success. "
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/5-workshop/5.1-workshop-overview/5.1.1-whatisrag/",
	"title": "What is Retrieval-Augmented Generation (RAG)",
	"tags": [],
	"description": "",
	"content": "Brief Definition RAG (short for Retrieval-Augmented Generation) is a technique or software architecture in the field of Artificial Intelligence (AI), designed to optimize the output of a Large Language Model (LLM).\nIn essence, RAG is a combination of two mechanisms:\nInformation Retrieval Mechanism: Searching for data from a highly reliable External Knowledge Base. Text Generation Mechanism: Using the LLM\u0026rsquo;s language understanding and synthesis capabilities to generate natural responses. The goal of RAG is to provide the LLM with accurate, up-to-date, and specific context, helping the model overcome the limitations of static training data.\nWhy is RAG needed? Traditional LLM models often face 3 major problems that RAG can solve:\nInformation Updates (Freshness): The LLM does not need Re-training or Fine-tuning yet can still answer the latest information, simply by updating the search database. Data Ownership (Proprietary Data): Allows AI to answer questions regarding private enterprise data (internal documents, code base, customer information) that the original model does not know. Authenticity (Grounding): Minimizes \u0026ldquo;Hallucination\u0026rdquo; (AI fabricating information) by forcing the AI to cite or rely on actual text passages found. Operational Architecture The process of handling a question in RAG proceeds as follows:\nStep Name Action Description 1 Retrieval (Truy xuất) The system searches for text segments most relevant to the question in the data repository (usually using a Vector Database). 2 Augmentation (Tăng cường) Combine the user\u0026rsquo;s question + The data just found into a complete \u0026ldquo;prompt\u0026rdquo;. 3 Generation (Tạo sinh) Send that prompt to the AI (LLM) for it to synthesize and write out the final answer for the user. "
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": "On this page, you will need to introduce your worklog. How did you complete it? How many weeks did you take to complete the program? What did you do in those weeks?\nTypically, and as a standard, a worklog is carried out over about 3 months (throughout the internship period) with weekly contents as follows:\nWeek 1: Getting familiar with AWS, Networking, and Compute Services\nWeek 2: Advanced Networking, Hybrid Storage, and Backup Strategies\nWeek 3: Advanced Storage, Data Migration, and Cost Optimization\nWeek 4: Security Core, Database Services, and Encryption\nWeek 5: Containerization, Data Lake, and NoSQL Deep Dive\nWeek 6: Nginx, Serverless, Microservices, and CI/CD Automation\nWeek 7: AWS Cloud Practitioner Study and Core Services Review\nWeek 8: Exam Preparation, Advanced Services, and Project Kickoff\nWeek 9: Project Init: Next.js, Design System, and Authentication\nWeek 10: Core Features: Wallet, Jars System, and Budget Logic\nWeek 11: AI Integration, Chatbot, and Advanced Analytics\nWeek 12: Production Deployment (AWS/Docker) and Final Polish\n"
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/5-workshop/5.3-knowledge-base/5.3.2-sync-data/",
	"title": "Check Vector Store and Data Synchronization",
	"tags": [],
	"description": "",
	"content": "Target Before the AI can answer, data must be ingested into the vector storage (Vector Store). We will perform a \u0026ldquo;Before and After\u0026rdquo; check to clearly see how Bedrock automatically encodes and stores data into OpenSearch.\nImplementation Steps Step 1: Check Vector Store (Empty State)\nWe will directly access Amazon OpenSearch Serverless to confirm that no data exists yet.\nIn the AWS Console search bar, type Amazon OpenSearch Service and select Amazon OpenSearch Service. In the left menu, under Serverless, select Collections. Click on the Collection name newly created by Bedrock (usually named like bedrock-knowledge-data...). On the Collection details page, click the Open Dashboard button (located at the top right of the screen).\nNote: If asked to log in, use your current AWS credentials. In the OpenSearch Dashboard interface: Click the Menu (3 horizontal lines) icon in the top left corner. Select Dev Tools (usually located at the bottom of the menu list). In the Console pane (on the left), enter the following command to check data: GET _search\r{\r\u0026quot;query\u0026quot;: {\r\u0026quot;match_all\u0026quot;: {}\r}\r}\rClick the Play (Run) button (small triangle next to the command line).\nResult: Observe the right pane, hits -\u0026gt; total -\u0026gt; value is 0.\nStep 2: Sync Data\nNow we will trigger Bedrock to read files from S3 and load them into OpenSearch.\nReturn to the Amazon Bedrock tab on the browser. Select Knowledge bases in the left menu and click on the KB name you just created. Scroll down to the Data source section, check the box (tick) next to the data source name (s3-datasource). Click the Sync button (Orange). Wait: This process will take 5 - 10 minutes depending on the sample document size. Wait until the Sync status column changes from Syncing to Available. Step 3: Re-check Vector Store (Populated)\nAfter Bedrock reports Sync completion, we return to the repository to verify the data has been successfully ingested.\nSwitch to the OpenSearch Dashboard tab (still open from Step 1). In Dev Tools, click the Play (Run) button again with the old command: GET _search\r{\r\u0026#34;query\u0026#34;: {\r\u0026#34;match_all\u0026#34;: {}\r}\r} Result: The hits -\u0026gt; total -\u0026gt; value section will be greater than 0 (e.g., 10, 20\u0026hellip; depending on the number of text chunks). You will see details of the vectors (number arrays) and text content stored in the _source field. Congratulations! You have completed building the \u0026ldquo;brain\u0026rdquo; for the AI. The data has been encoded and sits safely in the Vector Database, ready for retrieval.\n"
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/5-workshop/5.2-prerequiste/5.2.2-prepare-data/",
	"title": "Prepare source data",
	"tags": [],
	"description": "",
	"content": "Overview Initialize an object storage (S3 Bucket) to hold original documents (PDF, Word, Text). This acts as the \u0026ldquo;Source of Truth\u0026rdquo; that the Knowledge Base will access to read, analyze, and synchronize knowledge for the AI. You can store knowledge related to your field to use in creating your own personal assistant or Chatbot.\nData Preparation We will create an S3 Bucket to store original documents, acting as the knowledge source for the Chatbot.\nStep 1. Create S3 Bucket\nAccess the S3 service from the search bar. AWS Region: Select United States (N. Virginia us-east-1). Click Create bucket. Configure Bucket information: Bucket Type: Click General purpose Bucket name: Enter rag-workshop-demo Object Ownership: Keep default ACLs disabled. Block Public Access settings: Keep default (Selected Block all public access). Scroll to the bottom of the page, Click Create bucket. Check that the S3 Bucket has been created successfully. Step 2. Upload sample documents\nThis is a sample document, relevant to overview of AWS cloud computing knowledge. You can use it to run demos or upload your data. PDF format file\nIn the Buckets list, Click on the bucket name you just created. Click Upload. In the Upload interface:\nClick Add files. Select the sample document file attached above or a file from your computer (PDF or Word file with a lot of text is recommended). Scroll to the bottom of the page, Click Upload.\nWhen you see the green \u0026ldquo;Upload succeeded\u0026rdquo; notification, Click Close. "
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/5-workshop/5.2-prerequiste/",
	"title": "Prerequiste",
	"tags": [],
	"description": "",
	"content": "Objectives Before building the application, we need to establish a solid foundation. Similar to preparing ingredients before cooking, this section ensures your AWS account is ready with the necessary permissions and data.\nIn this section, we will complete 3 key initialization objectives:\nSelect Region: Set up the working environment in the United States N. Virginia (us-east-1) region to optimize connection speed and ensure service availability. Enable Model (Model Access): Check and ensure the account has permission to invoke the Anthropic Claude 3 model – the main linguistic \u0026ldquo;brain\u0026rdquo; of the system. Prepare Data (Data Setup): Initialize storage (S3 Bucket) and upload source documents to serve the knowledge ingestion process later. Key Components In this preparation section, we will interact with the following components:\nAWS Management Console (Region Selector): The general management interface to switch the working Region to United States N. Virginia. Amazon Bedrock (Model Access \u0026amp; Playground): The place to manage access to Foundation Models and the chat tool to quickly test AI response capabilities. Amazon S3 (Simple Storage Service): Object storage service where we will create a Bucket to hold original document files (PDF, Word, Text). Implementation Steps Check Model Access Prepare Source Data "
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "Personal Finance Management App (Vicobi) You can read the full proposal here: Vicobi Proposal 1. Executive Summary The Vicobi (Personal Finance Management App) project aims to provide an intelligent, modern, and highly automated personal financial management platform. Vicobi simplifies financial management through 4 main pillars:\nSmart Recording: Voice input and invoice scanning, eliminating manual data entry barriers. Goal-based Budgeting: Automated creation and flexible management of money jars. Analysis \u0026amp; Control: Provides visual reports and intelligent alert systems. AI Financial Assistant (Chatbot): Integrates AI Chatbot acting as an advisor, supporting inquiries and enhancing financial knowledge. From a technical perspective, Vicobi is built on a Microservices architecture using .NET Aspire and FastAPI, deployed on AWS Cloud, ensuring flexibility and data security. The development process follows the Agile/Scrum model (2 weeks/sprint during the main development phase), with the goal of completing MVP within 2 months of execution.\n2. Problem Statement Current Problem In today\u0026rsquo;s dynamic market, users face difficulties in controlling finances due to \u0026ldquo;behavioral inertia\u0026rdquo; — reluctance to manually record each transaction. Existing applications (like Money Lover, Misa Money Keeper) still rely heavily on manual input, causing \u0026ldquo;input fatigue\u0026rdquo; and high abandonment rates.\nSolution Vicobi solves the problem through high automation of the data entry process using AWS Cloud and Microservices:\nCore Technology: Integrates AI for Vietnamese voice processing (Voice-to-Text) and detailed invoice recognition (OCR). Optimized Architecture: Uses AWS ECS Fargate running Multi-container Task model (combining .NET Backend and AI Service) to reduce infrastructure costs while ensuring seamless communication. Modern Frontend: Uses Next.js hosted on Amazon S3 and distributed globally via Amazon CloudFront. Benefits and Return on Investment (ROI) The solution provides clear competitive advantages:\nUser Value: Reduces over 70% manual operations. Voice recognition accuracy reaches 90% and invoice extraction reaches 80%. Economic Efficiency: Maximizes AWS Free Tier usage (S3, CloudFront, Cognito). Lean operating budget around ~$60/month for infrastructure and ~$15/month for AI compute. ROI: Expected to achieve ROI within 6–12 months thanks to time savings and increased efficiency. Scalability: Microservices architecture ready for Mobile App integration or Open Banking. 3. Solution Architecture The system is designed with a distributed Microservices model, using API Gateway as the single entry point.\nTech Stack Details: Component Technology Details Frontend Next.js 16 App Router, TypeScript, Tailwind CSS, Zustand, React Query. Backend Core .NET Aspire Orchestrates Microservices (User, Wallet, Transaction, Report, Notification). AI Service FastAPI (Python) Handles Voice (PhoWhisper), OCR (Bedrock), Chatbot (RAG). Database Polyglot PostgreSQL, MongoDB, Elasticsearch, Qdrant (Vector DB). Messaging RabbitMQ Asynchronous communication between services. AWS Workflow: Access: Users access via Route 53, protected by AWS WAF and accelerated by CloudFront. Authentication: Amazon Cognito manages identity and issues JWT Tokens. API Processing: Requests go through API Gateway, connecting securely via AWS PrivateLink to Application Load Balancer (ALB). Compute: ALB distributes load to containers in ECS Fargate (located in Private Subnet). DevOps: CI/CD process fully automated by GitLab, builds images pushed to Amazon ECR and updates tasks on ECS. 4. Technical Deployment Implementation Phases The project spans 4 months (including internship):\nMonth 0 (Pre-internship): Ideation and overall planning. Month 1 (Foundation): Learn AWS, upgrade .NET/Next.js/AI skills. Set up VPC, IAM. Month 2 (Design): Design High-level \u0026amp; Detailed architecture on AWS. Month 3-4 (Realization): Coding, Integration Testing, Deploy to AWS Production, set up Monitoring. After Month 5: Research and develop Mobile App. Detailed Technical Requirements: Frontend: Deploy Next.js 16 on S3 + CloudFront. Use Origin Access Control (OAC) to secure bucket. Backend: Use .NET Aspire to manage Cloud-native configuration. Database-per-service: PostgreSQL \u0026amp; MongoDB. Elasticsearch for complex transaction search. Background Jobs: Use Hangfire. AI Service Pipelines: Voice: Preprocessing with Pydub, PhoWhisper-small Model (VinAI) for Vietnamese. OCR: Amazon Bedrock (Claude 3.5 Sonnet Multimodal) to accurately extract invoice information. Chatbot (RAG): Knowledge Base stored in Qdrant, generates responses via Amazon Bedrock (Claude 3.5 Sonnet). Security: Data encryption in transit (HTTPS/TLS 1.2+) and at rest (AES-256). Secrets management not deeply integrated (currently at MVP level), will upgrade to AWS Secrets Manager in the future. 5. Timeline \u0026amp; Milestones (Sprints) The main execution phase is divided into 4 Sprints:\nSprint 1: Core Foundation Authentication (Cognito), Wallet Management, Spending Jars. Sprint 2: Core Features Transactions (CRUD), AI Voice Processing. Sprint 3: Analytics Reports/Charts, Notification System (SES), Message Broker. Sprint 4: Stabilization Integration Testing, UI Refinement, Deploy to AWS ECS \u0026amp; CloudFront. Testing \u0026amp; Go-live: Domain Configuration, SSL, Monitoring Dashboard, UAT and project defense. 6. Budget Estimation Based on detailed cost estimates for the MVP phase.\nYou can review the detailed cost estimation by downloading the following files: 📊 CSV Pricing File 💾 JSON Pricing File\nAWS Service Component / Usage Cost (USD/month) Elastic Load Balancing Application Load Balancer $18.98 Amazon ECS Fargate (vCPU \u0026amp; Memory) $17.30 Amazon VPC VPC Endpoints \u0026amp; NAT $10.49 AWS WAF Web ACL \u0026amp; Requests $7.20 Amazon API Gateway API Calls \u0026amp; Data Transfer $2.50 Amazon CloudFront Data Transfer Out $2.00 Amazon ECR Storage $1.00 Amazon Route 53 Hosted Zones $0.54 Amazon S3 Standard Storage $0.34 TOTAL AWS COST ~$60.35 Other Costs:\nCategory Details Cost (USD/month) AI Compute / Tooling Gemini API, Amazon Bedrock ~$15.00 PROJECT TOTAL ~$75.35 / month (Based on On-Demand pricing in Singapore region - ap-southeast-1)\n7. Risk Assessment Main Risks: User information leakage (Impact: High), AWS Region connection loss (Impact: High), AI misrecognition (Impact: Medium). Mitigation Strategies: Security: AES-256 encryption, HTTPS, IAM Least Privilege, AWS WAF. High Availability: Multi-AZ deployment for ECS and ALB. AI: Continuously improve model with real data. Resilience: Use internal RabbitMQ for asynchronous processing and retry. Disaster Recovery Plan: Use IaC (Infrastructure as Code) for rapid infrastructure restoration. 8. Expected Results \u0026amp; Team Expected Results of the Project Automated financial data entry: The application helps users avoid manual entry, just take a photo of the invoice or record a voice for the system to automatically classify spending. Intuitive financial management: Users can view spending charts, monthly reports, and receive savings suggestions based on consumer behavior. Minimal user experience: Friendly web interface, modern design, optimized for mobile devices and suitable for people new to financial management. Stable, scalable system: Microservices architecture makes it easy to add new features such as spending reminders, AI predictive analysis, or expand to a mobile app. Improving development team skills: Project members have practical access to DevOps processes, CI/CD implementation, and cloud-based application optimization. Project Limitations Vietnamese AI model is still limited: The ability to recognize regional voices or handwritten invoices has not yet achieved high accuracy. No separate mobile application: The MVP version only supports the web platform, there is no native mobile app. Implementation Team: Name Role Email Le Vu Phuong Hoa Backend Developer (Leader) hoalvpse181951@fpt.edu.vn Nguyen Van Anh Duy AI Developer (Member) duynvase181823@fpt.edu.vn Uong Tuan Vu Frontend Developer (Member) vuutse180241@fpt.edu.vn Tran Nguyen Bao Minh AI Developer (Member) baominhbrthcs@gmail.com Mentor Support:\nNguyen Gia Hung - Head of Solution Architects Van Hoang Kha - Cloud Security Engineer "
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/5-workshop/5.1-workshop-overview/5.1.2-services/",
	"title": "Services",
	"tags": [],
	"description": "",
	"content": "The solution architecture is built upon the coordination of the following 4 key service components:\nKnowledge Bases for Amazon Bedrock This is a fully managed capability that helps connect Foundation Models to the enterprise\u0026rsquo;s internal data sources.\nRAG workflow automation: Manages the entire end-to-end workflow, including ingestion, chunking, embedding, and retrieval. Contextual connection: Enables AI applications to answer questions based on private data rather than relying solely on generic training data. No infrastructure management: Eliminates the need to build and maintain complex data pipelines. Amazon Simple Storage Service (Amazon S3) An object storage service with scalability, 99.999999999% (11 nines) data durability, and top-tier security.\nData Source Role: Acts as the \u0026ldquo;source of truth\u0026rdquo;. Document storage: Contains unstructured files such as PDF, Word, or Text that the business wants the AI to learn. Synchronization: The Knowledge Base will periodically scan this S3 bucket to synchronize and update the latest knowledge. Amazon OpenSearch Serverless A serverless deployment option for Amazon OpenSearch Service that helps run search and analytics workloads without managing clusters.\nVector Store Role: Stores vector embeddings generated from original documents. Semantic Search: Performs similarity search algorithms (k-NN) to identify text segments with meanings closest to the user\u0026rsquo;s question. Auto-scaling: Automatically adjusts compute and storage resources based on actual demand. Amazon Bedrock Foundation Models (FMs) Provides access to leading AI models via a unified API. In this architecture, we use two types of models with distinct roles:\nEmbedding Model (Amazon Titan Embeddings v2): Converts text (documents from S3 and user questions) into numerical vectors. Enables computers to compare semantic similarity between text segments. Text Generation Model (Anthropic Claude 3): Acts as the reasoning \u0026ldquo;brain\u0026rdquo;. Receives the question along with contextual information retrieved from the Vector Store. Synthesizes information and generates natural, accurate answers with source citations. "
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/5-workshop/5.3-knowledge-base/",
	"title": "Create and Configure Knowledge Base",
	"tags": [],
	"description": "",
	"content": "Objectives After completing the environment and data preparation, the next step is to set up the core component of the RAG architecture. In this section, we will initialize the Knowledge Base, acting as an intelligent intermediary mechanism that connects unstructured data sources with the reasoning capabilities of foundation models.\nWe will accomplish 3 key technical objectives:\nEstablish an Automated Pipeline: Configure the Knowledge Base to automate the entire RAG data processing workflow (including extraction, text chunking, and vector creation) to eliminate manual processing tasks. Initialize Vector Store: Deploy a collection on Amazon OpenSearch Serverless to store semantic vectors, serving accurate and efficient information retrieval. Data Synchronization (Data Ingestion): Perform the initial data ingestion process, converting static documents from S3 into searchable vectors within the system. Key Components During this configuration process, we will interact with and connect the following services:\nKnowledge Bases for Amazon Bedrock: A managed service acting as the orchestrator of data flow, connecting information sources, and executing semantic queries. Amazon Titan Embeddings G1 - Text v2: A specialized model for converting text data into numerical vectors (Embeddings) with high accuracy and multi-language support. Amazon OpenSearch Serverless: A fully managed vector database responsible for storage and executing similarity search algorithms (k-NN). Implementation Steps Initialize Knowledge Base Check Vector Store and Sync Data "
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/3-blogstranslated/",
	"title": "Translated Blogs",
	"tags": [],
	"description": "",
	"content": "Blog 1 - Running and Optimizing Small Language Models On-Premises and at the Edge This blog provides a comprehensive guide on deploying Small Language Models (SLMs) to AWS on-premises or edge environments (AWS Local Zones and AWS Outposts) to meet requirements for data residency, information security policies, and low latency. The content covers: differences between LLMs and SLMs, benefits of SLMs (faster inference time, lower resource requirements, suitable for edge computing), deployment architecture using the Llama.cpp framework, specific steps for installation and configuration (launching GPU instances, installing NVIDIA drivers, installing Llama.cpp, downloading and converting models to GGUF format), and optimization examples for chatbot and text summarization use cases.\nBlog 2 - AWS EUC New York Summit: The AI Advantage: Unlocking the Full Potential of Your EUC Services This blog introduces a discussion session at the AWS EUC New York Summit, focusing on how to apply AI to optimize End-User Computing (EUC) administrative tasks and enhance user productivity. Main content includes: leveraging Amazon Bedrock\u0026rsquo;s agentic AI combined with Amazon WorkSpaces and Amazon CloudWatch to automate administrative tasks, providing actionable insights from critical metrics and logs, collecting data on user connectivity, platforms, and IP addresses, and analyzing data to optimize end-user operations. The builders session provides in-depth knowledge and hands-on learning experience on integrating AI into existing EUC frameworks.\nBlog 3 - How to Set Up Automated Alerts for Newly Purchased AWS Savings Plans This blog provides a guide on setting up an automated monitoring and alerting system to detect underutilized AWS Savings Plans within the eligible refund period (first 7 days and within the same purchase month). Content includes: explanation of AWS Savings Plans and refund policy (commitments ≤ $100/hour), solution architecture using AWS CloudFormation, Step Functions, SNS, EventBridge, and IAM roles, deployment across two accounts (Management Account and Member Account) following AWS security best practices, automated process to analyze Savings Plans utilization rates and send email alerts when detecting plans below predefined thresholds, detailed step-by-step deployment instructions and solution testing procedures.\n"
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/4-eventparticipated/",
	"title": "Events Participated",
	"tags": [],
	"description": "",
	"content": "During my internship, I participated in six events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: Vietnam Cloud Day 2025 : Ho Chi Minh City Connect Edition for Builders (Track 1: GenAI \u0026amp; Data)\nDate \u0026amp; Time: 09:00, September 18, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: GenAI-powered App-DB Modernization workshop\nDate \u0026amp; Time: 09:00, August 13, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 3 Event Name: Workshop Data Science on AWS\nDate \u0026amp; Time: 09:00, October 16, 2025\nLocation: FPT University HCMC Campus\nRole: Attendee\nEvent 4 Event Name: AWS Cloud Mastery Series #1: GENERATIVE AI, RAG \u0026amp; AWS AGENTIC AI\nDate \u0026amp; Time: 08:30, November 15, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 5 Event Name: AWS Cloud Mastery Series #2: DevOps, IaC, Container \u0026amp; Observability\nDate \u0026amp; Time: 08:30, November 17, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 6 Event Name: AWS Cloud Mastery Series #3: Cloud Security \u0026amp; Operations Mastery\nDate \u0026amp; Time: 08:30, November 29, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/5-workshop/5.4-test-chatbox/",
	"title": "Testing Chatbot (RAG)",
	"tags": [],
	"description": "",
	"content": "Target After successfully ingesting data into the Vector Store, it is time to verify the results. In this section, you will act as an end-user, asking the Chatbot questions directly within the AWS Console interface to observe how the RAG system operates.\nWe will focus on 2 factors:\nAccuracy: Does the AI answer correctly based on the documents? Transparency: Can the AI cite the source (Citation) of the information? Implementation Steps Step 1: Configure the test window\nTo start chatting, we need to select a Foundation Model that will act as the \u0026ldquo;responder\u0026rdquo;.\nIn your Knowledge Base details interface, look at the right panel titled Test knowledge base. Click the Select model button.\nIn the selection panel that appears: Category: Select Anthropic. Model: Select Claude 3 Sonnet (or Claude 3.5 Sonnet / Haiku depending on the model you enabled). Throughput: Keep On-demand. Click Apply. Step 2: Conduct conversation (Chat)\nNow, try asking a question related to the document content you uploaded.\nIn the input box (Message input), type your question. Example: If you uploaded the \u0026ldquo;AWS Overview\u0026rdquo; document, ask: \u0026ldquo;Can you explain to me what EC2 is?\u0026rdquo;. Click Run. Observe the result: The AI will think for a few seconds (querying the Vector Store). Then, it will answer in natural language, summarizing the found information. Step 3: Verify data source\nThis is the most important feature of RAG that distinguishes it from standard ChatGPT: the ability to prove the source of information.\nIn the AI\u0026rsquo;s response, pay attention to the small numbers (footnotes) or the text Show source details. Click on those numbers or the details button. A Source details window will appear, displaying: Source chunk: The exact original text segment that the AI found in the document. Score: Similarity score (relevance). S3 Location: Path to the original file. Seeing this original text segment proves that the AI is not \u0026ldquo;hallucinating\u0026rdquo; but is actually reading your documents.\nStep 4: Test with irrelevant questions (Optional)\nTo see how the system reacts when information is not found.\nAsk a question completely unrelated to the documents. Example: \u0026ldquo;Can you explain some knowledge about personal finance?\u0026rdquo; (While your documents are about Cloud Computing). Expected Result: The AI might answer based on its general knowledge (if not restricted). OR the AI will answer \u0026ldquo;Sorry, I am unable to answer your question based on the retrieved data\u0026rdquo; - This is the ideal behavior for an enterprise RAG application. "
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/5-workshop/5.5-client-integration/",
	"title": "Client Application Integration (Optional)",
	"tags": [],
	"description": "",
	"content": "Target You will turn Python code into a professional Web Chatbot GUI (Graphical User Interface) that is user-friendly (similar to the ChatGPT interface) in just a few minutes.\nWe use:\nBackend: Python. Frontend: Streamlit. AI Model: Claude 3.5 Sonnet. Implementation Steps Part I: Configure AWS Credentials\nStep 1: Install AWS CLI\nOpen Terminal on your computer.\n# macOS brew install awscli # Linux curl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34; unzip awscliv2.zip sudo ./aws/install Step 2: Configure credentials\naws configure Enter the information when prompted:\nAWS Access Key ID: YOUR_ACCESS_KEY AWS Secret Access Key: YOUR_SECRET_KEY Default region name: us-east-1 Default output format: json Step 3: Verify configuration\n# Check credentials aws sts get-caller-identity # Check Bedrock connection aws bedrock-agent-runtime list-knowledge-bases --region ap-southeast-1 Security notes:\nDO NOT commit credentials to Git DO NOT share credentials with others Use IAM roles when possible Rotate credentials periodically Required permissions:\nIAM User needs the following permissions:\nbedrock:InvokeModel bedrock:RetrieveAndGenerate bedrock:Retrieve s3:GetObject (for Knowledge Base) Troubleshooting:\nError \u0026ldquo;Unable to locate credentials\u0026rdquo;:\nCheck if ~/.aws/credentials file exists Check file format is correct Try running aws configure again Error \u0026ldquo;AccessDeniedException\u0026rdquo;:\nCheck IAM permissions Ensure region is correct (ap-southeast-1) Check Knowledge Base ID is correct Error \u0026ldquo;ExpiredToken\u0026rdquo;:\nCredentials have expired Need to create new credentials from AWS Console Part II: Clone Project from pre-made GitHub\nStep 1: Access the following GitHub link\nPlease download and open the folder above using Visual Studio Code:\nhttps://github.com/DazielNguyen/chatbot_with_bedrock.git\nStep 2: Install libraries and Python environment\nInstall environment:\nMacOS: python3 -m venv .venv Win: python -m venv .venv Activate environment:\nMacOS: source .venv/bin/activate Win: .venv\\Scripts\\activate Install libraries:\nMacOS/ Win: pip install -r requirements.txt Step 3: Get the ID of the created Knowledge Base\nAccess Amazon Bedrock -\u0026gt; Knowledge Base -\u0026gt; knowledge-base-demo Update \u0026ldquo;KB_ID=\u0026ldquo;YOUR_KNOWLEDGE_BASE_ID\u0026rdquo;\u0026rdquo; Step 4: Run Streamlit - Chatbot UI and Experience\nRun Terminal: streamlit run start.py When the command finishes running, the following page will appear: Try asking some questions you uploaded to the Knowledge Base earlier. The Chatbot has returned results based on the data file you provided, with citations of your data sources. Conclusion Congratulations on successfully building a Web Chatbot built with Amazon Bedrock\n"
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": "Building a RAG Application using Knowledge Bases for Amazon Bedrock Overview Knowledge Bases for Amazon Bedrock is a fully managed feature that helps you implement RAG (Retrieval-Augmented Generation) techniques by connecting Foundation Models to your internal data sources to deliver accurate, cited, and contextually relevant responses.\nRAG is a technique to optimize Large Language Model (LLM) output by retrieving information from a trusted external database (Retrieval) and adding it to the context (Augmentation) before generating the answer (Generation). This method helps overcome limitations regarding outdated training data and ensures the AI answers based on the actual provided information.\nIn this lab, we will learn how to build an AI assistant capable of \u0026ldquo;reading and understanding\u0026rdquo; proprietary enterprise documents. You will perform the process from data ingestion and creating vector indexes to configuring the model to answer questions based on those documents without managing any servers.\nWe will use three main components to set up a complete RAG processing workflow:\nData Source (Amazon S3) - Acts as the repository of \u0026ldquo;truth\u0026rdquo;. You will upload documents (PDF, Word, Text) to an S3 bucket. The Knowledge Base will use this source to synchronize data. Vector Store (OpenSearch Serverless) - The place to store vector embeddings (numerically encoded data). When a user asks a question, the system will perform a semantic search here to extract the most relevant text segments instead of standard keyword searching. Foundation Model (Claude 3) - The Large Language Model acting as the processing brain. It receives the user\u0026rsquo;s question along with information found from the Vector Store, then synthesizes and generates a natural, accurate answer accompanied by source citations. Outcomes By the end of the workshop, you will have a practical, functioning Chatbot system with the following features:\nQ\u0026amp;A chat regarding proprietary document content. Accurate answers, no hallucinations. Source citations (knowing exactly which page the answer comes from). Rapid deployment without writing complex data processing code. Contents Workshop Overview Environment Preparation Create and Configure Knowledge Base Test Chatbot (RAG) Client Application Integration (Optional) Update Data Clean Up Resources "
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/6-self-evaluation/",
	"title": "Self-Assessment",
	"tags": [],
	"description": "",
	"content": "During my internship at AWS/First cloud journey program from September 8, 2025 to December 12, 2025, I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in learning AWS services, as well as applying learned knowledge to a group project on FinTech, through which I improved my teamwork skills and specifically achieved desired skills such as Docker, DevOps, and applying AWS services to support practical project implementation.\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ✅ ☐ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ☐ ✅ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ✅ ☐ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to schedules, rules, and work processes ✅ ☐ ☐ 6 Progressive mindset Willingness to receive feedback and improve oneself ✅ ☐ ☐ 7 Communication Presenting ideas and reporting work clearly ✅ ☐ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ☐ ✅ ☐ 12 Overall General evaluation of the entire internship period ✅ ☐ ☐ Needs Improvement Strengthen discipline and strictly comply with the rules and regulations of the company or any organization Improve problem-solving thinking Enhance communication skills in both daily interactions and professional contexts, including handling situations effectively In-depth Learning Ability: Need to dedicate time to \u0026ldquo;Deep Dive\u0026rdquo; into the core principles of AWS services instead of just stopping at \u0026ldquo;How-to\u0026rdquo; knowledge, to enable better debugging and system optimization. Proactive Contribution: Need to be bolder in proposing ideas for improvement or applying new technologies (such as Serverless, Automation) to the project, rather than just focusing on completing assigned tasks. "
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/5-workshop/5.6-update-data/",
	"title": "Update data",
	"tags": [],
	"description": "",
	"content": "Target One of the biggest advantages of RAG compared to Fine-tuning (retraining) a model is the ability to update data quickly. When a business has new regulations, you simply need to ingest them into the Knowledge Base, and the AI will \u0026ldquo;learn\u0026rdquo; them immediately.\nIn this section, we will simulate the following scenario:\nAsk the AI for a piece of non-existent information (The AI will answer that it doesn\u0026rsquo;t know). Provide that information to the system by uploading a new file. Ask the same question again to witness the AI answer correctly immediately. Implementation Steps Step 1: Verify the initial \u0026ldquo;lack of knowledge\u0026rdquo;\nWe need to confirm that the current AI knows nothing about the confidential information we are about to create.\nReturn to the Streamlit Chatbot interface (created in Part 5) or use the Test Knowledge Base window on the Console. Ask a question about hypothetical fake information. Example: \u0026ldquo;What is the activation code for Project Omega?\u0026rdquo; Observe the result: The AI will answer that it cannot find the information in the provided documents or will attempt a generic answer (if not restricted). Step 2: Create new data\nWe will create a text file containing this \u0026ldquo;secret\u0026rdquo; to ingest into the system.\nOn your computer, open Notepad (Windows) or TextEdit (Mac). Copy and paste the following content into the file: CONFIDENTIAL NOTICE: The secret Project Omega officially launches on 01/12/2025. The activation code is: \u0026#34;AWS-ROCKS-2025-SINGAPORE\u0026#34;. The Project Manager is Mr. Robot. Please keep this information strictly confidential. Save the file as: secret-project.txt. You can download the file here: TXT format file\nStep 3: Upload and Sync\nNow, we will feed this new knowledge into the AI\u0026rsquo;s \u0026ldquo;brain\u0026rdquo;.\nAccess the S3 Console, navigate to your old bucket (rag-workshop-demo).\nClick Upload -\u0026gt; Add files -\u0026gt; Select the secret-project.txt file -\u0026gt; Upload.\nSwitch to the Amazon Bedrock Console -\u0026gt; Select Knowledge bases from the left menu. Click on your Knowledge Base name. Scroll down to the Data source section, select the data source (s3-datasource). Click the Sync button (Orange). Wait: Wait for about 30 seconds to 1 minute until the Status column changes from Syncing to Available. Step 4: Verify again (The \u0026ldquo;Wow\u0026rdquo; Moment)\nThe system now possesses the new knowledge. Let\u0026rsquo;s challenge the AI once again.\nReturn to the Streamlit Chatbot interface (No need to reload the page or restart the server). Ask the exact same question as before: \u0026ldquo;What is the activation code for Project Omega?\u0026rdquo; Expected Result: The AI answers correctly: \u0026ldquo;The activation code is AWS-ROCKS-2025-SINGAPORE\u0026rdquo;. The AI cites the source as the secret-project.txt file. Conclusion You have just witnessed the true power of RAG!\nNo code editing required. No model retraining required. Simply Sync the data. Your Chatbot has become smarter and updated with the latest information in just a few simple steps. This is exactly why businesses choose this solution to build internal virtual assistants.\n"
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/5-workshop/5.7-cleanup/",
	"title": "Clean Resources",
	"tags": [],
	"description": "",
	"content": "Target To avoid incurring unwanted costs after finishing the practice lab, we need to delete the created resources.\n⚠️ WARNING: Deleting the Knowledge Base DOES NOT automatically delete the Vector Store (OpenSearch Serverless). You must manually delete the OpenSearch Serverless Collection as this is the costliest service in this Lab.\nImplementation Steps Step 1: Delete Knowledge Base\nAccess the Amazon Bedrock Console -\u0026gt; Knowledge bases.\nSelect the radio button next to your Knowledge Base name.\nClick the Delete button.\nA dialog box appears, enter the Knowledge Base name to confirm (or type delete).\nClick Delete. This process takes 10-15 minutes to complete successfully, so please be patient.\nStep 2: Delete Vector Store\nAccess the Amazon OpenSearch Service. In the left menu, under Serverless, select Collections. You will see a Collection named like bedrock-knowledge-base-.... Select the radio button next to that Collection name. Click the Delete button. Type confirm or the collection name to confirm deletion. Click Delete. Step 3: Delete Data on S3\nAccess the Amazon S3 service. Select the bucket rag-workshop-demo. Click the Empty button first. Type permanently delete to confirm deleting all files inside. After the bucket is empty, return to the Buckets list. Select that bucket again and click the Delete button. Enter the bucket name to confirm. Completion Congratulations on fully completing the Workshop \u0026ldquo;Building a RAG Application with Amazon Bedrock\u0026rdquo;. Your system has been cleaned up and is safe!\n"
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/7-feedback/",
	"title": "Sharing and Feedback",
	"tags": [],
	"description": "",
	"content": " Here, you can freely share your personal opinions about your experience participating in the First Cloud Journey program. This will help the FCJ team improve any shortcomings based on the following aspects:\nOverall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help whenever I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better. However, I think it would be nice to have more social gatherings or team bonding activities to strengthen relationships.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I don’t understand, and always encourages me to ask questions. The admin team supports administrative tasks, provides necessary documents, and creates favorable conditions for me to work effectively. I especially appreciate that the mentor allows me to try and solve problems myself instead of just giving the answer.\n3. Relevance of Work to Academic Major\nThe tasks I was assigned align well with the knowledge I learned at university, while also introducing me to new areas I had never encountered before. This allowed me to both strengthen my foundational knowledge and gain practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared valuable real-world experiences that helped me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable. When there are urgent projects, everyone works together and supports one another regardless of their position. This made me feel like a real part of the team, even as an intern.\n6. Internship Policies / Benefits\nThe company provides an internship allowance and offers flexible working hours when needed. In addition, having the opportunity to join internal training sessions is a big plus.\nAdditional Questions What did you find most satisfying during your internship? What do you think the company should improve for future interns? If recommending to a friend, would you suggest they intern here? Why or why not? Suggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience? Would you like to continue this program in the future? Any other comments (free sharing): "
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]