[
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/1-worklog/1.7-week7/",
	"title": "Week 7 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 7 Objectives: Practice deploying static website to AWS and configure Github Actions with AWS IAM User. Practice converting Monolith application to Microservices: separating components, containerization. Learn and practice AWS Developer Tools: CodeStar, CodePipeline, CodeBuild, CodeDeploy. Practice CI/CD Pipeline: automate build, test, and deployment processes. Practice creating Microservices: design, deploy, and manage microservices on AWS. Practice restructuring data and workflows: optimize application architecture. Practice Amazon Kinesis: streaming data, real-time data processing. Practice building Single Page Application (SPA) with authentication: integrate authentication and authorization. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Practice deploying static website to AWS and configure Github Actions with AWS IAM User 10/20/2025 10/20/2025 https://www.youtube.com/watch?v=oSZ0tzlkuWo 2 - Practice converting Monolith Application 10/21/2025 10/21/2025 https://000050.awsstudygroup.com/vi/ 3 - Practice configuring automated application release (AWS CodeStar, AWS CodePipeline, AWS CodeBuild, AWS CodeDeploy) 10/22/2025 10/22/2025 https://000051.awsstudygroup.com/vi/ 4 - Practice creating a Microservice - Restructure data and workflows 10/23/2025 10/23/2025 https://000052.awsstudygroup.com/vi/ https://000053.awsstudygroup.com/vi/ 5 - Practice messaging with Kinesis 10/24/2025 10/24/2025 https://000054.awsstudygroup.com/vi/ 6 - Practice creating an authenticated Single Page Application (SPA) 10/25/2025 10/25/2025 https://000055.awsstudygroup.com/vi/ Week 7 Achievements: Successfully practiced deploying static website to AWS with Github Actions: Configured S3 bucket to host static website Set up AWS IAM User with appropriate permissions Created and configured Github Actions workflow Automated deployment process when pushing code to repository Completed converting Monolith application to Microservices lab: Analyzed and separated components of monolith application Containerized services with Docker Deployed independent containers on AWS Understood advantages of microservices architecture Successfully practiced AWS Developer Tools - CI/CD Pipeline: Set up AWS CodeStar for project management Configured AWS CodePipeline for automation workflow Used AWS CodeBuild to build applications Automated deployment with AWS CodeDeploy Integrated entire process from commit to production Completed creating Microservice lab: Designed and developed independent microservice Configured API endpoints and communication Deployed microservice to AWS Testing and monitoring microservice Successfully practiced restructuring data and workflows: Optimized database schema for microservices Refactored code and workflow Applied best practices for distributed architecture Improved performance and scalability Completed messaging with Amazon Kinesis lab: Set up Kinesis Data Streams Processed real-time data streaming Integrated Kinesis with Lambda functions Analyzed and processed data in real-time Successfully practiced building Single Page Application (SPA) with authentication: Developed SPA with modern JavaScript framework Integrated authentication system Configured authorization and access control Deployed SPA to AWS with secure configuration Connected frontend with backend APIs "
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/1-worklog/1.6-week6/",
	"title": "Week 6 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 6 Objectives: Learn and master Docker and Docker Compose: containerization, docker images, docker-compose. Practice deploying applications on Docker with AWS: create Docker images, push to ECR. Practice Amazon Elastic Container Service (ECS): deploy and manage containers on AWS. Learn about Nginx: reverse proxy, load balancing, web server configuration. Participate in \u0026ldquo;DATA SCIENCE ON AWS\u0026rdquo; workshop: explore data science and machine learning services on AWS. Practice Serverless Architecture: API Gateway, Lambda, SAM deployment. Practice Amazon Cognito: user authentication and authorization. Practice S3 Static Website with SSL: set up secure static website on S3. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Learn about docker, docker-compose 10/13/2025 10/13/2025 https://github.com/tuanvu250/AWS-FCJ/tree/main/bonus/docker https://docs.docker.com/get-started/ 2 - Practice deploying applications on Docker with AWS - Practice deploying applications on Amazon Elastic Container Service 10/14/2025 10/14/2025 https://000015.awsstudygroup.com/vi/ https://000016.awsstudygroup.com/vi/ 3 - Learn about Nginx 10/15/2025 10/15/2025 https://github.com/tuanvu250/AWS-FCJ/tree/main/bonus/nginx https://www.youtube.com/watch?v=C_cVyFWD2KQ\u0026t=2774s 4 - WORKSHOP \u0026ldquo;DATA SCIENCE ON AWS\u0026rdquo; – UNLOCKING THE POWER OF DATA WITH CLOUD COMPUTING 10/16/2025 10/16/2025 https://qhdn-hcmuni.fpt.edu.vn/2025/10/13/workshop-data-science-on-aws-mo-khoa-suc-manh-du-lieu-cung-dien-toan-dam-may/ 5 - Serverless - Guide to writing Front-end calling API Gateway - Serverless - Deploy application on SAM 10/17/2025 10/17/2025 https://000079.awsstudygroup.com/vi/ https://000080.awsstudygroup.com/vi/ 6 - Serverless - Authentication with Amazon Cognito - Serverless - Set up SSL S3 Static website 10/18/2025 10/18/2025 https://000081.awsstudygroup.com/vi/ https://000082.awsstudygroup.com/vi/ Week 6 Achievements: Completed learning about Docker and Docker Compose: Understood containerization and basic concepts Mastered creating Dockerfile and docker-compose.yml Managed Docker images and containers Successfully practiced deploying applications on Docker with AWS: Created and managed Docker images Pushed images to Amazon ECR (Elastic Container Registry) Deployed containerized applications on AWS Completed Amazon Elastic Container Service (ECS) lab: Created and configured ECS cluster Deployed task definitions and services Managed and scaled containers on ECS Successfully learned about Nginx: Understood reverse proxy and load balancing Configured Nginx as web server Applied Nginx in microservices architecture Participated in \u0026ldquo;DATA SCIENCE ON AWS\u0026rdquo; workshop: explored data science and machine learning services on AWS, learned how to apply cloud computing to data analysis. Completed Serverless Architecture labs: Wrote Front-end calling API Gateway Deployed application with AWS SAM (Serverless Application Model) Integrated Lambda functions with API Gateway Successfully practiced Amazon Cognito: Set up user pools and identity pools User authentication and authorization Integrated Cognito with web application Completed S3 Static Website with SSL lab: Set up S3 bucket to host static website Configured SSL/TLS certificate Deployed secure static website with HTTPS "
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/1-worklog/1.5-week5/",
	"title": "Week 5 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 5 Objectives: Learn and master AWS database services: Amazon RDS, Amazon Aurora, Redshift, Elasticache. Practice with Amazon RDS: create and manage RDS instances. Practice building DataLake on AWS: data collection and storage, create Data Catalog with Amazon Glue, analyze and visualize with Amazon Athena. Practice Amazon DynamoDB Immersion Day: work with DynamoDB, understand NoSQL database concepts. Practice cost and performance analysis using AWS Glue and Amazon Athena. Practice building a Datalake with real-world data. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - AWS Database Services: + Amazon RDS \u0026amp; Amazon Aurora + Redshift - Elasticache 10/06/2025 10/06/2025 https://github.com/tuanvu250/AWS-FCJ/blob/main/module/module-05/note.md 2 - Practice getting started with Amazon RDS \u0026amp;emsp - Practice DataLake on AWS + Data collection and storage + Create Data Catalog (Amazon Glue)\n+ Analyze and visualize (Amazon Athena) 10/07/2025 10/07/2025 https://000005.awsstudygroup.com/vi/ https://000035.awsstudygroup.com/vi/ 3 - Practice Amazon DynamoDB Immersion Day 10/08/2025 10/08/2025 https://000039.awsstudygroup.com/vi/ 4 - Practice Amazon DynamoDB Immersion Day 10/09/2025 10/09/2025 https://000039.awsstudygroup.com/vi/ 5 - Practice cost and performance analysis with AWS Glue and Amazon Athena - Practice working with Amazon DynamoDB 10/10/2025 10/10/2025 https://000040.awsstudygroup.com/vi/ https://000060.awsstudygroup.com/vi/ 6 - Practice building a Datalake with your data 10/11/2025 10/11/2025 https://000070.awsstudygroup.com/vi/ Week 5 Achievements: Completed learning about AWS database services: Amazon RDS, Amazon Aurora, Redshift, and Elasticache. Successfully practiced with Amazon RDS: created, configured, and managed RDS instances. Completed DataLake on AWS lab: Collected and stored data on S3 Created Data Catalog using Amazon Glue Analyzed and visualized data with Amazon Athena Successfully completed Amazon DynamoDB Immersion Day: mastered working with NoSQL database, concepts of partition key, sort key, GSI/LSI. Completed cost and performance analysis lab with AWS Glue and Amazon Athena: optimized queries, managed partitions. Successfully practiced building a Datalake with real-world data: applied all learned knowledge about data pipelines, ETL, and analytics. "
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/1-worklog/1.4-week4/",
	"title": "Week 4 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 4 Objectives: Learn and master AWS security services: Share Responsibility Model, AWS IAM, Amazon Cognito, AWS Organizations, AWS KMS. Practice AWS Security Hub: enable Security Hub and score each security standard. Practice optimizing EC2 costs with Lambda: create Tags for Instances, Roles for Lambda, and Lambda Functions. Manage resources with Tags and Resource Groups: manage access to EC2 Resource Tags, limit user permissions with IAM Permission Boundary. Practice data encryption with AWS KMS: create KMS, S3, CloudTrail, Athena, and test encrypted data sharing. Practice IAM Role \u0026amp; Condition: grant applications access to AWS services. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - AWS Security Services: + Share Responsibility Model + AWS Identity and Access Management + Amazon Cognito + AWS Organizations + Amazon Key Management Service 29/09/2025 29/09/2025 https://github.com/tuanvu250/AWS-FCJ/blob/main/module/module-05/note.md 2 - Practice Getting Started with AWS Security Hub: + Enable Security Hub \u0026amp; score each standard - Practice Optimizing EC2 Costs with Lambda: + Create Tags for Instances \u0026amp; Create Role for Lambda + Create Lambda Function \u0026amp; verify results 30/09/2025 30/09/2025 https://000018.awsstudygroup.com/vi/ https://000022.awsstudygroup.com/vi/ 3 - Manage resources with Tags and Resource Groups - Manage access to EC2 Resource Tags with AWS IAM - Limit user permissions with IAM Permission Boundary 01/10/2025 01/10/2025 https://000027.awsstudygroup.com/vi/ https://000028.awsstudygroup.com/vi/ https://000030.awsstudygroup.com/vi/ 4 - Practice Encryption at Rest with AWS KMS: + Create Key Management Service \u0026amp; Amazon S3 + Create AWS CloudTrail \u0026amp; Amazon Athena + Test and share encrypted data on S3 - Practice IAM Role \u0026amp; Condition 02/10/2025 02/10/2025 https://000033.awsstudygroup.com/vi/ https://000044.awsstudygroup.com/vi/ 5 - Event [AWS GenAI Builder Club] AI-Driven Development Life Cycle: Reimagining Software Engineering (2pm Friday 3/10/2025) 03/10/2025 03/10/2025 6 - Practice Granting applications access to AWS services with IAM Role - AWS Database Services: Database Concepts review 04/10/2025 04/10/2025 https://000048.awsstudygroup.com/ https://github.com/tuanvu250/AWS-FCJ/blob/main/module/module-06/note.md Week 4 Achievements: Completed learning about AWS security services: Share Responsibility Model, AWS IAM, Amazon Cognito, AWS Organizations, and AWS KMS. Successfully practiced AWS Security Hub: enabled Security Hub and scored each security standard. Completed optimizing EC2 costs with Lambda lab: created Tags for Instances, Roles for Lambda, and Lambda Functions, verified results successfully. Successfully practiced managing resources with Tags and Resource Groups, managing access to EC2 Resource Tags with AWS IAM, and limiting user permissions with IAM Permission Boundary. Completed encryption at rest with AWS KMS lab: created KMS, S3, CloudTrail, Athena, and successfully tested encrypted data sharing. Successfully practiced IAM Role \u0026amp; Condition: granted applications access to AWS services with IAM Role. Attended [AWS GenAI Builder Club] AI-Driven Development Life Cycle: Reimagining Software Engineering event. Learned and took notes on Database Concepts on AWS. "
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/1-worklog/1.3-week3/",
	"title": "Week 3 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 3 Objectives: Learn and master AWS Storage Services: Amazon S3, Access Point, Storage Class, S3 Static Website, CORS, Control Access, Object Key \u0026amp; Performance, Glacier. Understand Snow Family, Storage Gateway and AWS Backup: data migration and backup solutions. Practice deploying AWS Backup: create S3 bucket, backup plan, set up notifications and verify results. Practice VM Import/Export: import virtual machines to AWS, deploy EC2 from AMI and export EC2 Instance. Practice deploying FSx on Windows: create SSD/HDD Multi-AZ file system, File Shares, performance monitoring and advanced features. Practice Amazon S3: create bucket, static website, CloudFront integration, versioning and cross-region replication. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - AWS Storage Services: + Amazon Simple Storage Service ( S3 ) + Access Point + Storage Class + S3 Static Website \u0026amp; CORS 22/09/2025 22/09/2025 https://github.com/tuanvu250/AWS-FCJ/blob/main/module/module-04/note.md 2 - AWS Storage Services: + Control Access + Object Key \u0026amp; Performance + Glacier + Snow Family - Storage Gateway - Backup 23/09/2025 23/09/2025 https://github.com/tuanvu250/AWS-FCJ/blob/main/module/module-04/note.md 3 - Practice deploying AWS Backup for system: + Create S3 Bucket \u0026amp; deploy infrastructure + Create Backup plan, set up notifications + Verify operations 24/09/2025 24/09/2025 https://000013.awsstudygroup.com/vi/ 4 - Practice VM Import/Export: + Import virtual machine to AWS + Deploy EC2 Instance from AMI + Export EC2 Instance from AWS 24/09/2025 24/09/2025 https://000014.awsstudygroup.com/vi/ 5 - Practice deploying FSx on Windows: + Create one SSD and one HDD Multi-AZ file system + Create File Shares, check and monitor performance + Enable data deduplication and shadow copies + Enable storage quotas and continuous access sharing with throughput and storage expansion capabilities; 26/09/2025 26/09/2025 https://000025.awsstudygroup.com/vi/ 6 - Practice Getting Started With Amazon S3: + Create S3 bucket and upload sample web data + Enable static website feature and verify + Accelerate Static Website with CloudFront + Bucket Versioning and transfer S3 Object to another region 27/09/2025 27/09/2025 https://000057.awsstudygroup.com/vi/ Week 3 Achievements: Completed learning about AWS Storage Services: Amazon S3, Access Point, Storage Class, S3 Static Website, CORS, Control Access, Object Key \u0026amp; Performance, Glacier. Learned and documented thoroughly about Snow Family, Storage Gateway and AWS Backup: understood data migration and backup solutions clearly. Practiced deploying AWS Backup: created S3 bucket \u0026amp; infrastructure, created backup plan, set up notifications and verified successful operations. Completed VM Import/Export lab: imported virtual machine to AWS, deployed EC2 Instance from AMI and exported EC2 Instance from AWS. Practiced deploying FSx on Windows: created SSD and HDD Multi-AZ file system, created File Shares, checked performance, enabled deduplication, shadow copies, quota and expanded throughput/storage. Completed Amazon S3 lab: created bucket \u0026amp; uploaded web data, enabled static website, accelerated with CloudFront, enabled versioning and cross-region replication successfully. "
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/1-worklog/1.2-week2/",
	"title": "Week 2 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 2 Objectives: Practice deploying AWS Transit Gateway: create, configure Attachments, Route Tables, and update routing between VPCs. Gain knowledge of Compute VM services on AWS: EC2, Auto Scaling, EFS, FSx, Lightsail, MGN. Practice deploying AWS Backup: create S3 bucket, backup plan, set up notifications, and verify results. Practice deploying AWS Storage Gateway: create S3 bucket, EC2, configure File Shares, and connect from On-premise. Get familiar with Amazon S3: create bucket, enable static website hosting, accelerate with CloudFront, manage versioning, and cross-region replication. Tasks to be carried out this week: Day Task Start Date Completion Date Reference 1 - Practice AWS Transit Gateway: + Create Transit Gateway + Create Transit Gateway Attachments + Create Transit Gateway Route Tables + Add Transit Gateway Routes into VPC Route Tables 09/15/2025 09/15/2025 https://000020.awsstudygroup.com/ 2 - Compute VM services on AWS - Amazon Elastic Compute Cloud (EC2) - EC2 Auto Scaling - EFS/FSx - Lightsail - MGN 09/16/2025 09/16/2025 https://github.com/tuanvu250/AWS-FCJ/blob/main/module/module-03/note.md 3 - Practice deploying AWS Backup for the system: + Create S3 Bucket \u0026amp; deploy infrastructure + Create Backup plan, set up notifications + Verify operation 09/17/2025 09/17/2025 https://000013.awsstudygroup.com/vi/ 4 - Attend event Vietnam Cloud Day 2025: Ho Chi Minh City Connect Edition for Builders (Track 1: GenAI \u0026amp; Data) 09/18/2025 09/18/2025 https://pages.awscloud.com/vietnam-cloud-day-hcmc-connect-edition 5 - Practice deploying AWS Storage Gateway: + Create S3 Bucket \u0026amp; EC2 for Storage Gateway + Create Storage Gateway and File Shares + Connect File Shares from On-premise machine 09/18/2025 09/18/2025 https://000024.awsstudygroup.com/vi/ 6 - Practice Getting Started with Amazon S3: + Create S3 bucket and upload sample web data + Enable static website hosting and verify + Accelerate Static Website with CloudFront + Enable Bucket Versioning and replicate S3 Objects across regions 09/20/2025 09/20/2025 https://000057.awsstudygroup.com/vi/ Week 2 Achievements: Completed AWS Transit Gateway lab: created TGW, configured Attachments, Route Tables, and updated VPC Route Tables for traffic transit. Learned and documented AWS Compute VM services: EC2, Auto Scaling, EFS, FSx, Lightsail, MGN. Practiced deploying AWS Backup: created bucket, backup plan, notifications, and successfully verified backup jobs. Attended Vietnam Cloud Day 2025 in HCMC, Track GenAI \u0026amp; Data, gaining updated insights from AWS experts. Completed AWS Storage Gateway deployment: created bucket, EC2, configured File Shares, and successfully accessed from On-premise. Finished Amazon S3 lab: deployed static website, integrated CloudFront, enabled versioning, and tested cross-region replication. "
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/1-worklog/1.1-week1/",
	"title": "Week 1 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1 Objectives: Get acquainted with FCJ members; understand internal rules and working procedures. Prepare the workshop environment: VS Code, Hugo, Git, AWS CLI, SSH. Set up the AWS account and baseline security: enable MFA, create Admin Group/User, configure Console and Support cases. Master core AWS Networking fundamentals: VPC, Subnet, Route Table, IGW, NAT GW, Security Group, NACL, Route 53. Practice compute and connectivity: launch EC2, SSH access, Site-to-Site VPN (lab). Set up Hybrid DNS with Route 53 Resolver (Microsoft AD, forwarders, resolver rules) for bidirectional name resolution. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Get acquainted with FCJ members - Read and note internal rules and procedures - Prepare workshop tools (VS Code, Hugo, \u0026hellip;) - Intro to AWS \u0026amp; core concepts 08/09/2025 08/09/2025 https://van-hoang-kha.github.io/vi/ 2 - Create a new AWS account - Enable MFA for the AWS account - Create Admin Group and Admin User - Support account verification - Explore and configure AWS Management Console - Create and manage AWS Support cases 09/09/2025 09/09/2025 https://000001.awsstudygroup.com/vi/ 3 - AWS Networking services: + VPC, Subnet, Route Table + Internet Gateway, NAT Gateway + Security Group \u0026amp; NACL + VPC Peering, Transit Gateway (concepts) + Elastic Load Balancing, Route 53 (DNS) 10/09/2025 10/09/2025 https://github.com/tuanvu250/AWS-FCJ/blob/main/module/module-02/note.md 4 - EC2 practice: + Launch EC2 instance + SSH access + Configure Site‑to‑Site VPN 11/09/2025 11/09/2025 https://000003.awsstudygroup.com/ 5 - Set up Hybrid DNS with Route 53 Resolver: + Connect to RDGW + Deploy Microsoft AD + Configure DNS 12/09/2025 12/09/2025 https://000010.awsstudygroup.com/vi/ 6 - Set up VPC Peering + Create peering connection between VPCs + Update Route tables and validate connectivity 13/09/2025 13/09/2025 https://000019.awsstudygroup.com/ Week 1 Achievements: Prepared the working environment and understood core AWS concepts (account, region, AZ, service groups). Account setup and security: created AWS account, enabled MFA, created Admin Group/User, completed account verification, configured Console and Support cases. Networking fundamentals on AWS: VPC, Subnet, Route Table, Internet Gateway, NAT Gateway, Security Group, NACL, Elastic Load Balancing, Route 53; concepts of Peering, VPN, Transit Gateway. Compute \u0026amp; connectivity hands‑on: launched an EC2 instance, connected via SSH, and configured a Site‑to‑Site VPN (lab level). Hybrid DNS with Route 53 Resolver: connected to RDGW, deployed Microsoft AD, configured DNS/forwarders and Resolver rules for bidirectional lookups. Completed Week 1 worklog and cleaned up unused resources. "
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/3-blogstranslated/3.1-blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": "Chạy và tối ưu các Small Language Models tại on-premises và tại edge Bởi Chris McEvilly, Fernando Galves Guy Ben Baruchn | Ngày: 23/06/2025\n| In Advanced (300), AWS Outposts, Technical How-to\nKhi bạn chuyển các triển khai generative AI của mình từ giai đoạn prototype sang production, bạn có thể nhận thấy nhu cầu cần chạy các foundation models (FMs) on-premises hoặc at the edge để đáp ứng các yêu cầu về data residency, information security (InfoSec) policy, hoặc low latency. Ví dụ, các khách hàng trong những ngành được quản lý chặt chẽ như financial services, healthcare, và telecom có thể muốn tận dụng các chatbots để hỗ trợ customer queries, tối ưu hóa internal workflows cho các complex reporting, và tự động phê duyệt yêu cầu - đồng thời vẫn giữ dữ liệu trong phạm vi quốc gia. Tương tự, một số tổ chức chọn triển khai các small language models (SLMs) của riêng họ để phù hợp với các yêu cầu InfoSec nội bộ nghiêm ngặt. Ví dụ khác, các nhà sản xuất có thể muốn triển khai SLMs ngay trong nhà máy của họ để phân tích dữ liệu sản xuất và cung cấp chẩn đoán thiết bị theo thời gian thực. Để đáp ứng các nhu cầu về data residency, latency và InfoSec của người dùng, bài viết này cung cấp hướng dẫn về cách triển khai generative AI FMs vào AWS Local Zones và AWS Outposts. Mục tiêu là trình bày một framework giúp chạy nhiều loại SLMs khác nhau nhằm đáp ứng các yêu cầu xử lý dữ liệu tại chỗ dựa trên customer engagements.\nCác tùy chọn triển khai Generative AI Sự phát triển của generative AI trong triển khai và thử nghiệm đã tăng tốc với hai tùy chọn triển khai doanh nghiệp chính. Tùy chọn đầu tiên là sử dụng large language model (LLM) để đáp ứng các nhu cầu của doanh nghiệp. LLMs có tính linh hoạt đáng kinh ngạc: một mô hình duy nhất có thể thực hiện nhiều nhiệm vụ hoàn toàn khác nhau, chẳng hạn như trả lời câu hỏi, viết mã (coding), tóm tắt tài liệu, dịch ngôn ngữ, và tạo nội dung (content generation). LLMs có tiềm năng làm thay đổi cách con người tạo nội dung cũng như cách sử dụng công cụ tìm kiếm và trợ lý ảo. Tùy chọn triển khai thứ hai là sử dụng small language models (SLMs), tập trung vào một use case cụ thể. SLMs là các compact transformer models chủ yếu sử dụng decoder-only hoặc encoder-decoder architectures, thường có ít hơn 20 tỷ parameters, mặc dù định nghĩa này đang phát triển khi các mô hình lớn hơn ra đời. SLMs có thể đạt được hiệu suất tương đương hoặc thậm chí vượt trội khi được fine-tuned cho các domain hoặc task cụ thể, khiến chúng trở thành lựa chọn thay thế tuyệt vời cho các ứng dụng chuyên biệt.\nNgoài ra, SLMs còn mang lại thời gian suy luận (inference time) nhanh hơn, yêu cầu tài nguyên thấp hơn, và phù hợp để triển khai trên nhiều loại thiết bị hơn, đặc biệt hữu ích cho các ứng dụng chuyên biệt và edge computing, nơi không gian và nguồn điện bị giới hạn. Mặc dù SLMs có phạm vi và độ chính xác hạn chế hơn so với LLMs, bạn có thể nâng cao hiệu suất của chúng cho nhiệm vụ cụ thể thông qua Retrieval Augmented Generation (RAG) và fine-tuning. Sự kết hợp này tạo ra một SLM có khả năng trả lời các truy vấn liên quan đến một domain cụ thể với mức độ chính xác tương đương LLM, đồng thời giảm thiểu hiện tượng hallucinations. Nhìn chung, SLMs cung cấp các giải pháp hiệu quả, cân bằng giữa nhu cầu người dùng và hiệu quả chi phí.\nTổng quan kiến trúc Giải pháp được trình bày trong bài viết này sử dụng Llama.cpp, một framework được tối ưu hóa được viết bằng C/C++ nhằm chạy hiệu quả nhiều loại SLMs. Llama.cpp có thể hoạt động hiệu quả trong nhiều môi trường tính toán khác nhau, cho phép generative AI models vận hành trong Local Zones hoặc Outposts mà không cần các cụm GPU lớn (GPU clusters) như thường thấy khi chạy LLMs trong native frameworks của chúng. Framework này mở rộng lựa chọn mô hình và tăng hiệu suất hoạt động khi triển khai SLMs vào Local Zones và Outposts.\nKiến trúc này cung cấp một template cho việc triển khai nhiều loại SLMs nhằm hỗ trợ các use case như chatbot hoặc content generation. Giải pháp bao gồm một front-end application nhận user queries, định dạng các prompts để trình bày cho mô hình và trả về các phản hồi từ mô hình cho người dùng. Để hỗ trợ một giải pháp có khả năng mở rộng (scalable), application servers và Amazon EC2 G4dn GPU-enabled instances được đặt phía sau Application Load Balancer (ALB).\nTrong trường hợp số lượng prompts đến vượt quá khả năng xử lý của SLMs, có thể triển khai message queue ở phía trước SLMs. Ví dụ, bạn có thể triển khai một RabbitMQ cluster để hoạt động như queue manager cho hệ thống.\nHình 1: Architecture overview\nTriển khai giải pháp Các hướng dẫn sau đây mô tả cách khởi chạy một SLM bằng Llama.cpp trong Local Zones hoặc trên Outposts. Mặc dù phần kiến trúc tổng quan trước đó trình bày một giải pháp hoàn chỉnh với nhiều thành phần, bài viết này tập trung cụ thể vào các bước cần thiết để triển khai SLM trong EC2 instance sử dụng Llama.cpp.\nĐiều kiện tiên quyết Để triển khai giải pháp này, bạn cần chuẩn bị các điều kiện sau:\nAWS account đã được allowlisted cho Local Zones, hoặc có một logical Outpost đã được cài đặt, cấu hình và hoạt động.\nQuyền truy cập vào G4dn instances trong tài khoản của bạn tại vị trí đã chọn\n_(kiểm tra trong AWS Service Quotas).\n_\nMột VPC đã được tạo để lưu trữ môi trường triển khai.\nPublic và private subnets để hỗ trợ môi trường trong VPC.\nMột security group được liên kết với EC2 instance của bạn.\nAWS Identity and Access Management (IAM) role với quyền AWS Systems Manager Session Manager permissions.\nHình 1: Architecture overview\n1. Khởi chạy GPU instance cho SLM Đăng nhập vào AWS Management Console, mở Amazon EC2 console,\nvà khởi chạy một g4dn.12xlarge EC2 instance trong Local Zone hoặc Outposts environment của bạn.\nCấu hình bao gồm:\nRed Hat Enterprise Linux 9 (HVM), SSD Volume Type\nPrivate subnet liên kết với Local Zone hoặc Outposts rack\n30 GiB gp2 root volume và thêm 300 GiB gp2 EBS volume\nIAM role đã được cấu hình với các quyền cần thiết cho Systems Manager\nSSM Agent được cài đặt để kết nối tới instance\n(tham khảo hướng dẫn trong Install SSM Agent on RHEL 8.x and 9.x trong Systems Manager User Guide)\nĐể biết hướng dẫn chi tiết về việc khởi chạy EC2 instance, tham khảo: Launch an EC2 instance using the launch instance wizard in the console hoặc Launch an instance on your Outposts rack.\nHình 2: SLM instance launched\n2. Cài đặt NVIDIA drivers Kết nối tới SLM instance bằng Systems Manager.\nBạn có thể làm theo hướng dẫn tại Connect to your Amazon EC2 instance using Session Manager trong Amazon EC2 User Guide.\nCài đặt kernel packages và các công cụ cần thiết:\nsudo su - dnf update -y \u0026lt;br\u0026gt;subscription-manager repos --enable codeready-builder-for-rhel-9-x86_64-rpms dnf install -y \u0026lt;https://dl.fedoraproject.org/pub/epel/epel-release-latest-9.noarch.rpm\u0026gt; dnf install -y ccache cmake gcc-c++ git git-lfs htop python3-pip unzip wget dnf install -y dkms elfutils-libelf-devel kernel-devel kernel-modules-extra \\\\ libglvnd-devel vulkan-devel xorg-x11-server-Xorg \u0026lt;br\u0026gt;systemctl enable --now dkms reboot Cài đặt Miniconda3 trong thư mục /opt/miniconda3 hoặc trình quản lý package tương thích khác để quản lý Python dependencies.\nCài đặt NVIDIA drivers:\ndnf config-manager --add-repo \\\\ \u0026lt;http://developer.download.nvidia.com/compute/cuda/repos/rhel9/x86_64/cuda-rhel9.repo\u0026gt; dnf module install -y nvidia-driver:latest-dkms dnf install -y cuda-toolkit echo \u0026#39;export PATH=/usr/local/cuda/bin:\\$PATH\u0026#39; \u0026gt;\u0026gt; ~/.bashrc echo \u0026#39;export LD_LIBRARY_PATH=/usr/local/cuda/lib64:\\$LD_LIBRARY_PATH\u0026#39; \u0026gt;\u0026gt; ~/.bashrc source ~/.bashrc 3. Tải xuống và cài đặt Llama.cpp Tạo và mount filesystem của Amazon EBS volume bạn đã tạo trước đó vào thư mục /opt/slm. Xem hướng dẫn tại Make an Amazon EBS volume available for use trong Amazon EBS User Guide.\nChạy các lệnh sau để tải và cài đặt Llama.cpp:\ncd /opt/slm\rgit clone -b b4942 \\\\\u0026lt;https://github.com/ggerganov/llama.cpp.git\u0026gt;\rcd llama.cpp cmake -B build -DGGML_CUDA=ON\rcmake --build build --config Release -j\\$(nproc) conda install python=3.12\rpip install -r requirements.txt\rpip install nvitop 4. Tải xuống và chuyển đổi SLM model Để chạy SLM hiệu quả với Llama.cpp, bạn cần chuyển đổi model sang định dạng GGUF (GPT-Generated Unified Format). Việc chuyển đổi này giúp tối ưu hiệu năng và mức sử dụng bộ nhớ cho các môi trường edge deployments có tài nguyên giới hạn. GGUF được thiết kế đặc biệt để hoạt động với Llama.cpp inference engine. Các bước sau đây minh họa cách tải SmolLM2 1.7B và chuyển đổi sang định dạng GGUF:\nmkdir /opt/slm/models cd /opt/slm/models git lfs install git clone \u0026lt;https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct\u0026gt; cd /opt/slm/llama.cpp python3 convert_hf_to_gguf.py --outtype f16 \\\\ --outfile /opt/slm/llama.cpp/models/SmolLM2-1.7B-Instruct-f16.gguf \\\\ /opt/slm/models/SmolLM2-1.7B-Instruct echo \u0026#39;export PATH=/opt/slm/llama.cpp/build/bin:\\$PATH\u0026#39; \u0026gt;\u0026gt; ~/.bashrc echo \u0026#39;export LD_LIBRARY_PATH=/opt/slm/llama.cpp/build/bin:\\$LD_LIBRARY_PATH\u0026#39; \u0026gt;\u0026gt; ~/.bashrc source ~/.bashrc Bạn cũng có thể tải các models khác được công khai từ Hugging Face nếu cần,\nvà thực hiện quá trình chuyển đổi tương tự.\nSLM Operation and Optimization Việc triển khai SLMs thông qua Llama.cpp mang lại tính linh hoạt cao trong vận hành, cho phép tùy chỉnh môi trường và tối ưu hóa theo các use case cụ thể. Với Llama.cpp, bạn có thể chọn nhiều tham số khác nhau để tối ưu việc sử dụng tài nguyên hệ thống và hoạt động của mô hình, giúp tận dụng hiệu quả tài nguyên mà không tiêu tốn không cần thiết hoặc ảnh hưởng đến hiệu suất. Các tham số phổ biến khi chạy Llama.cpp giúp kiểm soát cách mô hình hoạt động bao gồm:\n-ngl N, \u0026ndash;n-gpu-layers N: Khi biên dịch với GPU support, tùy chọn này cho phép chuyển một số layer sang GPU để tính toán, giúp tăng hiệu suất xử lý. -t N, \u0026ndash;threads N: Xác định số lượng threads sử dụng trong quá trình sinh nội dung. Để đạt hiệu suất tối ưu, nên đặt giá trị này bằng số lõi CPU vật lý có trong hệ thống. -n N, \u0026ndash;n-predict N: Xác định số lượng tokens cần sinh ra khi tạo văn bản.\nĐiều chỉnh giá trị này sẽ ảnh hưởng đến độ dài đầu ra của văn bản. -sm, \u0026ndash;split-mode: Xác định cách chia mô hình giữa nhiều GPU khi chạy trong môi trường multi-GPU. Nên thử \u0026ldquo;row\u0026rdquo; splitting mode, vì trong một số trường hợp, nó mang lại hiệu suất tốt hơn so với chia theo layer-based mặc định. --temp N: Temperature điều khiển mức độ ngẫu nhiên trong đầu ra của SLM. Giá trị thấp hơn (ví dụ 0.2-0.5) tạo ra câu trả lời nhất quán và xác định hơn, giá trị cao hơn (ví dụ 0.9-1.2) giúp mô hình sáng tạo và đa dạng hơn (mặc định: 0.88) -s SEED, \u0026ndash;seed SEED: Cung cấp phương pháp kiểm soát ngẫu nhiên của mô hình. Việc đặt seed cố định giúp tái tạo kết quả nhất quán trong nhiều lần chạy (mặc định: -1, -1 = random seed). -c, \u0026ndash;ctx-size N: Xác định context size, số lượng tokens mà FM có thể xử lý trong một prompt. Giá trị này ảnh hưởng đến mức RAM cần thiết và độ chính xác của mô hình. Ví dụ: với Phi-3, khuyến nghị giảm context size còn 8k hoặc 16k để tối ưu hiệu suất. Lệnh mẫu: \u0026ndash;ctx-size XXXX trong đó XXXX là context size. Phần này minh họa cách tối ưu hóa hiệu suất SLM cho các use case cụ thể bằng Llama.cpp, gồm hai kịch bản phổ biến: Chatbot interactions và Text summarization\nChatbot Use Case Example Token Size Requirements Đối với ứng dụng chatbot, kích thước token thông thường: Input: khoảng 50-150 tokens, hỗ trợ người dùng hỏi 1-2 câu và Output: khoảng 100-300 tokens, giúp mô hình phản hồi ngắn gọn nhưng chi tiết.\nSample Command ./build/bin/llama-cli -m ./models/SmolLM2-1.7B-Instruct-f16.gguf \\\\ -ngl 99 -n 512 --ctx-size 8192 -sm row --temp 0\r--single-turn \\\\ -sys \u0026#34;You are a helpful assistant\u0026#34; -p \u0026#34;Hello\u0026#34; Hình 3: Chatbot example\nCommand Explanation -m ./models/SmolLM2-1.7B-Instruct-f16.gguf : Chỉ định file model sử dụng -ngl 99 : Gán 99 GPU layers để đạt hiệu suất tối ưu -n 512 : Tối đa 512 output tokens (đủ cho 100-300 tokens cần thiết) --ctx-size 8192 : Đặt kích thước context window để xử lý hội thoại phức tạp -sm row : Chia hàng across GPUs --temp 0 : Đặt temperature bằng 0 để giảm tính sáng tạo --single-turn : Tối ưu cho các phản hồi một lượt -sys \u0026ldquo;You are a helpful assistant\u0026rdquo; : Thiết lập system prompt định nghĩa vai trò trợ lý -p \u0026ldquo;Hello\u0026rdquo; : Nhập prompt của người dùng Text Summarization Example Dòng lệnh dưới đây cho thấy SmolLM2-1.7B chạy tác vụ tóm tắt văn bản:\nPROMPT_TEXT=\u0026#34;Summarize the following text: Amazon DynamoDB is a serverless, NoSQL database service that allows you to develop modern applications at any scale. As a serverless database, you only pay for what you use and DynamoDB scales to zero, has no cold starts, no version upgrades, no maintenance windows, no patching, and no downtime maintenance. DynamoDB offers a broad set of security controls and compliance standards. For globally distributed applications, DynamoDB global tables is a multi-Region, multi-active database with a 99.999% availability SLA and increased resilience. DynamoDB reliability is supported with managed backups, point-in-time recovery, and more. With DynamoDB streams, you can build serverless event-driven applications.\u0026#34; ./build/bin/llama-cli -m ./models/SmolLM2-1.7B-Instruct-f16.gguf \\\\ -ngl 99 -n 512 --ctx-size 8192 -sm row --single-turn \\\\ -sys \u0026#34;You are a technical writer\u0026#34; \\\\ --prompt \u0026#34;\\$PROMPT_TEXT\u0026#34; Hình 4: Summarization example\nCleaning Up Để tránh chi phí phát sinh không cần thiết, hãy thực hiện các bước sau để xóa tài nguyên sau khi hoàn tất:\nTerminate EC2 instance để ngừng tính phí. Xác minh rằng EBS volume 300 GiB đã được xóa đúng cách bằng cách kiểm tra mục Volumes trong phần Elastic Block Store.Nếu vẫn còn volume, hãy chọn và thực hiện: Actions \u0026gt; Delete volume. Kết luận Bài viết này đã hướng dẫn bạn từng bước triển khai SLMs vào môi trường AWS on-premises hoặc edge nhằm đáp ứng các nhu cầu xử lý dữ liệu cục bộ. Phần đầu bài viết đã thảo luận về lợi ích kinh doanh của SLMs, bao gồm: Thời gian suy luận (inference time) nhanh hơn, Giảm chi phí vận hành và cải thiện kết quả đầu ra của mô hình. Các SLMs được triển khai bằng Llama.cpp và tối ưu hóa cho các use case cụ thể có thể cung cấp dịch vụ người dùng hiệu quả từ edge theo cách mở rộng linh hoạt (scalable). Các tham số tối ưu hóa được mô tả trong bài viết này cung cấp nhiều phương pháp cấu hình khác nhau để điều chỉnh mô hình cho các kịch bản triển khai đa dạng. Bạn có thể làm theo các bước và kỹ thuật được trình bày trong bài để triển khai generative AI phù hợp với yêu cầu về data residency, latency, hoặc InfoSec compliance, đồng thời vận hành hiệu quả trong giới hạn tài nguyên của môi trường edge computing. Để tìm hiểu thêm, hãy truy cập AWS Local Zones và AWS Outposts.\n"
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": "Hội nghị AWS EUC New York Summit: EUC201 - The AI Advantage: Unlocking the full potential of your EUC Services Bởi Dave Jaskie và Matt Aylward | ngày 27/06/2025 | Amazon AppStream 2.0, Amazon Bedrock, Amazon Bedrock Agents, Amazon CloudWatch, Amazon WorkSpaces, Desktop \u0026amp; Application Streaming, End User Computing.\nBạn có đang tìm cách ứng dụng AI để tối ưu các tác vụ quản trị và nâng cao năng suất người dùng không?\nTrong bối cảnh kỹ thuật số liên tục phát triển, thành công của chiến lược End-User Computing (EUC) của doanh nghiệp phụ thuộc vào khả năng người dùng cuối tiếp cận và sử dụng dịch vụ hiệu quả.\nPhiên thảo luận tương tác này sẽ trình bày cách bạn có thể tận dụng AI agentic của Amazon Bedrock kết hợp với Amazon WorkSpaces và Amazon CloudWatch.\nNhững công cụ này giúp tự động hóa các tác vụ quản trị và cung cấp thông tin chi tiết có thể hành động (actionable insights) từ các metrics và logs quan trọng.\nTrong buổi học, người tham dự sẽ được giới thiệu các chiến lược EUC quan trọng và học cách AI có thể chuyển đổi quy trình làm việc của họ. Bạn sẽ khám phá cách Amazon Bedrock giúp đơn giản hóa các quy trình phức tạp, mang đến cho quản trị viên các công cụ cần thiết để tăng hiệu suất. Ngoài ra, các bài thực hành (hands-on) cùng Amazon CloudWatch sẽ giúp bạn học cách thu thập dữ liệu quan trọng - bao gồm user connectivity, platforms, và IP addresses. Thông qua Amazon Bedrock, bạn sẽ phân tích dữ liệu để rút ra các thông tin giúp tối ưu hoạt động người dùng cuối.\nPhiên này không chỉ mang lại kiến thức chuyên sâu, mà còn là trải nghiệm học tập thực tế. Người tham dự sẽ có hiểu biết rõ ràng về cách tích hợp AI và CloudWatch vào framework hiện có. Dù bạn là IT professional, system administrator, hay decision-maker, đây là cơ hội để nâng cao chiến lược EUC của bạn và đảm bảo cả admin lẫn người dùng đều hưởng lợi từ các công cụ và quy trình tối ưu hóa.\nBuổi builders session này diễn ra vào ngày 16 tháng 7, lúc 9:15 AM EDT, tại Javits Convention Center. Vui lòng thêm buổi này vào lịch trình của bạn qua liên kết sau khi đăng ký.\nĐừng bỏ lỡ cơ hội thay đổi cách bạn tiếp cận end-user computing. Hãy tham gia để khai phá tiềm năng của AI, tự động hóa với sự tự tin, và nắm bắt insights giúp tổ chức của bạn tiến xa hơn. Đăng ký ngay hôm nay!\nDave Jaskie có 15 năm kinh nghiệm trong lĩnh vực End User Computing. Ngoài công việc, Dave thích du lịch và leo núi cùng vợ và 4 người con. Matt Aylward là Solutions Architect tại Amazon Web Services (AWS), chuyên tạo ra các giải pháp đơn giản cho những thách thức kinh doanh phức tạp. Trước khi gia nhập AWS, Matt đã làm việc trong lĩnh vực triển khai hạ tầng, kiểm thử khôi phục sau thảm họa, và quản lý phân phối ứng dụng ảo. Ngoài giờ làm, anh thích dành thời gian cho gia đình, xem phim, và đi dã ngoại cùng chú chó năng động của mình. "
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/3-blogstranslated/3.3-blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": "Cách thiết lập cảnh báo tự động cho các AWS Savings Plans mới mua Bởi Syed Muhammad Tawha và Dan Johns | ngày 26/06/2025 | Amazon Simple Notification Service (SNS), AWS Cloud Financial Management, AWS CloudFormation, Cloud Cost Optimization\nKhi tổ chức phát triển, các nhóm FinOps cần một cái nhìn tổng thể về AWS Savings Plans để tối ưu hóa việc sử dụng. Giải pháp này giúp giám sát tự động và thiết lập cảnh báo nhằm phát hiện các Savings Plans sử dụng kém hiệu quả trong thời hạn hoàn trả hợp lệ.\nKhi mua Savings Plan, bạn cam kết sử dụng trong 1 hoặc 3 năm. Các Savings Plan có cam kết theo giờ ≤ $100 có thể được hoàn trả nếu được mua trong vòng 7 ngày gần nhất và trong cùng tháng dương lịch, miễn là bạn chưa vượt quá giới hạn hoàn trả. Sau khi tháng kết thúc (theo UTC), Savings Plans đó không thể hoàn trả.\nỞ bài viết này, chúng tôi cung cấp các AWS CloudFormation templates giúp tạo AWS Step Functions state machine, Amazon Simple Notification Service (SNS) topic, Amazon EventBridge scheduler, và các AWS Identity and Access Management (IAM) roles cần thiết để giám sát tự động các Savings Plans mới mua và phát hiện những gói sử dụng thấp.\nTổng quan giải pháp Giải pháp này tuân theo AWS security best practices bằng cách tách triển khai trên hai tài khoản. Một CloudFormation stack sẽ được tạo trong Management account để thiết lập các IAM roles cần thiết để truy xuất dữ liệu sử dụng của Savings Plans. Một CloudFormation stack khác sẽ được triển khai trong Member Account đã chọn trong AWS Organization của bạn.\nCloudFormation stack trong Member Account tạo một state machine thực hiện assume role trong Management Account của bạn và phân tích tất cả Savings Plans trong Management Account, bao gồm cả những gói đã được mua trong toàn bộ tổ chức của bạn. Workflow lọc các Savings Plans hoạt động theo ngày mua, tập trung vào các plan được mua trong 7 ngày gần nhất và tháng hiện tại. Sau đó, hệ thống đánh giá tỷ lệ sử dụng của chúng và xác định các plan dưới ngưỡng định sẵn.\nState machine sẽ thực thi theo tần suất bạn chỉ định và sử dụng Amazon SNS để gửi cảnh báo qua email đến các địa chỉ bạn cung cấp khi tạo CloudFormation stack. Các cảnh báo này sẽ chứa thông tin chi tiết về các Savings Plans sử dụng thấp và hướng dẫn về quy trình hoàn trả.\nHình 1: Kiến trúc AWS - Member Account nhận quyền đọc dữ liệu từ Management Account và kích hoạt Step Function để gửi cảnh báo qua SNS.\nTriển khai giải pháp Điều kiện tiên quyết Có AWS Account\nCó IAM permissions để tạo CloudFormation Stack và IAM Role trong Management Account\nCó IAM permissions để tạo Step Functions, SNS, IAM Roles, và EventBridge trong Member Account\nTriển khai giải pháp Trong phần này, chúng ta sẽ triển khai các tài nguyên cho giải pháp này trong tài khoản của bạn:\nPhần 1 - Triển khai trong Member Account Trong phần này, chúng ta sẽ triển khai các tài nguyên cho giải pháp này trong Member Account đã chọn.\nĐăng nhập vào AWS Management Console của Member Account nơi bạn muốn triển khai giải pháp.\nTriển khai CloudFormation Stack này Launch Stack\nCung cấp Stack Name là new-sp-utilization-alert-member.\nTrong tham số AlertEmails, nhập danh sách email cách nhau bởi dấu phẩy mà sẽ nhận thông báo về Savings Plans sử dụng kém.\nTrong tham số ManagementAccountId, nhập 12 chữ số AWS Account ID của Management Account.\nTrong tham số ScheduleExpression, chỉ định tần suất thực thi cho Step Functions state machine theo định dạng cron (mặc định là hàng ngày vào lúc 9 AM UTC).\nTrong tham số UtilizationThreshold, chỉ định tỷ lệ sử dụng tối thiểu cho Savings Plans của bạn. Bạn sẽ nhận thông báo khi tỷ lệ sử dụng giảm dưới ngưỡng này.\nNhấn Next, chọn ô acknowledgment, và tạo stack.\nChờ cho đến khi stack hoàn thành và hiển thị trạng thái CREATE-COMPLETE.\nBạn sẽ nhận một email để xác nhận đăng ký nhận thông báo từ SNS topic do stack này tạo. Vui lòng xác nhận đăng ký để bắt đầu nhận thông báo.\nTruy cập vào tab Outputs của stack vừa tạo và ghi lại các giá trị của ExecutionRoleArn và StateMachineArn, bạn sẽ cần chúng trong phần tiếp theo.\nPhần 2 - Triển khai trong Management Account Đăng nhập vào AWS Management Console. Lưu ý: Đây phải là tài khoản giống như tài khoản đã nhập trong tham số ManagementAccountId ở phần trước.\nTriển khai CloudFormation Stack này Launch Stack\nCung cấp Stack Name là new-sp-utilization-alert-management.\nTrong tham số ExecutionRoleArn, cung cấp giá trị đã sao chép từ stack outputs của stack đã triển khai trong Member Account.\nTrong tham số StateMachineArn, cung cấp giá trị đã sao chép từ stack outputs của stack đã triển khai trong Member Account.\nNhấn Next, chọn ô acknowledgment, và tạo stack.\nChờ cho đến khi stack hoàn thành và hiển thị trạng thái CREATE-COMPLETE.\nKiểm thử giải pháp Bây giờ mà Step Functions state machine và các tài nguyên liên quan đã được triển khai trong Member Account, chúng ta sẽ kiểm tra việc triển khai:\nĐăng nhập lại vào AWS Management Console của Member Account nơi bạn đã triển khai phần 1 của giải pháp này.\nTruy cập vào tab Resources trong CloudFormation stack và tìm SavingsPlansAlerts Step Functions state machine. Nhấp vào hyperlink màu xanh dương.\nBạn sẽ được chuyển hướng đến Step Functions console. Nhấn Start execution ở bên phải.\nXem chi tiết execution trong mục Events để theo dõi tiến trình của state machine. Nếu có Savings Plans được mua trong vòng 7 ngày gần nhất và tháng hiện tại, bạn sẽ nhận email thông báo.\nMột execution thành công được hiển thị bằng ô màu xanh lá trong Graph view. Nếu bất kỳ Savings Plans nào rơi dưới ngưỡng tỷ lệ sử dụng đã chỉ định, bạn sẽ nhận email tại địa chỉ đã cung cấp.\nDọn dẹp tài nguyên Tất cả các tài nguyên đã triển khai cho giải pháp này có thể được xóa bằng cách xóa CloudFormation stacks. Bạn có thể xóa stack thông qua AWS Management Console hoặc AWS CLI.\nĐể xóa stack trong Management Account (CLI):\naws cloudformation delete-stack --stack-name new-sp-utilization-alert_management Để xóa stack trong Member Account (CLI):\naws cloudformation delete-stack --stack-name new-sp-utilization-alert_member Hiểu và xử lý cảnh báo Khi bạn nhận được cảnh báo về Savings Plans sử dụng thấp, bạn nên xem lại chi tiết sử dụng được cung cấp trong thông báo email. Phân tích các chỉ số sử dụng của bạn so với cam kết ban đầu khi mua Savings Plan, và điều tra xem tỷ lệ sử dụng thấp có phải là điều đã dự kiến hay do các yếu tố khác như di chuyển khối lượng công việc, thay đổi kiến trúc, hoặc ước tính sai nhu cầu công suất. Hãy cân nhắc hoàn trả Savings Plan nếu tỷ lệ sử dụng vẫn duy trì dưới ngưỡng của bạn, plan được mua trong vòng 7 ngày gần nhất, mua trong tháng hiện tại, và cam kết mỗi giờ ≤ $100. Ghi lại lý do hoàn trả để tham khảo và lập kế hoạch cho tương lai.\nKết luận Bài viết đã hướng dẫn cách sử dụng Savings Plan và Cost Explorer APIs để xác định Savings Plans sử dụng thấp trong tổ chức của bạn. Sau đó, chúng tôi đã minh họa cách sử dụng Step Functions State Machine để lọc các Savings Plans được mua trong 7 ngày gần nhất và tháng hiện tại, điều này rất quan trọng vì bạn có thể hoàn trả Savings Plans trong thời gian hoàn trả hợp lệ nếu chúng không được sử dụng hiệu quả. Để biết thêm chi tiết về việc hoàn trả Savings Plan, tham khảo tài liệu Returning a Purchased Savings Plan\nSyed Muhammad Tawha Syed Muhammad Tawha là Principal Technical Account Manager tại AWS, có trụ sở tại Dublin, Ireland. Tawha chuyên môn về Storage, Resilience và Cloud Cost Optimization. Anh đam mê giúp đỡ khách hàng của AWS tối ưu hóa chi phí và cải thiện hiệu suất. Ngoài công việc, Tawha còn yêu thích dành thời gian với bạn bè và gia đình Dan Johns Dan Johns là Senior Solutions Architect Engineer, hỗ trợ khách hàng xây dựng trên AWS và đáp ứng các yêu cầu kinh doanh. Ngoài công việc, anh thích đọc sách, dành thời gian với gia đình và tự động hóa các tác vụ trong nhà. "
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/5-workshop/5.3-s3-vpc/5.3.1-create-gwe/",
	"title": "Create a gateway endpoint",
	"tags": [],
	"description": "",
	"content": " Open the Amazon VPC console In the navigation pane, choose Endpoints, then click Create Endpoint: You will see 6 existing VPC endpoints that support AWS Systems Manager (SSM). These endpoints were deployed automatically by the CloudFormation Templates for this workshop.\nIn the Create endpoint console: Specify name of the endpoint: s3-gwe In service category, choose AWS services In Services, type s3 in the search box and choose the service with type gateway For VPC, select VPC Cloud from the drop-down. For Configure route tables, select the route table that is already associated with two subnets (note: this is not the main route table for the VPC, but a second route table created by CloudFormation). For Policy, leave the default option, Full Access, to allow full access to the service. You will deploy a VPC endpoint policy in a later lab module to demonstrate restricting access to S3 buckets based on policies. Do not add a tag to the VPC endpoint at this time. Click Create endpoint, then click x after receiving a successful creation message. "
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/4-eventparticipated/4.1-event1/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": "Summary Report: “GenAI \u0026amp; Data” Event Objectives Update on GenAI trends and strategies on AWS Learn how to build a unified data foundation for AI/Analytics Introduction to the AI-Driven Development Lifecycle (AI-DLC) in software development Gain insights into security for GenAI applications and the role of AI Agents in enterprises Speakers Jun Kai Loke – AI/ML Specialist SA, AWS Kien Nguyen – Solutions Architect, AWS Tamelly Lim – Storage Specialist SA, AWS Binh Tran – Senior Solutions Architect, AWS Taiki Dang – Solutions Architect, AWS Michael Armentano – Principal WW GTM Specialist, AWS Key Highlights 1. Building a Unified Data Foundation on AWS for AI \u0026amp; Analytics Strategies for constructing a unified, scalable data foundation for AI/Analytics. End-to-end data pipeline: ingestion → storage → processing → access → governance. Overcoming 3 silos (Data/People/Business); enable self-service with standardized governance. Core services: S3, Glue, Redshift, Lake Formation, OpenSearch, Kinesis/MSK. 2. Building the Future: GenAI Adoption Strategy on AWS Vision and trends of GenAI; roadmap for enterprise adoption. Amazon Bedrock: model choice, customization/RAG, guardrails, cost/latency optimization. AgentCore: framework-independent runtime, tool integration gateway, identity \u0026amp; observability. Amazon Nova and ecosystem frameworks (CrewAI, LangGraph, LlamaIndex, Strands). 3. Securing Generative AI Applications with AWS Risks per OWASP LLM (LLM01/02); ensuring safe output handling. Security at multiple layers: infrastructure, models, applications; IAM, encryption, zero-trust, continuous monitoring. 5 Security Pillars: Compliance \u0026amp; Governance, Legal \u0026amp; Privacy, Controls, Risk Management, Resilience. Generative AI Security Scoping Matrix (Scope 1 → 5): from consumer apps to self-trained models. Bedrock Guardrails: filter sensitive content with configurable thresholds. Human-in-the-loop: human approval/intervention when needed. Observability (OpenTelemetry): transparent monitoring, logging, and tracing of AI behaviors. 4. Beyond Automation: AI Agents as Productivity Multipliers Agentic AI: from assistants → multi-agent systems; less human oversight, more autonomy. Applications: customer support, BI with Amazon Q (QuickSight), workflow automation. Amazon Q in QuickSight: Build Dashboards/Reports, Data Q\u0026amp;A, Executive Summaries. Expected value: exponential productivity gains; requires strong data foundation \u0026amp; governance. 5. Reliability and Veracity of GenAI Challenge of hallucination → mitigated via Prompt Engineering, RAG, Fine-tuning, Parameter Tuning. RAG in action: user input → embeddings → contextual retrieval → LLM → grounded response. 6. AI-Driven Development Lifecycle (AI-DLC) An AI-centric lifecycle: Inception → Construction → Operation. Evolution: AI-Assisted → AI-Driven → AI-Managed; AI orchestrates, humans approve. Deployment infrastructure: IaC, automated testing, observability, risk management. 7. Amazon SageMaker (Unified Studio – Next Gen) One unified environment for data, analytics, and AI: SQL analytics, data processing, model development/training, GenAI app development, BI, streaming, search analytics.\nLakehouse + Governance: catalog/lineage, policy-based access, auditing; unified Data \u0026amp; AI governance.\nZero-ETL integration: core S3 ↔ Redshift, connections to Aurora, DynamoDB, RDS, OpenSearch, Kinesis/MSK, Salesforce, SAP, ServiceNow.\nFull MLOps: pipelines/experiments, model registry, deployment endpoints, Feature Store, monitoring.\nIntegrated with Bedrock \u0026amp; JumpStart: access to foundation models (via Bedrock), reference solutions, and accelerated deployment on SageMaker.\nSDLC automation: From planning to maintenance\nCode transformation: Java upgrade, .NET modernization\nAWS Transform agents: VMware, Mainframe, .NET migration\nKey Takeaways Design Mindset Business-first approach: Always start from business needs, not technology. Ubiquitous language: The importance of shared vocabulary between business and tech, especially in teamwork and communication with mentors. Bounded contexts: Understanding how to partition domains to avoid complexity when scaling. Architecture \u0026amp; Technology Unified Data Foundation: ingestion → storage → processing → access → governance. GenAI on AWS: Bedrock (model choice, guardrails, RAG), AgentCore (runtime, gateway, identity, observability), Nova LLMs. AI Agents: from assistants → multi-agent systems; real-world use cases like customer support and BI with Amazon Q. AI-DLC: AI as the core collaborator in SDLC (Inception → Construction → Operation). Security-first mindset: Guardrails, human-in-the-loop, governance \u0026amp; monitoring (OpenTelemetry). Strategy \u0026amp; Application Phased approach: Avoid rushing; need a clear roadmap for modernization \u0026amp; AI adoption. Zero-ETL \u0026amp; Unified Studio (SageMaker): Simplify data integration, centralize AI lifecycle management. ROI measurement: Not just cost savings, but agility and productivity. Applying to Work In my project:\nExperiment with AI Agents for workflows like registration/login or customer support. Apply validation/guardrails to ensure safe integration of GenAI features. In team projects (Sprint 0, serverless vs containerization):\nApply AI-DLC principles to split tasks logically: AI supports research/code generation, team reviews \u0026amp; approves. Understand when to use Lambda (serverless) vs ECS/Fargate (containers). In my learning path:\nRecognize the need for a business-first approach when writing documents and gathering requirements. Acknowledge that a strong data foundation is critical for any successful GenAI application. Event Experience Learned directly from AWS experts on Data, GenAI adoption, Security, AI Agents, and AI-DLC. Slides and case studies gave me a clear picture of how AgentCore works in real-world scenarios. Understood how AWS envisions the future of software development: AI not just as an assistant but as a core lifecycle component. Realized that successful GenAI adoption requires solid data foundation + strong security + structured strategy. Lesson learned AI Agents and AgentCore will soon become critical in enterprise applications → I should learn early to stay ahead. Data platform \u0026amp; governance are essential → not just coding, but also managing data properly. AI-DLC highlights AI’s role in future SDLC → I can experiment with small projects now. Security is not an afterthought; it must be built into GenAI systems from the start. Some event photos Add your event photos here\nOverall, the event not only provided technical knowledge but also helped me reshape my thinking about application design, system modernization, and cross-team collaboration.\n"
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/4-eventparticipated/4.2-event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": "Summary Report: \u0026ldquo;AI-Driven Development Life Cycle: Reimagining Software Engineering\u0026rdquo; Event Objectives Explore how AI is transforming the entire software development lifecycle, from planning to deployment. Learn how to integrate AI to increase productivity and focus on high-value creative tasks.\nEvent Information Time: 2:00 PM - 4:30 PM, Friday, October 3rd, 2025 Location: AWS Event Hall, L26 Bitexco Tower, HCMC Speakers: Toan Huynh, My Nguyen Key Highlights 1. AI in Development - Outcomes AI adoption brings:\nVelocity - Reduce time-to-market Quality - Meet usability \u0026amp; reliability expectations Market Responsiveness - React quickly to market changes Innovation - Drive innovation Developer Engagement - Increase developer satisfaction Productivity - Increase value, reduce costs 2. Challenges with AI Development AI-Managed: Unreliable, hard to explain, lacks flexibility\nAI-Assisted: Not truly effective, manual inefficiencies, technical debt accumulation\n3. AI-Driven Development Lifecycle (AI-DLC) Core Concept:\nAI as Collaborator: AI assists developers, humans control critical decisions Human-Centric: Developers remain central, AI enhances not replaces Accelerated Delivery: Development cycles reduced from weeks/months to hours/days Two phases:\nInception: Build Context → User Stories → Plan Construction: Domain Model → Generate code → Deploy with IaaC 4. 5-Stage Sequential Process Product Owner → 2. Architect (Design) → 3. Architect (Construction) → 4. Engineer (POC) → 5. Engineer (MVP) 5. Anti-Patterns - 7 Things to Avoid Don\u0026rsquo;t single-shot multi-step problems Maximize semantics-to-token ratio Refresh context strategically Control AI over-reach Model knows old better than new Brownfield needs special context building Think surgical precision 6. Amazon Q Developer Prompt Structure: Role → Plan (markdown with checkboxes) → Task\nExample: Build travel booking app with AI integration\nWorkflow: Create folder → User stories → Clarification → Checkbox → Review → Execute\n7. Kiro - AI-Powered Coding Assistant Kiro is AWS\u0026rsquo;s AI coding assistant with 4 key features:\nAgent Hooks: Auto-trigger tasks on events (file save), generate docs/tests/optimize VS Code Compatible: Support plugins, themes, settings Claude Models: Sonnet 3.7/4 with powerful reasoning Enterprise Security: Built \u0026amp; operated by AWS Advantages: High automation, context-aware, documentation-driven, enterprise-ready\nKey Takeaways Participating in the \u0026ldquo;GenAI-powered App-DB Modernization\u0026rdquo; workshop was an incredibly enriching experience, providing me with a comprehensive perspective on modernizing applications and databases using contemporary methods and tools. Some notable highlights include:\nLearning from highly skilled speakers Speakers from AWS and major technology organizations shared best practices in modern application design. Through real-world case studies, I gained a deeper understanding of how to apply Domain-Driven Design (DDD) and Event-Driven Architecture to large-scale projects. Hands-on technical exposure Participating in event storming sessions helped me visualize how to model business processes into domain events. Learned how to split microservices and define bounded contexts to manage complexity in large systems. Understood the trade-offs between synchronous and asynchronous communication, as well as integration patterns like pub/sub, point-to-point, and streaming. Leveraging modern tools Directly explored Amazon Q Developer, an AI tool supporting the SDLC from planning to maintenance. Learned how to automate code transformation and pilot serverless with AWS Lambda, thereby enhancing development productivity. Lessons learned Applying DDD and event-driven patterns helps reduce coupling while increasing scalability and resilience for systems. Modernization strategies require a phased approach and ROI measurement; rushing to transform the entire system should be avoided. AI tools like Amazon Q Developer can boost productivity when integrated into the current development workflow. Some event photos Add your event photos here\nOverall, the event not only provided technical knowledge but also helped me reshape my thinking about application design, system modernization, and cross-team collaboration.\n"
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report Student Information: Full Name: Uong Tuan Vu\nPhone Number: 0329 06 024\nEmail: vuutse180241@fpt.edu.vn\nUniversity: FPT University Campus Ho Chi Minh\nMajor: Information Technology\nClass: SE180241\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 12/08/2025 to 12/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/5-workshop/5.1-workshop-overview/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "VPC endpoints VPC endpoints are virtual devices. They are horizontally scaled, redundant, and highly available VPC components. They allow communication between your compute resources and AWS services without imposing availability risks. Compute resources running in VPC can access Amazon S3 using a Gateway endpoint. PrivateLink interface endpoints can be used by compute resources running in VPC or on-premises. Workshop overview In this workshop, you will use two VPCs.\n\u0026ldquo;VPC Cloud\u0026rdquo; is for cloud resources such as a Gateway endpoint and an EC2 instance to test with. \u0026ldquo;VPC On-Prem\u0026rdquo; simulates an on-premises environment such as a factory or corporate datacenter. An EC2 instance running strongSwan VPN software has been deployed in \u0026ldquo;VPC On-prem\u0026rdquo; and automatically configured to establish a Site-to-Site VPN tunnel with AWS Transit Gateway. This VPN simulates connectivity from an on-premises location to the AWS cloud. To minimize costs, only one VPN instance is provisioned to support this workshop. When planning VPN connectivity for your production workloads, AWS recommends using multiple VPN devices for high availability. "
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/5-workshop/5.4-s3-onprem/5.4.1-prepare/",
	"title": "Prepare the environment",
	"tags": [],
	"description": "",
	"content": "To prepare for this part of the workshop you will need to:\nDeploying a CloudFormation stack Modifying a VPC route table. These components work together to simulate on-premises DNS forwarding and name resolution.\nDeploy the CloudFormation stack The CloudFormation template will create additional services to support an on-premises simulation:\nOne Route 53 Private Hosted Zone that hosts Alias records for the PrivateLink S3 endpoint One Route 53 Inbound Resolver endpoint that enables \u0026ldquo;VPC Cloud\u0026rdquo; to resolve inbound DNS resolution requests to the Private Hosted Zone One Route 53 Outbound Resolver endpoint that enables \u0026ldquo;VPC On-prem\u0026rdquo; to forward DNS requests for S3 to \u0026ldquo;VPC Cloud\u0026rdquo; Click the following link to open the AWS CloudFormation console. The required template will be pre-loaded into the menu. Accept all default and click Create stack. It may take a few minutes for stack deployment to complete. You can continue with the next step without waiting for the deployemnt to finish.\nUpdate on-premise private route table This workshop uses a strongSwan VPN running on an EC2 instance to simulate connectivty between an on-premises datacenter and the AWS cloud. Most of the required components are provisioned before your start. To finalize the VPN configuration, you will modify the \u0026ldquo;VPC On-prem\u0026rdquo; routing table to direct traffic destined for the cloud to the strongSwan VPN instance.\nOpen the Amazon EC2 console\nSelect the instance named infra-vpngw-test. From the Details tab, copy the Instance ID and paste this into your text editor\nNavigate to the VPC menu by using the Search box at the top of the browser window.\nClick on Route Tables, select the RT Private On-prem route table, select the Routes tab, and click Edit Routes.\nClick Add route. Destination: your Cloud VPC cidr range Target: ID of your infra-vpngw-test instance (you saved in your editor at step 1) Click Save changes "
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/1-worklog/1.8-week8/",
	"title": "Week 8 Worklog",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 8 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 8 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/1-worklog/1.9-week9/",
	"title": "Week 9 Worklog",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 9 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 9 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": "On this page, you will need to introduce your worklog. How did you complete it? How many weeks did you take to complete the program? What did you do in those weeks?\nTypically, and as a standard, a worklog is carried out over about 3 months (throughout the internship period) with weekly contents as follows:\nWeek 1: Getting familiar with AWS and basic AWS services\nWeek 2: AWS Transit Gateway, Compute VM Services and attending Vietnam Cloud Day 2025\nWeek 3: AWS Storage Services, AWS Backup, VM Import/Export and FSx Windows\nWeek 4: Doing task C\u0026hellip;\nWeek 5: Doing task D\u0026hellip;\nWeek 6: Doing task E\u0026hellip;\nWeek 7: Doing task G\u0026hellip;\nWeek 8: Doing task H\u0026hellip;\nWeek 9: Doing task I\u0026hellip;\nWeek 10: Doing task L\u0026hellip;\nWeek 11: Doing task M\u0026hellip;\nWeek 12: Doing task N\u0026hellip;\n"
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/5-workshop/5.4-s3-onprem/5.4.2-create-interface-enpoint/",
	"title": "Create an S3 Interface endpoint",
	"tags": [],
	"description": "",
	"content": "In this section you will create and test an S3 interface endpoint using the simulated on-premises environment deployed as part of this workshop.\nReturn to the Amazon VPC menu. In the navigation pane, choose Endpoints, then click Create Endpoint.\nIn Create endpoint console:\nName the interface endpoint In Service category, choose aws services In the Search box, type S3 and press Enter. Select the endpoint named com.amazonaws.us-east-1.s3. Ensure that the Type column indicates Interface. For VPC, select VPC Cloud from the drop-down. Make sure to choose \u0026ldquo;VPC Cloud\u0026rdquo; and not \u0026ldquo;VPC On-prem\u0026rdquo;\nExpand Additional settings and ensure that Enable DNS name is not selected (we will use this in the next part of the workshop) Select 2 subnets in the following AZs: us-east-1a and us-east-1b For Security group, choose SGforS3Endpoint: Keep the default policy - full access and click Create endpoint Congratulation on successfully creating S3 interface endpoint. In the next step, we will test the interface endpoint.\n"
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/5-workshop/5.2-prerequiste/",
	"title": "Prerequiste",
	"tags": [],
	"description": "",
	"content": "IAM permissions Add the following IAM permission policy to your user account to deploy and cleanup this workshop.\n{\r\u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;,\r\u0026#34;Statement\u0026#34;: [\r{\r\u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;,\r\u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;,\r\u0026#34;Action\u0026#34;: [\r\u0026#34;cloudformation:*\u0026#34;,\r\u0026#34;cloudwatch:*\u0026#34;,\r\u0026#34;ec2:AcceptTransitGatewayPeeringAttachment\u0026#34;,\r\u0026#34;ec2:AcceptTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:AllocateAddress\u0026#34;,\r\u0026#34;ec2:AssociateAddress\u0026#34;,\r\u0026#34;ec2:AssociateIamInstanceProfile\u0026#34;,\r\u0026#34;ec2:AssociateRouteTable\u0026#34;,\r\u0026#34;ec2:AssociateSubnetCidrBlock\u0026#34;,\r\u0026#34;ec2:AssociateTransitGatewayRouteTable\u0026#34;,\r\u0026#34;ec2:AssociateVpcCidrBlock\u0026#34;,\r\u0026#34;ec2:AttachInternetGateway\u0026#34;,\r\u0026#34;ec2:AttachNetworkInterface\u0026#34;,\r\u0026#34;ec2:AttachVolume\u0026#34;,\r\u0026#34;ec2:AttachVpnGateway\u0026#34;,\r\u0026#34;ec2:AuthorizeSecurityGroupEgress\u0026#34;,\r\u0026#34;ec2:AuthorizeSecurityGroupIngress\u0026#34;,\r\u0026#34;ec2:CreateClientVpnEndpoint\u0026#34;,\r\u0026#34;ec2:CreateClientVpnRoute\u0026#34;,\r\u0026#34;ec2:CreateCustomerGateway\u0026#34;,\r\u0026#34;ec2:CreateDhcpOptions\u0026#34;,\r\u0026#34;ec2:CreateFlowLogs\u0026#34;,\r\u0026#34;ec2:CreateInternetGateway\u0026#34;,\r\u0026#34;ec2:CreateLaunchTemplate\u0026#34;,\r\u0026#34;ec2:CreateNetworkAcl\u0026#34;,\r\u0026#34;ec2:CreateNetworkInterface\u0026#34;,\r\u0026#34;ec2:CreateNetworkInterfacePermission\u0026#34;,\r\u0026#34;ec2:CreateRoute\u0026#34;,\r\u0026#34;ec2:CreateRouteTable\u0026#34;,\r\u0026#34;ec2:CreateSecurityGroup\u0026#34;,\r\u0026#34;ec2:CreateSubnet\u0026#34;,\r\u0026#34;ec2:CreateSubnetCidrReservation\u0026#34;,\r\u0026#34;ec2:CreateTags\u0026#34;,\r\u0026#34;ec2:CreateTransitGateway\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayPeeringAttachment\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayPrefixListReference\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayRoute\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayRouteTable\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:CreateVpc\u0026#34;,\r\u0026#34;ec2:CreateVpcEndpoint\u0026#34;,\r\u0026#34;ec2:CreateVpcEndpointConnectionNotification\u0026#34;,\r\u0026#34;ec2:CreateVpcEndpointServiceConfiguration\u0026#34;,\r\u0026#34;ec2:CreateVpnConnection\u0026#34;,\r\u0026#34;ec2:CreateVpnConnectionRoute\u0026#34;,\r\u0026#34;ec2:CreateVpnGateway\u0026#34;,\r\u0026#34;ec2:DeleteCustomerGateway\u0026#34;,\r\u0026#34;ec2:DeleteFlowLogs\u0026#34;,\r\u0026#34;ec2:DeleteInternetGateway\u0026#34;,\r\u0026#34;ec2:DeleteNetworkInterface\u0026#34;,\r\u0026#34;ec2:DeleteNetworkInterfacePermission\u0026#34;,\r\u0026#34;ec2:DeleteRoute\u0026#34;,\r\u0026#34;ec2:DeleteRouteTable\u0026#34;,\r\u0026#34;ec2:DeleteSecurityGroup\u0026#34;,\r\u0026#34;ec2:DeleteSubnet\u0026#34;,\r\u0026#34;ec2:DeleteSubnetCidrReservation\u0026#34;,\r\u0026#34;ec2:DeleteTags\u0026#34;,\r\u0026#34;ec2:DeleteTransitGateway\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayPeeringAttachment\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayPrefixListReference\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayRoute\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayRouteTable\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:DeleteVpc\u0026#34;,\r\u0026#34;ec2:DeleteVpcEndpoints\u0026#34;,\r\u0026#34;ec2:DeleteVpcEndpointServiceConfigurations\u0026#34;,\r\u0026#34;ec2:DeleteVpnConnection\u0026#34;,\r\u0026#34;ec2:DeleteVpnConnectionRoute\u0026#34;,\r\u0026#34;ec2:Describe*\u0026#34;,\r\u0026#34;ec2:DetachInternetGateway\u0026#34;,\r\u0026#34;ec2:DisassociateAddress\u0026#34;,\r\u0026#34;ec2:DisassociateRouteTable\u0026#34;,\r\u0026#34;ec2:GetLaunchTemplateData\u0026#34;,\r\u0026#34;ec2:GetTransitGatewayAttachmentPropagations\u0026#34;,\r\u0026#34;ec2:ModifyInstanceAttribute\u0026#34;,\r\u0026#34;ec2:ModifySecurityGroupRules\u0026#34;,\r\u0026#34;ec2:ModifyTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:ModifyVpcAttribute\u0026#34;,\r\u0026#34;ec2:ModifyVpcEndpoint\u0026#34;,\r\u0026#34;ec2:ReleaseAddress\u0026#34;,\r\u0026#34;ec2:ReplaceRoute\u0026#34;,\r\u0026#34;ec2:RevokeSecurityGroupEgress\u0026#34;,\r\u0026#34;ec2:RevokeSecurityGroupIngress\u0026#34;,\r\u0026#34;ec2:RunInstances\u0026#34;,\r\u0026#34;ec2:StartInstances\u0026#34;,\r\u0026#34;ec2:StopInstances\u0026#34;,\r\u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsEgress\u0026#34;,\r\u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsIngress\u0026#34;,\r\u0026#34;iam:AddRoleToInstanceProfile\u0026#34;,\r\u0026#34;iam:AttachRolePolicy\u0026#34;,\r\u0026#34;iam:CreateInstanceProfile\u0026#34;,\r\u0026#34;iam:CreatePolicy\u0026#34;,\r\u0026#34;iam:CreateRole\u0026#34;,\r\u0026#34;iam:DeleteInstanceProfile\u0026#34;,\r\u0026#34;iam:DeletePolicy\u0026#34;,\r\u0026#34;iam:DeleteRole\u0026#34;,\r\u0026#34;iam:DeleteRolePolicy\u0026#34;,\r\u0026#34;iam:DetachRolePolicy\u0026#34;,\r\u0026#34;iam:GetInstanceProfile\u0026#34;,\r\u0026#34;iam:GetPolicy\u0026#34;,\r\u0026#34;iam:GetRole\u0026#34;,\r\u0026#34;iam:GetRolePolicy\u0026#34;,\r\u0026#34;iam:ListPolicyVersions\u0026#34;,\r\u0026#34;iam:ListRoles\u0026#34;,\r\u0026#34;iam:PassRole\u0026#34;,\r\u0026#34;iam:PutRolePolicy\u0026#34;,\r\u0026#34;iam:RemoveRoleFromInstanceProfile\u0026#34;,\r\u0026#34;lambda:CreateFunction\u0026#34;,\r\u0026#34;lambda:DeleteFunction\u0026#34;,\r\u0026#34;lambda:DeleteLayerVersion\u0026#34;,\r\u0026#34;lambda:GetFunction\u0026#34;,\r\u0026#34;lambda:GetLayerVersion\u0026#34;,\r\u0026#34;lambda:InvokeFunction\u0026#34;,\r\u0026#34;lambda:PublishLayerVersion\u0026#34;,\r\u0026#34;logs:CreateLogGroup\u0026#34;,\r\u0026#34;logs:DeleteLogGroup\u0026#34;,\r\u0026#34;logs:DescribeLogGroups\u0026#34;,\r\u0026#34;logs:PutRetentionPolicy\u0026#34;,\r\u0026#34;route53:ChangeTagsForResource\u0026#34;,\r\u0026#34;route53:CreateHealthCheck\u0026#34;,\r\u0026#34;route53:CreateHostedZone\u0026#34;,\r\u0026#34;route53:CreateTrafficPolicy\u0026#34;,\r\u0026#34;route53:DeleteHostedZone\u0026#34;,\r\u0026#34;route53:DisassociateVPCFromHostedZone\u0026#34;,\r\u0026#34;route53:GetHostedZone\u0026#34;,\r\u0026#34;route53:ListHostedZones\u0026#34;,\r\u0026#34;route53domains:ListDomains\u0026#34;,\r\u0026#34;route53domains:ListOperations\u0026#34;,\r\u0026#34;route53domains:ListTagsForDomain\u0026#34;,\r\u0026#34;route53resolver:AssociateResolverEndpointIpAddress\u0026#34;,\r\u0026#34;route53resolver:AssociateResolverRule\u0026#34;,\r\u0026#34;route53resolver:CreateResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:CreateResolverRule\u0026#34;,\r\u0026#34;route53resolver:DeleteResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:DeleteResolverRule\u0026#34;,\r\u0026#34;route53resolver:DisassociateResolverEndpointIpAddress\u0026#34;,\r\u0026#34;route53resolver:DisassociateResolverRule\u0026#34;,\r\u0026#34;route53resolver:GetResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:GetResolverRule\u0026#34;,\r\u0026#34;route53resolver:ListResolverEndpointIpAddresses\u0026#34;,\r\u0026#34;route53resolver:ListResolverEndpoints\u0026#34;,\r\u0026#34;route53resolver:ListResolverRuleAssociations\u0026#34;,\r\u0026#34;route53resolver:ListResolverRules\u0026#34;,\r\u0026#34;route53resolver:ListTagsForResource\u0026#34;,\r\u0026#34;route53resolver:UpdateResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:UpdateResolverRule\u0026#34;,\r\u0026#34;s3:AbortMultipartUpload\u0026#34;,\r\u0026#34;s3:CreateBucket\u0026#34;,\r\u0026#34;s3:DeleteBucket\u0026#34;,\r\u0026#34;s3:DeleteObject\u0026#34;,\r\u0026#34;s3:GetAccountPublicAccessBlock\u0026#34;,\r\u0026#34;s3:GetBucketAcl\u0026#34;,\r\u0026#34;s3:GetBucketOwnershipControls\u0026#34;,\r\u0026#34;s3:GetBucketPolicy\u0026#34;,\r\u0026#34;s3:GetBucketPolicyStatus\u0026#34;,\r\u0026#34;s3:GetBucketPublicAccessBlock\u0026#34;,\r\u0026#34;s3:GetObject\u0026#34;,\r\u0026#34;s3:GetObjectVersion\u0026#34;,\r\u0026#34;s3:GetBucketVersioning\u0026#34;,\r\u0026#34;s3:ListAccessPoints\u0026#34;,\r\u0026#34;s3:ListAccessPointsForObjectLambda\u0026#34;,\r\u0026#34;s3:ListAllMyBuckets\u0026#34;,\r\u0026#34;s3:ListBucket\u0026#34;,\r\u0026#34;s3:ListBucketMultipartUploads\u0026#34;,\r\u0026#34;s3:ListBucketVersions\u0026#34;,\r\u0026#34;s3:ListJobs\u0026#34;,\r\u0026#34;s3:ListMultipartUploadParts\u0026#34;,\r\u0026#34;s3:ListMultiRegionAccessPoints\u0026#34;,\r\u0026#34;s3:ListStorageLensConfigurations\u0026#34;,\r\u0026#34;s3:PutAccountPublicAccessBlock\u0026#34;,\r\u0026#34;s3:PutBucketAcl\u0026#34;,\r\u0026#34;s3:PutBucketPolicy\u0026#34;,\r\u0026#34;s3:PutBucketPublicAccessBlock\u0026#34;,\r\u0026#34;s3:PutObject\u0026#34;,\r\u0026#34;secretsmanager:CreateSecret\u0026#34;,\r\u0026#34;secretsmanager:DeleteSecret\u0026#34;,\r\u0026#34;secretsmanager:DescribeSecret\u0026#34;,\r\u0026#34;secretsmanager:GetSecretValue\u0026#34;,\r\u0026#34;secretsmanager:ListSecrets\u0026#34;,\r\u0026#34;secretsmanager:ListSecretVersionIds\u0026#34;,\r\u0026#34;secretsmanager:PutResourcePolicy\u0026#34;,\r\u0026#34;secretsmanager:TagResource\u0026#34;,\r\u0026#34;secretsmanager:UpdateSecret\u0026#34;,\r\u0026#34;sns:ListTopics\u0026#34;,\r\u0026#34;ssm:DescribeInstanceProperties\u0026#34;,\r\u0026#34;ssm:DescribeSessions\u0026#34;,\r\u0026#34;ssm:GetConnectionStatus\u0026#34;,\r\u0026#34;ssm:GetParameters\u0026#34;,\r\u0026#34;ssm:ListAssociations\u0026#34;,\r\u0026#34;ssm:ResumeSession\u0026#34;,\r\u0026#34;ssm:StartSession\u0026#34;,\r\u0026#34;ssm:TerminateSession\u0026#34;\r],\r\u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;\r}\r]\r} Provision resources using CloudFormation In this lab, we will use N.Virginia region (us-east-1).\nTo prepare the workshop environment, deploy this CloudFormation Template (click link): PrivateLinkWorkshop . Accept all of the defaults when deploying the template.\nTick 2 acknowledgement boxes Choose Create stack The ClouddFormation deployment requires about 15 minutes to complete.\n2 VPCs have been created 3 EC2s have been created "
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nIn this section, you need to summarize the contents of the workshop that you plan to conduct.\nIoT Weather Platform for Lab Research A Unified AWS Serverless Solution for Real-Time Weather Monitoring 1. Executive Summary The IoT Weather Platform is designed for the ITea Lab team in Ho Chi Minh City to enhance weather data collection and analysis. It supports up to 5 weather stations, with potential scalability to 10-15, utilizing Raspberry Pi edge devices with ESP32 sensors to transmit data via MQTT. The platform leverages AWS Serverless services to deliver real-time monitoring, predictive analytics, and cost efficiency, with access restricted to 5 lab members via Amazon Cognito.\n2. Problem Statement What’s the Problem? Current weather stations require manual data collection, becoming unmanageable with multiple units. There is no centralized system for real-time data or analytics, and third-party platforms are costly and overly complex.\nThe Solution The platform uses AWS IoT Core to ingest MQTT data, AWS Lambda and API Gateway for processing, Amazon S3 for storage (including a data lake), and AWS Glue Crawlers and ETL jobs to extract, transform, and load data from the S3 data lake to another S3 bucket for analysis. AWS Amplify with Next.js provides the web interface, and Amazon Cognito ensures secure access. Similar to Thingsboard and CoreIoT, users can register new devices and manage connections, though this platform operates on a smaller scale and is designed for private use. Key features include real-time dashboards, trend analysis, and low operational costs.\nBenefits and Return on Investment The solution establishes a foundational resource for lab members to develop a larger IoT platform, serving as a study resource, and provides a data foundation for AI enthusiasts for model training or analysis. It reduces manual reporting for each station via a centralized platform, simplifying management and maintenance, and improves data reliability. Monthly costs are $0.66 USD per the AWS Pricing Calculator, with a 12-month total of $7.92 USD. All IoT equipment costs are covered by the existing weather station setup, eliminating additional development expenses. The break-even period of 6-12 months is achieved through significant time savings from reduced manual work.\n3. Solution Architecture The platform employs a serverless AWS architecture to manage data from 5 Raspberry Pi-based stations, scalable to 15. Data is ingested via AWS IoT Core, stored in an S3 data lake, and processed by AWS Glue Crawlers and ETL jobs to transform and load it into another S3 bucket for analysis. Lambda and API Gateway handle additional processing, while Amplify with Next.js hosts the dashboard, secured by Cognito. The architecture is detailed below:\nAWS Services Used AWS IoT Core: Ingests MQTT data from 5 stations, scalable to 15. AWS Lambda: Processes data and triggers Glue jobs (two functions). Amazon API Gateway: Facilitates web app communication. Amazon S3: Stores raw data in a data lake and processed outputs (two buckets). AWS Glue: Crawlers catalog data, and ETL jobs transform and load it. AWS Amplify: Hosts the Next.js web interface. Amazon Cognito: Secures access for lab users. Component Design Edge Devices: Raspberry Pi collects and filters sensor data, sending it to IoT Core. Data Ingestion: AWS IoT Core receives MQTT messages from the edge devices. Data Storage: Raw data is stored in an S3 data lake; processed data is stored in another S3 bucket. Data Processing: AWS Glue Crawlers catalog the data, and ETL jobs transform it for analysis. Web Interface: AWS Amplify hosts a Next.js app for real-time dashboards and analytics. User Management: Amazon Cognito manages user access, allowing up to 5 active accounts. 4. Technical Implementation Implementation Phases This project has two parts—setting up weather edge stations and building the weather platform—each following 4 phases:\nBuild Theory and Draw Architecture: Research Raspberry Pi setup with ESP32 sensors and design the AWS serverless architecture (1 month pre-internship) Calculate Price and Check Practicality: Use AWS Pricing Calculator to estimate costs and adjust if needed (Month 1). Fix Architecture for Cost or Solution Fit: Tweak the design (e.g., optimize Lambda with Next.js) to stay cost-effective and usable (Month 2). Develop, Test, and Deploy: Code the Raspberry Pi setup, AWS services with CDK/SDK, and Next.js app, then test and release to production (Months 2-3). Technical Requirements\nWeather Edge Station: Sensors (temperature, humidity, rainfall, wind speed), a microcontroller (ESP32), and a Raspberry Pi as the edge device. Raspberry Pi runs Raspbian, handles Docker for filtering, and sends 1 MB/day per station via MQTT over Wi-Fi. Weather Platform: Practical knowledge of AWS Amplify (hosting Next.js), Lambda (minimal use due to Next.js), AWS Glue (ETL), S3 (two buckets), IoT Core (gateway and rules), and Cognito (5 users). Use AWS CDK/SDK to code interactions (e.g., IoT Core rules to S3). Next.js reduces Lambda workload for the fullstack web app. 5. Timeline \u0026amp; Milestones Project Timeline\nPre-Internship (Month 0): 1 month for planning and old station review. Internship (Months 1-3): 3 months. Month 1: Study AWS and upgrade hardware. Month 2: Design and adjust architecture. Month 3: Implement, test, and launch. Post-Launch: Up to 1 year for research. 6. Budget Estimation You can find the budget estimation on the AWS Pricing Calculator.\nOr you can download the Budget Estimation File.\nInfrastructure Costs AWS Services: AWS Lambda: $0.00/month (1,000 requests, 512 MB storage). S3 Standard: $0.15/month (6 GB, 2,100 requests, 1 GB scanned). Data Transfer: $0.02/month (1 GB inbound, 1 GB outbound). AWS Amplify: $0.35/month (256 MB, 500 ms requests). Amazon API Gateway: $0.01/month (2,000 requests). AWS Glue ETL Jobs: $0.02/month (2 DPUs). AWS Glue Crawlers: $0.07/month (1 crawler). MQTT (IoT Core): $0.08/month (5 devices, 45,000 messages). Total: $0.7/month, $8.40/12 months\nHardware: $265 one-time (Raspberry Pi 5 and sensors). 7. Risk Assessment Risk Matrix Network Outages: Medium impact, medium probability. Sensor Failures: High impact, low probability. Cost Overruns: Medium impact, low probability. Mitigation Strategies Network: Local storage on Raspberry Pi with Docker. Sensors: Regular checks and spares. Cost: AWS budget alerts and optimization. Contingency Plans Revert to manual methods if AWS fails. Use CloudFormation for cost-related rollbacks. 8. Expected Outcomes Technical Improvements: Real-time data and analytics replace manual processes.\nScalable to 10-15 stations.\nLong-term Value 1-year data foundation for AI research.\nReusable for future projects.\n"
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/5-workshop/5.3-s3-vpc/5.3.2-test-gwe/",
	"title": "Test the Gateway Endpoint",
	"tags": [],
	"description": "",
	"content": "Create S3 bucket Navigate to S3 management console In the Bucket console, choose Create bucket In the Create bucket console Name the bucket: choose a name that hasn\u0026rsquo;t been given to any bucket globally (hint: lab number and your name) Leave other fields as they are (default) Scroll down and choose Create bucket Successfully create S3 bucket. Connect to EC2 with session manager For this workshop, you will use AWS Session Manager to access several EC2 instances. Session Manager is a fully managed AWS Systems Manager capability that allows you to manage your Amazon EC2 instances and on-premises virtual machines (VMs) through an interactive one-click browser-based shell. Session Manager provides secure and auditable instance management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys.\nFirst cloud journey Lab for indepth understanding of Session manager.\nIn the AWS Management Console, start typing Systems Manager in the quick search box and press Enter: From the Systems Manager menu, find Node Management in the left menu and click Session Manager: Click Start Session, and select the EC2 instance named Test-Gateway-Endpoint. This EC2 instance is already running in \u0026ldquo;VPC Cloud\u0026rdquo; and will be used to test connectivity to Amazon S3 through the Gateway endpoint you just created (s3-gwe).\nSession Manager will open a new browser tab with a shell prompt: sh-4.2 $\nYou have successfully start a session - connect to the EC2 instance in VPC cloud. In the next step, we will create a S3 bucket and a file in it.\nCreate a file and upload to s3 bucket Change to the ssm-user\u0026rsquo;s home directory by typing cd ~ in the CLI Create a new file to use for testing with the command fallocate -l 1G testfile.xyz, which will create a file of 1GB size named \u0026ldquo;testfile.xyz\u0026rdquo;. Upload file to S3 bucket with command aws s3 cp testfile.xyz s3://your-bucket-name. Replace your-bucket-name with the name of S3 bucket that you created earlier. You have successfully uploaded the file to your S3 bucket. You can now terminate the session.\nCheck object in S3 bucket Navigate to S3 console. Click the name of your s3 bucket In the Bucket console, you will see the file you have uploaded to your S3 bucket Section summary Congratulation on completing access to S3 from VPC. In this section, you created a Gateway endpoint for Amazon S3, and used the AWS CLI to upload an object. The upload worked because the Gateway endpoint allowed communication to S3, without needing an Internet Gateway attached to \u0026ldquo;VPC Cloud\u0026rdquo;. This demonstrates the functionality of the Gateway endpoint as a secure path to S3 without traversing the Public Internet.\n"
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/1-worklog/1.10-week10/",
	"title": "Week 10 Worklog",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 10 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 10 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/1-worklog/1.11-week11/",
	"title": "Week 11 Worklog",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 11 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 11 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/1-worklog/1.12-week12/",
	"title": "Week 12 Worklog",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 12 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 12 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/5-workshop/5.3-s3-vpc/",
	"title": "Access S3 from VPC",
	"tags": [],
	"description": "",
	"content": "Using Gateway endpoint In this section, you will create a Gateway eendpoint to access Amazon S3 from an EC2 instance. The Gateway endpoint will allow upload an object to S3 buckets without using the Public Internet. To create an endpoint, you must specify the VPC in which you want to create the endpoint, and the service (in this case, S3) to which you want to establish the connection.\nContent Create gateway endpoint Test gateway endpoint "
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/5-workshop/5.4-s3-onprem/5.4.3-test-endpoint/",
	"title": "Test the Interface Endpoint",
	"tags": [],
	"description": "",
	"content": "Get the regional DNS name of S3 interface endpoint From the Amazon VPC menu, choose Endpoints.\nClick the name of newly created endpoint: s3-interface-endpoint. Click details and save the regional DNS name of the endpoint (the first one) to your text-editor for later use.\nConnect to EC2 instance in \u0026ldquo;VPC On-prem\u0026rdquo; Navigate to Session manager by typing \u0026ldquo;session manager\u0026rdquo; in the search box\nClick Start Session, and select the EC2 instance named Test-Interface-Endpoint. This EC2 instance is running in \u0026ldquo;VPC On-prem\u0026rdquo; and will be used to test connectivty to Amazon S3 through the Interface endpoint we just created. Session Manager will open a new browser tab with a shell prompt: sh-4.2 $\nChange to the ssm-user\u0026rsquo;s home directory with command \u0026ldquo;cd ~\u0026rdquo;\nCreate a file named testfile2.xyz\nfallocate -l 1G testfile2.xyz Copy file to the same S3 bucket we created in section 3.2 aws s3 cp --endpoint-url https://bucket.\u0026lt;Regional-DNS-Name\u0026gt; testfile2.xyz s3://\u0026lt;your-bucket-name\u0026gt; This command requires the \u0026ndash;endpoint-url parameter, because you need to use the endpoint-specific DNS name to access S3 using an Interface endpoint. Do not include the leading \u0026rsquo; * \u0026rsquo; when copying/pasting the regional DNS name. Provide your S3 bucket name created earlier Now the file has been added to your S3 bucket. Let check your S3 bucket in the next step.\nCheck Object in S3 bucket Navigate to S3 console Click Buckets Click the name of your bucket and you will see testfile2.xyz has been added to your bucket "
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/3-blogstranslated/",
	"title": "Translated Blogs",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nThis section will list and introduce the blogs you have translated. For example:\nBlog 1 - Running and Optimizing Small Language Models On-Premises and at the Edge This blog provides a comprehensive guide on deploying Small Language Models (SLMs) to AWS on-premises or edge environments (AWS Local Zones and AWS Outposts) to meet requirements for data residency, information security policies, and low latency. The content covers: differences between LLMs and SLMs, benefits of SLMs (faster inference time, lower resource requirements, suitable for edge computing), deployment architecture using the Llama.cpp framework, specific steps for installation and configuration (launching GPU instances, installing NVIDIA drivers, installing Llama.cpp, downloading and converting models to GGUF format), and optimization examples for chatbot and text summarization use cases.\nBlog 2 - AWS EUC New York Summit: The AI Advantage: Unlocking the Full Potential of Your EUC Services This blog introduces a discussion session at the AWS EUC New York Summit, focusing on how to apply AI to optimize End-User Computing (EUC) administrative tasks and enhance user productivity. Main content includes: leveraging Amazon Bedrock\u0026rsquo;s agentic AI combined with Amazon WorkSpaces and Amazon CloudWatch to automate administrative tasks, providing actionable insights from critical metrics and logs, collecting data on user connectivity, platforms, and IP addresses, and analyzing data to optimize end-user operations. The builders session provides in-depth knowledge and hands-on learning experience on integrating AI into existing EUC frameworks.\nBlog 3 - How to Set Up Automated Alerts for Newly Purchased AWS Savings Plans This blog provides a guide on setting up an automated monitoring and alerting system to detect underutilized AWS Savings Plans within the eligible refund period (first 7 days and within the same purchase month). Content includes: explanation of AWS Savings Plans and refund policy (commitments ≤ $100/hour), solution architecture using AWS CloudFormation, Step Functions, SNS, EventBridge, and IAM roles, deployment across two accounts (Management Account and Member Account) following AWS security best practices, automated process to analyze Savings Plans utilization rates and send email alerts when detecting plans below predefined thresholds, detailed step-by-step deployment instructions and solution testing procedures.\n"
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/5-workshop/5.4-s3-onprem/",
	"title": "Access S3 from on-premises",
	"tags": [],
	"description": "",
	"content": "Overview In this section, you will create an Interface endpoint to access Amazon S3 from a simulated on-premises environment. The Interface endpoint will allow you to route to Amazon S3 over a VPN connection from your simulated on-premises environment.\nWhy using Interface endpoint:\nGateway endpoints only work with resources running in the VPC where they are created. Interface endpoints work with resources running in VPC, and also resources running in on-premises environments. Connectivty from your on-premises environment to the cloud can be provided by AWS Site-to-Site VPN or AWS Direct Connect. Interface endpoints allow you to connect to services powered by AWS PrivateLink. These services include some AWS services, services hosted by other AWS customers and partners in their own VPCs (referred to as PrivateLink Endpoint Services), and supported AWS Marketplace Partner services. For this workshop, we will focus on connecting to Amazon S3. "
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/4-eventparticipated/",
	"title": "Events Participated",
	"tags": [],
	"description": "",
	"content": " In this section, you should list and describe in detail the events you have participated in during your internship or work experience.\nEach event should be presented in the format Event 1, Event 2, Event 3…, along with the following details:\nEvent name Date and time Location (if applicable) Your role in the event (attendee, event support, speaker, etc.) A brief description of the event’s content and main activities Outcomes or value gained (lessons learned, new skills, contribution to the team/project) This listing helps demonstrate your actual participation as well as the soft skills and experience you have gained from each event. During my internship, I participated in two events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: Vietnam Cloud Day 2025 : Ho Chi Minh City Connect Edition for Builders (Track 1: GenAI \u0026amp; Data)\nDate \u0026amp; Time: 09:00, September 18, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: GenAI-powered App-DB Modernization workshop\nDate \u0026amp; Time: 09:00, August 13, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/5-workshop/5.4-s3-onprem/5.4.4-dns-simulation/",
	"title": "On-premises DNS Simulation",
	"tags": [],
	"description": "",
	"content": "AWS PrivateLink endpoints have a fixed IP address in each Availability Zone where they are deployed, for the life of the endpoint (until it is deleted). These IP addresses are attached to Elastic Network Interfaces (ENIs). AWS recommends using DNS to resolve the IP addresses for endpoints so that downstream applications use the latest IP addresses when ENIs are added to new AZs, or deleted over time.\nIn this section, you will create a forwarding rule to send DNS resolution requests from a simulated on-premises environment to a Route 53 Private Hosted Zone. This section leverages the infrastructure deployed by CloudFormation in the Prepare the environment section.\nCreate DNS Alias Records for the Interface endpoint Navigate to the Route 53 management console (Hosted Zones section). The CloudFormation template you deployed in the Prepare the environment section created this Private Hosted Zone. Click on the name of the Private Hosted Zone, s3.us-east-1.amazonaws.com: Create a new record in the Private Hosted Zone: Record name and record type keep default options Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor (you saved when doing section 4.3) Click Add another record, and add a second record using the following values. Click Create records when finished to create both records. Record name: *. Record type: keep default value (type A) Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor The new records appear in the Route 53 console:\nCreate a Resolver Forwarding Rule Route 53 Resolver Forwarding Rules allow you to forward DNS queries from your VPC to other sources for name resolution. Outside of a workshop environment, you might use this feature to forward DNS queries from your VPC to DNS servers running on-premises. In this section, you will simulate an on-premises conditional forwarder by creating a forwarding rule that forwards DNS queries for Amazon S3 to a Private Hosted Zone running in \u0026ldquo;VPC Cloud\u0026rdquo; in-order to resolve the PrivateLink interface endpoint regional DNS name.\nFrom the Route 53 management console, click Inbound endpoints on the left side bar In the Inbound endpoints console, click the ID of the inbound endpoint Copy the two IP addresses listed to your text editor From the Route 53 menu, choose Resolver \u0026gt; Rules, and click Create rule: In the Create rule console: Name: myS3Rule Rule type: Forward Domain name: s3.us-east-1.amazonaws.com VPC: VPC On-prem Outbound endpoint: VPCOnpremOutboundEndpoint Target IP Addresses: Enter both IP addresses from your text editor (inbound endpoint addresses) and then click Submit You have successfully created resolver forwarding rule.\nTest the on-premises DNS Simulation Connect to Test-Interface-Endpoint EC2 instance with Session manager Test DNS resolution. The dig command will return the IP addresses assigned to the VPC Interface endpoint running in VPC Cloud (your IP\u0026rsquo;s will be different): dig +short s3.us-east-1.amazonaws.com The IP addresses returned are the VPC endpoint IP addresses, NOT the Resolver IP addresses you pasted from your text editor. The IP addresses of the Resolver endpoint and the VPC endpoint look similar because they are all from the VPC Cloud CIDR block.\nNavigate to the VPC menu (Endpoints section), select the S3 Interface endpoint. Click the Subnets tab and verify that the IP addresses returned by Dig match the VPC endpoint: Return to your shell and use the AWS CLI to test listing your S3 buckets: aws s3 ls --endpoint-url https://s3.us-east-1.amazonaws.com Terminate your Session Manager session: In this section you created an Interface endpoint for Amazon S3. This endpoint can be reached from on-premises through Site-to-Site VPN or AWS Direct Connect. Route 53 Resolver outbound endpoints simulated forwarding DNS requests from on-premises to a Private Hosted Zone running the cloud. Route 53 inbound Endpoints recieved the resolution request and returned a response containing the IP addresses of the VPC interface endpoint. Using DNS to resolve the endpoint IP addresses provides high availability in-case of an Availability Zone outage.\n"
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/5-workshop/5.5-policy/",
	"title": "VPC Endpoint Policies",
	"tags": [],
	"description": "",
	"content": "When you create an interface or gateway endpoint, you can attach an endpoint policy to it that controls access to the service to which you are connecting. A VPC endpoint policy is an IAM resource policy that you attach to an endpoint. If you do not attach a policy when you create an endpoint, AWS attaches a default policy for you that allows full access to the service through the endpoint.\nYou can create a policy that restricts access to specific S3 buckets only. This is useful if you only want certain S3 Buckets to be accessible through the endpoint.\nIn this section you will create a VPC endpoint policy that restricts access to the S3 bucket specified in the VPC endpoint policy.\nConnect to an EC2 instance and verify connectivity to S3 Start a new AWS Session Manager session on the instance named Test-Gateway-Endpoint. From the session, verify that you can list the contents of the bucket you created in Part 1: Access S3 from VPC: aws s3 ls s3://\\\u0026lt;your-bucket-name\\\u0026gt; The bucket contents include the two 1 GB files uploaded in earlier.\nCreate a new S3 bucket; follow the naming pattern you used in Part 1, but add a \u0026lsquo;-2\u0026rsquo; to the name. Leave other fields as default and click create Successfully create bucket\nNavigate to: Services \u0026gt; VPC \u0026gt; Endpoints, then select the Gateway VPC endpoint you created earlier. Click the Policy tab. Click Edit policy. The default policy allows access to all S3 Buckets through the VPC endpoint.\nIn Edit Policy console, copy \u0026amp; Paste the following policy, then replace yourbucketname-2 with your 2nd bucket name. This policy will allow access through the VPC endpoint to your new bucket, but not any other bucket in Amazon S3. Click Save to apply the policy. {\r\u0026#34;Id\u0026#34;: \u0026#34;Policy1631305502445\u0026#34;,\r\u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;,\r\u0026#34;Statement\u0026#34;: [\r{\r\u0026#34;Sid\u0026#34;: \u0026#34;Stmt1631305501021\u0026#34;,\r\u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;,\r\u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;,\r\u0026#34;Resource\u0026#34;: [\r\u0026#34;arn:aws:s3:::yourbucketname-2\u0026#34;,\r\u0026#34;arn:aws:s3:::yourbucketname-2/*\u0026#34;\r],\r\u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;\r}\r]\r} Successfully customize policy\nFrom your session on the Test-Gateway-Endpoint instance, test access to the S3 bucket you created in Part 1: Access S3 from VPC aws s3 ls s3://\u0026lt;yourbucketname\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy:\nReturn to your home directory on your EC2 instance cd~ Create a file fallocate -l 1G test-bucket2.xyz Copy file to 2nd bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-2nd-bucket-name\u0026gt; This operation succeeds because it is permitted by the VPC endpoint policy.\nThen we test access to the first bucket by copy the file to 1st bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-1st-bucket-name\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy.\nPart 3 Summary: In this section, you created a VPC endpoint policy for Amazon S3, and used the AWS CLI to test the policy. AWS CLI actions targeted to your original S3 bucket failed because you applied a policy that only allowed access to the second bucket you created. AWS CLI actions targeted for your second bucket succeeded because the policy allowed them. These policies can be useful in situations where you need to control access to resources through VPC endpoints.\n"
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nSecure Hybrid Access to S3 using VPC Endpoints Overview AWS PrivateLink provides private connectivity to AWS services from VPCs and your on-premises networks, without exposing your traffic to the Public Internet.\nIn this lab, you will learn how to create, configure, and test VPC endpoints that enable your workloads to reach AWS services without traversing the Public Internet.\nYou will create two types of endpoints to access Amazon S3: a Gateway VPC endpoint, and an Interface VPC endpoint. These two types of VPC endpoints offer different benefits depending on if you are accessing Amazon S3 from the cloud or your on-premises location\nGateway - Create a gateway endpoint to send traffic to Amazon S3 or DynamoDB using private IP addresses.You route traffic from your VPC to the gateway endpoint using route tables. Interface - Create an interface endpoint to send traffic to endpoint services that use a Network Load Balancer to distribute traffic. Traffic destined for the endpoint service is resolved using DNS. Content Workshop overview Prerequiste Access S3 from VPC Access S3 from On-premises VPC Endpoint Policies (Bonus) Clean up "
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/5-workshop/5.6-cleanup/",
	"title": "Clean up",
	"tags": [],
	"description": "",
	"content": "Congratulations on completing this workshop! In this workshop, you learned architecture patterns for accessing Amazon S3 without using the Public Internet.\nBy creating a gateway endpoint, you enabled direct communication between EC2 resources and Amazon S3, without traversing an Internet Gateway. By creating an interface endpoint you extended S3 connectivity to resources running in your on-premises data center via AWS Site-to-Site VPN or Direct Connect. clean up Navigate to Hosted Zones on the left side of Route 53 console. Click the name of s3.us-east-1.amazonaws.com zone. Click Delete and confirm deletion by typing delete. Disassociate the Route 53 Resolver Rule - myS3Rule from \u0026ldquo;VPC Onprem\u0026rdquo; and Delete it. Open the CloudFormation console and delete the two CloudFormation Stacks that you created for this lab: PLOnpremSetup PLCloudSetup Delete S3 buckets Open S3 console Choose the bucket we created for the lab, click and confirm empty. Click delete and confirm delete. "
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/6-self-evaluation/",
	"title": "Self-Assessment",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nDuring my internship at [Company/Organization Name] from [start date] to [end date], I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in [briefly describe the main project or task], through which I improved my skills in [list skills: programming, analysis, reporting, communication, etc.].\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ✅ ☐ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ☐ ✅ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ✅ ☐ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to schedules, rules, and work processes ☐ ☐ ✅ 6 Progressive mindset Willingness to receive feedback and improve oneself ☐ ✅ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ✅ ☐ ☐ 12 Overall General evaluation of the entire internship period ✅ ☐ ☐ Needs Improvement Strengthen discipline and strictly comply with the rules and regulations of the company or any organization Improve problem-solving thinking Enhance communication skills in both daily interactions and professional contexts, including handling situations effectively "
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/7-feedback/",
	"title": "Sharing and Feedback",
	"tags": [],
	"description": "",
	"content": " Here, you can freely share your personal opinions about your experience participating in the First Cloud Journey program. This will help the FCJ team improve any shortcomings based on the following aspects:\nOverall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help whenever I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better. However, I think it would be nice to have more social gatherings or team bonding activities to strengthen relationships.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I don’t understand, and always encourages me to ask questions. The admin team supports administrative tasks, provides necessary documents, and creates favorable conditions for me to work effectively. I especially appreciate that the mentor allows me to try and solve problems myself instead of just giving the answer.\n3. Relevance of Work to Academic Major\nThe tasks I was assigned align well with the knowledge I learned at university, while also introducing me to new areas I had never encountered before. This allowed me to both strengthen my foundational knowledge and gain practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared valuable real-world experiences that helped me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable. When there are urgent projects, everyone works together and supports one another regardless of their position. This made me feel like a real part of the team, even as an intern.\n6. Internship Policies / Benefits\nThe company provides an internship allowance and offers flexible working hours when needed. In addition, having the opportunity to join internal training sessions is a big plus.\nAdditional Questions What did you find most satisfying during your internship? What do you think the company should improve for future interns? If recommending to a friend, would you suggest they intern here? Why or why not? Suggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience? Would you like to continue this program in the future? Any other comments (free sharing): "
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://tuanvu250.github.io/FCJ-Report/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]